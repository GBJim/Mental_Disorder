{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "from Levenshtein import *\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing functions:\n",
    "\n",
    "\n",
    "\n",
    "def stringConditionProcess(group, method = \"first\"):\n",
    "    result = []\n",
    "    for timeSeries in group:\n",
    "        conditions = seriesContains(timeSeries, method)\n",
    "        result.append(timeSeries[conditions])\n",
    "    return result\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def getEmptyEmotions():\n",
    "    return {\"joy\":0,\"sadness\": 0,\"fear\":0, \"anticipation\": 0, \"anger\":0, \"trust\": 0, \"disgust\": 0 ,\"surprise\" : 0}\n",
    "\n",
    "def getUsersEmotions(collectionName):\n",
    "    collection = MongoClient(\"localhost\", 27017)['idea'][collectionName]\n",
    "    usersEmotions = {}\n",
    "    for tweet in collection.find():\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        \n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]\n",
    "        else:\n",
    "            emotion_2 = None\n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "        ambiguous = True if tweet['emotion']['ambiguous'] == 'yes' else False\n",
    "        if userID not in usersEmotions:\n",
    "            usersEmotions[userID] = {}\n",
    "        if date not in usersEmotions[userID]:\n",
    "            usersEmotions[userID][date] = getEmptyEmotions()\n",
    "        usersEmotions[userID][date]['text'] = text\n",
    "        usersEmotions[userID][date][emotion] += 1\n",
    "        usersEmotions[userID][date]['ambiguous'] = ambiguous\n",
    "        usersEmotions[userID][date]['emotion'] = emotion\n",
    "        usersEmotions[userID][date]['emotion_2'] = emotion_2\n",
    "\n",
    "    return usersEmotions\n",
    "\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['pos'] = usersEmotions[userID]['joy'] + usersEmotions[userID]['trust'] \n",
    "        usersEmotions[userID]['neg'] = usersEmotions[userID]['fear'] + usersEmotions[userID]['sadness'] + usersEmotions[userID]['disgust'] + usersEmotions[userID]['anger']\n",
    "        usersEmotions[userID]['polarity'] =  usersEmotions[userID]['pos'] - usersEmotions[userID]['neg']\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "\n",
    "def disambiguate(timeSeries):\n",
    "    conditions = np.logical_not(timeSeries['ambiguous'].values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "    timeSeries['dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries\n",
    "        \n",
    "def invalid_removal(data):\n",
    "    data = np.array(data)\n",
    "    return data[np.isfinite(data)]\n",
    "    \n",
    "def reject_outliers(data, m=2):\n",
    "    data = np.array(data)\n",
    "    data = data[np.isfinite(data)]\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computation fumctions\n",
    "\n",
    "\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def seriesContains(timeSeries,method =\"first\"):\n",
    "    if method == \"first\":\n",
    "        match_function = np.vectorize(firstPronuonDetect)\n",
    "    elif method == \"second\":\n",
    "        match_function = np.vectorize(secondPronuonDetect)\n",
    "    elif method == \"third\":\n",
    "            match_function = np.vectorize(thirdPronuonDetect)\n",
    "\n",
    "\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n",
    "    \n",
    "\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def secondPronuonDetect(words, matchers=[\"you\"]):\n",
    "    for matcher in matchers:\n",
    "        if matcher in words:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "\n",
    "def firstPronuonDetect(words, matchers=[\"i\",\"we\",\"i'd\",\"i'm\"]):\n",
    "    for matcher in matchers:\n",
    "        if matcher in words:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def getFlipsDurationMean(timeSeries, upperbound=np.inf, lowerbound=0):\n",
    "    flips = getFlips(timeSeries)\n",
    "    durations = getFlipsDuration(timeSeries, flips)\n",
    "    durations = durations[np.isfinite(durations)]\n",
    "    durations = durations[(durations > lowerbound) & (durations < upperbound)]\n",
    "    return np.mean(durations)\n",
    "    \n",
    "    \n",
    "    \n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries['dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "def getConditionalFlip(timeSeries, emotion, counter_emotion):\n",
    "    timeSeries['new_polarity'] = timeSeries[emotion] - timeSeries[counter_emotion]\n",
    "    return getFlips(timeSeries,attribute='new_polarity')\n",
    "\n",
    "def cleanPost(timeSeries):\n",
    "    left_text = timeSeries['text'].values[:-1]\n",
    "    right_text = timeSeries['text'].values[1:]\n",
    "    conditions = np.logical_not(timeSeries['ambiguous'].values)\n",
    "    edit_distance = np.vectorize(distance)\n",
    "    conditions[:-1] =  conditions[:-1] & (edit_distance(left_text, right_text) > 5)\n",
    "    patterns = ['http://','https://']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        conditions = conditions & np.logical_not(timeSeries['text'].str.contains(pattern).values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "\n",
    "    return timeSeries\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def userVerify(timeSeries, threshold = 0.5):\n",
    "    http_rows = getHTTPRows(timeSeries)\n",
    "    average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "    return (average_http_count < threshold) & (timeSeries.shape[0] > 50)\n",
    " \n",
    "\n",
    "def groupFilter(group):\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        if userVerify(timeSeries):\n",
    "            new_group.append(timeSeries)\n",
    "    return new_group\n",
    "\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "    \n",
    "    \n",
    "\n",
    "def getMeanDeltaTime(timeSeries, lowerbound=0, upperbound=3):\n",
    "    delta_time = timeSeries['dt'][:-1].values\n",
    "    mean_deal_time = np.mean(delta_time[(delta_time < upperbound) & (delta_time > lowerbound)])\n",
    "    return mean_deal_time\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "\n",
    "def getTimeRange(timeSeries):\n",
    "    time_range = (np.max(timeSeries.index.values) -np.min(timeSeries.index.values) ).astype('timedelta64[s]')\n",
    "    time_range = time_range / np.timedelta64(60, 's')\n",
    "    return time_range\n",
    "\n",
    "\n",
    "\n",
    "def getPairs(timeSeries, upperbound = 0.2):\n",
    "    conditions = timeSeries['dt'].values < upperbound\n",
    "    conditions[1:] = conditions[1:] | conditions[:-1]\n",
    "    return timeSeries[conditions]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comboPlot(groups, bins=10,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    for g,group in enumerate(groups):\n",
    "        tweets_length = np.zeros(len(group),dtype=int)\n",
    "        combos = []\n",
    "        for i,timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            combo = comboTracker(timeSeries)\n",
    "            tweets_length[i] = timeSeries.shape[0]\n",
    "            combos += [hit for element, hit in combo if element != 0 and hit >2 ]\n",
    "        combos = reject_outliers(combos)\n",
    "        plt.hist(combos,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(str(len(group)) + \" \"+legends[g] + \" people\")\n",
    "        plt.show()\n",
    "        print(\"Average size of total tweets: {} std:\".format(np.mean(tweets_length),np.std(tweets_length)))\n",
    "        print(\"Time Duration mean:{0:2f} STD:{1:2f}\".format(np.mean(combos),np.std(combos)))\n",
    "\n",
    "\n",
    "\n",
    "def emotionPieChart(groups,legends = [\"BPD\", \"Normal\"]):\n",
    "    colorTable = {\"joy\":\"#FADB4D\",\"sadness\": \"#729DC9\",\"fear\":\"#35A450\", \"anticipation\": \"#F2993A\", \"anger\":\"#E43054\", \"trust\": \"#99CC33\", \"disgust\": \"#9F78BA\" ,\"surprise\" : \"#3FA5C0\"}\n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    colors = [colorTable[emotion] for emotion in emotions]\n",
    "    \n",
    "    for g, group in enumerate(groups):\n",
    "        emotion_means = {}\n",
    "        for emotion in emotions:\n",
    "            emotion_means[emotion] = [] \n",
    "        for timeSeries in group:\n",
    "            if timeSeries.shape[0] < 100:\n",
    "                continue\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            summary = timeSeries.mean()\n",
    "            for emotion in emotions:\n",
    "                emotion_means[emotion].append(summary[emotion])\n",
    "            \n",
    "        for emotion in emotions:  \n",
    "            emotion_means[emotion] = np.array(emotion_means[emotion])\n",
    "            emotion_means[emotion] = emotion_means[emotion][np.isfinite(emotion_means[emotion])]\n",
    "\n",
    "            \n",
    "        emotion_means_np = [np.mean(emotion_means[emotion]) for emotion in emotions]\n",
    "            \n",
    "        #return emotion_means\n",
    "        plt.title(\"{} people\".format(legends[g]))\n",
    "        plt.pie(emotion_means_np, labels=emotions, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "        plt.figure(figsize=(60,60))\n",
    "        plt.show()\n",
    "        #return emotion_means\n",
    "        for emotion in emotions:\n",
    "            print(\"{} std: {}\".format(emotion, np.std(emotion_means[emotion])))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def flipDurationPlotPerUser(groups, bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    print(\"Flip Durations of each flip\")\n",
    "    for g,group in enumerate(groups):\n",
    "        delta_times = np.zeros(len(group),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            mean_deal_time = getFlipsDurationMean(timeSeries, lowerbound=0, upperbound=upperbound)\n",
    "            delta_times[i] = mean_deal_time\n",
    "        delta_times = delta_times[np.isfinite(delta_times)]\n",
    "        plt.hist(delta_times,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(\"{} {} people \".format(len(group),legends[g]))\n",
    "        plt.show()\n",
    "        print(\"Time Duration mean:{0:2f} STD:{1:2f}\".format(np.mean(delta_times),np.std(delta_times)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flipDurationPlot(groups, bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    for g,group in enumerate(groups):\n",
    "        tweets_length = np.zeros(len(group),dtype=int)\n",
    "        delta_times = np.array([])\n",
    "        for i,timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            flips = getFlips(timeSeries)\n",
    "            tweets_length[i] = timeSeries.shape[0]\n",
    "\n",
    "            delta_times = np.concatenate((delta_times,getFlipsDuration(timeSeries, flips)))\n",
    "        delta_times =delta_times[(delta_times < upperbound) & (delta_times > lowerbound)]\n",
    "        plt.hist(delta_times,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(str(len(group)) + \" \"+legends[g] + \" people\")\n",
    "        plt.show()\n",
    "        print(\"Average size of total tweets: {} std:\".format(np.mean(tweets_length),np.std(tweets_length)))\n",
    "        print(\"Time Duration mean:{0:2f} STD:{1:2f}\".format(np.mean(delta_times),np.std(delta_times)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def countPlotPerUser(groups,f, method=\"first\", bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100, xmax=None):\n",
    "\n",
    "        \n",
    "    for g,group in enumerate(groups):\n",
    "        counts = np.zeros(len(group),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            if method is None:\n",
    "                http_count = f(timeSeries)\n",
    "            else:\n",
    "\n",
    "                http_count = f(timeSeries,method)\n",
    "            average_count = np.sum(http_count) / timeSeries.shape[0]\n",
    "            counts[i] =  average_count\n",
    "    \n",
    "        counts = counts[np.isfinite(counts)]\n",
    "        \n",
    "        plt.hist(counts,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('people')\n",
    "        plt.xlabel('specific post / total post ')\n",
    "        plt.title(str(len(group)) + \" \" + legends[g] + \" people\")\n",
    "        if xmax is not None:\n",
    "            axes = plt.gca()\n",
    "            axes.set_xlim([0,xmax])\n",
    "        plt.show()\n",
    "        print(\"Mean:{0:2f} STD:{1:2f}\".format(np.mean(counts),np.std(counts)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def emotionCountPlotPerUser(groups,f, bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    counter_emotions = {'joy':'sadness','anticipation':'surprise','anger':'fear','trust':'disgust'}\n",
    "    for g,group in enumerate(groups):\n",
    "        counts = np.zeros(len(group),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            http_count = 0\n",
    "            for emotion, counter_emotion in counter_emotions.items():\n",
    "                 http_count += np.sum(f(timeSeries,emotion,counter_emotion))\n",
    "            average_count = http_count / timeSeries.shape[0]\n",
    "            counts[i] =  average_count\n",
    "        counts = counts[np.isfinite(counts)]\n",
    "        plt.hist(counts,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(legends[g])\n",
    "        plt.show()\n",
    "        print(\"Time Duration mean:{0:2f} STD:{1:2f}\".format(np.mean(counts),np.std(counts)))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def durationPlotPerUser(groups, bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    for g,group in enumerate(groups):\n",
    "        delta_times = np.zeros(len(group),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            mean_deal_time = getMeanDeltaTime(timeSeries, upperbound=upperbound, lowerbound=lowerbound)\n",
    "            delta_times[i] = mean_deal_time\n",
    "        delta_times = delta_times[np.isfinite(delta_times)]\n",
    "        plt.hist(delta_times,color=colors[g], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(legends[g])\n",
    "        plt.show()\n",
    "        print(\"Time Duration mean:{0:2f} STD:{1:2f}\".format(np.mean(delta_times),np.std(delta_times)))\n",
    "\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "def durationPlot(groups, bins=100,legends = [\"BPD\", \"Normal\"], colors =['red', 'green'], lowerbound = 0,upperbound = 100):\n",
    "    print(\"Duration of each tweets pair\")\n",
    "    for i,group in enumerate(groups):\n",
    "        delta_times = np.array([])\n",
    "        for timeSeries in group:\n",
    "            timeSeries = cleanPost(timeSeries)\n",
    "            delta_times = np.concatenate((delta_times,timeSeries['dt'][:-1].values))\n",
    "        delta_times =delta_times[(delta_times < upperbound) & (delta_times > lowerbound)]\n",
    "        plt.hist(delta_times,color=colors[i], bins = bins, edgecolor='none' )\n",
    "        plt.ylabel('Post count')\n",
    "        plt.xlabel('Time (mins)')\n",
    "        plt.title(legends[i])\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def polarityPlot(timeSeries, attribute = 'polarity'):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Sentiment Posts\",  fontsize=14)\n",
    "    ax.set_xlabel(\"Week\",  fontsize=14)\n",
    "    plt.plot(timeSeries[attribute])\n",
    "    \n",
    "    plt.axhline(y=0,linestyle=\"--\", color=\"w\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def peaksCount(timeSeries, attribute = 'polarity' ):\n",
    "    middle = timeSeries[attribute][1:-1]\n",
    "    left_elements = np.roll(timeSeries[attribute], 1)[1:-1]\n",
    "    right_elements = np.roll(timeSeries[attribute], -1)[1:-1]\n",
    "    local_maximum = (middle > left_elements) & (middle > right_elements)\n",
    "    local_minimum = (middle < left_elements) & (middle < right_elements)\n",
    "    return np.sum(local_maximum) , np.sum(local_minimum)\n",
    "\n",
    "def crossingCount(timeSeries, attribute = 'polarity', n = 0):\n",
    "    left_elements = np.roll(timeSeries[attribute], 1)[1:]\n",
    "    right_elements = np.roll(timeSeries[attribute], -1)[:-1]\n",
    "    positive_corssing = (left_elements < n) & (right_elements > n)\n",
    "    negative_crossing = (left_elements > n) & (right_elements < n)\n",
    "    return np.sum(positive_corssing | negative_crossing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def foureirPlot(timeSeries):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Post\",  fontsize=14)\n",
    "    ax.set_xlabel(\"Time (Week as unit)\",  fontsize=14)\n",
    "    timeSeries['pos'].plot()\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylabel(\"Amplitude\",  fontsize=14)\n",
    "    ax.set_xlabel(\"Frequency (1/Week)\",  fontsize=14)\n",
    "    plt.plot(np.fft.rfftfreq(timeSeries['pos'].size), np.abs(np.fft.rfft(timeSeries['pos'])))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def flipsPlot(groups, legends = [\"BPD\", \"Normal\"], colors =['red', 'green']):\n",
    "    \n",
    "    hist_results = []\n",
    "    for i,group in enumerate(groups):\n",
    "        group = groupFilter(group)\n",
    "        hist_result = []\n",
    "        flips_counts = []\n",
    "        tweets_counts = []\n",
    "        for timeSeries in group:\n",
    "            flips = np.sum(getFlips(timeSeries))\n",
    "            tweet_count = timeSeries['polarity'].shape[0]\n",
    "            \n",
    "            if np.isfinite(tweet_count) and tweet_count > 0:\n",
    "                flips_per_post = flips / tweet_count\n",
    "\n",
    "                hist_result.append(flips_per_post)\n",
    "                flips_counts.append(flips)\n",
    "                tweets_counts.append(tweet_count)\n",
    "            \n",
    "        \n",
    "        hist_result = reject_outliers(hist_result)\n",
    "        hist_results.append(hist_result)\n",
    "        plt.title(legends[i])\n",
    "        plt.xlabel(\"Flips per post\")\n",
    "        plt.hist(hist_result,color=colors[i], bins = 100, edgecolor='none')\n",
    "        plt.show()\n",
    "    \n",
    "        print(\"Tweets Mean {0:.2f} \".format(np.mean(tweets_counts)))\n",
    "        print(\"Flips Mean {0:.2f} \".format(np.mean(flips_counts)))\n",
    "\n",
    "        print(\"Flips per post Mean {0:.2f}\".format(np.mean(hist_result)))\n",
    "        print(\"Flips per post Standard Deviation {0:.2f}\".format(np.std(hist_result)))\n",
    "    plt.hist(hist_results,color=colors, bins = 100, edgecolor='none', histtype='bar', normed=1, stacked=True)\n",
    "   \n",
    "    plt.xlabel(\"minutes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics\n",
    "\n",
    "def emotionAmbiguity(group, name):\n",
    "    ambiguities = []\n",
    "    for timeSeries in group:\n",
    "        ambiguity = timeSeries.mean()['ambiguous']\n",
    "        ambiguities.append(ambiguity)\n",
    "    print(\"{} emotion ambiguity mean: {} std: {}\".format(name, np.mean(ambiguities), np.std(ambiguities)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BPDEmotions =  getUsersEmotions(\"BPD_fixed_emotion\")\n",
    "BPDtimeSeries = timeSeriesTransform(BPDEmotions) \n",
    "regular_Emotions =  getUsersEmotions(\"regularUser_en_fixed_emotion\")\n",
    "regular_timeSeries = timeSeriesTransform(regular_Emotions)\n",
    "BPD_clean = groupFilter(BPDtimeSeries)\n",
    "regular_clean = random.sample(groupFilter(regular_timeSeries), len(BPD_clean))\n",
    "groups = [BPD_clean, regular_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotionAmbiguity(groupFilter(BPDtimeSeries), \"BPD\")\n",
    "emotionAmbiguity(groupFilter(regular_timeSeries), \"Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotionPieChart(groups , [\"BPD\",\"Normal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countPlotPerUser(groups, getFlips, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Emotion Wheel Flips\n",
    "emotionCountPlotPerUser(groups, getConditionalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "durationPlotPerUser(groups, upperbound=3.5, bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countPlotPerUser(groups, getFlips, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flipDurationPlot(groups, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flipDurationPlotPerUser(groups, bins=50, upperbound=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "countPlotPerUser(groups, seriesContains)   #第一人稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countPlotPerUser(groups, seriesContains, method=\"second\")  #第二人稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countPlotPerUser(groups, seriesContains, method=\"third\")  #第三人稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_groups = []\n",
    "for group in groups:\n",
    "    preprocessed_groups.append(stringConditionProcess(group))\n",
    "\n",
    "flipDurationPlot(preprocessed_groups, bins=50)\n",
    "#flipDurationPlotPerUser(preprocessed_groups, bins=50)\n",
    "emotionPieChart(preprocessed_groups , [\"BPD\",\"Normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_groups = []\n",
    "for group in groups:\n",
    "    preprocessed_groups.append(stringConditionProcess(group, method=\"second\"))\n",
    "\n",
    "flipDurationPlot(preprocessed_groups, bins=50)\n",
    "#flipDurationPlotPerUser(preprocessed_groups, bins=50)\n",
    "emotionPieChart(preprocessed_groups , [\"BPD\",\"Normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_groups = []\n",
    "for group in groups:\n",
    "    preprocessed_groups.append(stringConditionProcess(group, method=\"third\"))\n",
    "\n",
    "flipDurationPlot(preprocessed_groups, bins=50)\n",
    "#flipDurationPlotPerUser(preprocessed_groups, bins=50)\n",
    "emotionPieChart(preprocessed_groups , [\"BPD\",\"Normal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comboPlot(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BPD_581_emotions = getUsersEmotions(\"BPD_581_emotion\")\n",
    "BPD_581_timeSeries = timeSeriesTransform(BPD_581_emotions)\n",
    "BPD_581_clean = groupFilter(BPD_581_timeSeries)\n",
    "regular_clean = random.sample(groupFilter(regular_timeSeries), len(BPD_581_clean))\n",
    "\n",
    "new_groups = [BPD_581_clean, regular_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotionPieChart(new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEmotionCount(timeSeries, matcher=\"fear\"):\n",
    "    return np.sum(timeSeries[\"emotion\"].values == matcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countPlotPerUser(new_groups,getEmotionCount , xmax=0.15,method=None)   # Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEmotionCount_anger(timeSeries, matcher=\"anger\"):\n",
    "    return np.sum(timeSeries[\"emotion\"].values == matcher)\n",
    "countPlotPerUser(new_groups,getEmotionCount_anger , xmax=0.35,method=None)   #Anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getEmotionCount_sadness(timeSeries, matcher=\"sadness\"):\n",
    "    return np.sum(timeSeries[\"emotion\"].values == matcher)\n",
    "countPlotPerUser(new_groups,getEmotionCount_sadness ,xmax=0.9, method=None)   #sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "def load_stopwords(file_location=\"../SmartStoplist\"):\n",
    "    f = open(file_location)\n",
    "    stopwords = [line.strip() for line in f]\n",
    "    return stopwords + [\"http\",\"https\", \"don\", \"thi\",\"http \", \"co\",\"dont\",\"im\"]\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "stemmer = PorterStemmer()\n",
    "stopSet = set(stopwords)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = []\n",
    "    global stopSet\n",
    "    \n",
    "    for word in text.lower().split():\n",
    "        word = re.sub('[!@#$?\\'\\\"]|&amp', '',word)\n",
    "        stemmedWord = stemmer.stem_word(word)\n",
    "        condition_Word = word not in stopSet  and \"http\" not in word\n",
    "        condition_StemmedWord = stemmedWord not in stopSet\n",
    "        \n",
    "        if  condition_Word and  condition_StemmedWord:\n",
    "            words.append(stemmedWord)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def keywordsComparison(groups, emotion, n_feature = 10):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "    group_texts = []\n",
    "    total_texts = []\n",
    "    for group in groups:\n",
    "        texts = \"\"\n",
    "        for timeSeries in group:\n",
    "            conditions = timeSeries[\"emotion\"].values == emotion\n",
    "            timeSeries = timeSeries[conditions]\n",
    "            text = preprocessor(\"\\n\".join(timeSeries[\"text\"].values))\n",
    "            texts += text\n",
    "            total_texts.append(text)\n",
    "        group_texts.append(texts)\n",
    "        \n",
    "       \n",
    "    tfidf.fit(total_texts)\n",
    "    word2vector = tfidf.get_feature_names()\n",
    "    vectors = tfidf.transform(group_texts).todense()\n",
    "    vectors = np.asarray(vectors)\n",
    "    for vector in vectors:\n",
    "        vector = np.argsort(vector)[::-1]\n",
    "        for feature in vector[:n_feature:]:\n",
    "            word = word2vector[feature]\n",
    "            print(word)\n",
    "        print(\"===========================\\n\")\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywordsComparison(new_groups, \"sadness\", n_feature=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywordsComparison(new_groups, \"fear\", n_feature=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.asarray(x.todense()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdifd.fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
