{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import age_gender_predictor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import age_gender_predictor\n",
    "import json\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "from random import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import vstack\n",
    "rcParams['figure.figsize'] = 10, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Fetching, transformation and filtering\n",
    "\n",
    "\n",
    "def getLangRatio(cursor):\n",
    "    lang_ratios = {}\n",
    "    for tweet in cursor:\n",
    "        lang = 1 if tweet[\"lang\"] == \"en\" else 0\n",
    "        user_id = tweet[\"user\"][\"id\"]\n",
    "        if user_id in lang_ratios:\n",
    "            lang_ratios[user_id].append(lang)\n",
    "        else:\n",
    "            lang_ratios[user_id] = [lang]\n",
    "    for user_id, ratio in lang_ratios.items():\n",
    "        lang_ratios[user_id] = np.sum(ratio) / len(ratio)\n",
    "    return lang_ratios\n",
    "                                   \n",
    "\n",
    "def getUsersTweets(dbName,collectionName, en_threshold=0.9):\n",
    "    cursor = MongoClient(\"localhost\", 27017)[dbName][collectionName].find()\n",
    "    lang_ratios = getLangRatio(cursor)\n",
    "\n",
    "    cursor = MongoClient(\"localhost\", 27017)[dbName][collectionName].find()\n",
    "    usersTweets = {}\n",
    "    for tweet in cursor:\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        if lang_ratios[userID] < en_threshold:\n",
    "            continue\n",
    "        #Processing emotions from Carlos' API\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]\n",
    "            \n",
    "        ambiguous = True if tweet['emotion']['ambiguous'] == 'yes' else False\n",
    "       \n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]    \n",
    "        else:\n",
    "            emotion_2 = None\n",
    "        if tweet[\"polarity\"] == \"positive\":\n",
    "            polarity = 1\n",
    "        elif tweet[\"polarity\"] == \"negative\":\n",
    "            polarity = -1\n",
    "        else:\n",
    "            polarity = 0\n",
    "   \n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "\n",
    "        if userID not in usersTweets:\n",
    "            usersTweets[userID] = {}\n",
    "        if date not in usersTweets[userID]:\n",
    "            usersTweets[userID][date] = {}\n",
    "            \n",
    "        usersTweets[userID][date]['text'] = text\n",
    "        usersTweets[userID][date]['polarity'] =  polarity\n",
    "        usersTweets[userID][date]['emotion'] =  emotion\n",
    "        usersTweets[userID][date]['emotion_2'] =  emotion_2\n",
    "        usersTweets[userID][date]['ambiguous'] =  ambiguous\n",
    "    return usersTweets\n",
    "\n",
    "\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "\n",
    "def userFilter(group, spam_threshold=0.5,tweets_threshold=100):    #Spam and inactive user filter\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        http_rows = getHTTPRows(timeSeries)\n",
    "        average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "        if (average_http_count < spam_threshold) and (timeSeries.shape[0] > tweets_threshold):\n",
    "            new_group.append(timeSeries)\n",
    "    return new_group\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Group(object):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, name, group=None, **kwargs):\n",
    "        self.name = name\n",
    "\n",
    "        dbName  = kwargs.get(\"dbName\", None)\n",
    "        collectionName  = kwargs.get(\"collectionName\", None)\n",
    "        \n",
    "        if dbName is None or collectionName is None:\n",
    "            self.group = group\n",
    "        else:\n",
    "\n",
    "            self.group = userFilter(timeSeriesTransform(getUsersTweets(dbName,collectionName)))\n",
    "        \n",
    " \n",
    "        \n",
    "    def getTexts(self):\n",
    "        return [\"\\n\".join(timeSeries[\"text\"].values) for timeSeries in self.group]\n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    def getSize(self):\n",
    "        return len(self.group)\n",
    "    def __repr__(self):\n",
    "        return repr(self.group)\n",
    "    def __add__(self, other):\n",
    "        return Group(self.name, self.group + other.group)\n",
    "    \n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        self.group += other.group\n",
    "        return self    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BPD_group_clean = Group(\"BPD\",dbName = \"patients\",collectionName=\"BPD_clean\")\n",
    "regular_group = Group(\"Random Samples\",dbName = \"idea\",collectionName=\"regularUser_en_fixed_emotion\")\n",
    "bipolar_group_clean = Group(\"Bipolar\",dbName =\"patients\", collectionName=\"bipolar_clean\")\n",
    "mix_group = Group(\"Mix\", dbName = \"patients\",collectionName=\"bb_mix\")\n",
    "\n",
    "\n",
    "BPD_all = BPD_group_clean + mix_group\n",
    "bipolar_all = bipolar_group_clean + mix_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 Normal and 284 Bipolar in Training Data\n",
      "55 Normal and 32 Bipolar in Test Data\n",
      "\n",
      "493 Normal and 217 BPD in Training Data\n",
      "55 Normal and 24 BPD in Test Data\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHBCAYAAAAo6sxCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYXFWB9/HvyQZhCYQtYBJ2RCKrrLJoEBd21EEREBQc\n",
       "txmcGV911BnHSo3jwqjIuAyiIu6gsgkubEKAYZVdBCLIlrCFQCAhG0n6vH+cW/RN092prqpbdav7\n",
       "+3me+/S5S517OvWQ/Djn3HNDjBFJkiR11qhON0CSJEmGMkmSpFIwlEmSJJWAoUySJKkEDGWSJEkl\n",
       "YCiTJEkqAUOZpCIdD1xWx3VnAJ8ruC3t8n7gutx+D7B1Z5oiqZsYyqSR6xFgMbAQeAo4G1i7xff4\n",
       "OfC2Oq77KPBfLb43wAxgOel3fB64CTiggPs0423AtcACYC4wEziikw2S1BmGMmnkisDhwLrA64A9\n",
       "6L+3akw7G9ViETiH9DtuCFwJnNfRFq3qaOBXwI+AycAmwOdpLJSFbJPUpQxlkgCeAC4FXpvt9wD/\n",
       "ADwAzMqOHQ7cCcwHrgd2yn1+KnABqadnHvCt7Pj76R3KC8A3gKeBF4C7gWnZuR8BX8jV98Hs3s8C\n",
       "vwE2y53rAT4M/DVry7cH+b3yQWUl8AtgY2Cj7Nh6wFnZ7z8na0P+78UPAveSerH+AuyWHf8M8GDu\n",
       "+NsHacNgbTsN+E/gh6TePEi9Zh/KyjOAn+Y+syXp96+1cSaph/F6YBHwKeBPfe7zcdKfIcAawNeA\n",
       "R0m9o2cAazbQdkkFMJRJI1stsEwFDgHuyJ07CtiTFJx2I4WXDwIbAGcCFwNjgdHAb4GHgS1IPT7n\n",
       "9HOvt5KGDrcjhaF3Ac9l52K2AbwJ+FJ2fjNSgDi3T12HkXr2dgbeTX1DpOOAE4G/kYIjpDD4ErBN\n",
       "9ju+Ffj77Ny7gApwAjABOJIUEiEFsv2z41XgZ8CkOtqQtz0whcF77up5D957szavA3w3q3fb3Pnj\n",
       "SMPIAF/Jzu2S/ZxM6pmTVAKGMmnkCsBFpN6m60i9Ll/Knf8yaR7WMlLPzZmkXpgI/CQ7/npgL1J4\n",
       "+hSwJDt+Qz/3W04aRtyB9HfPLFJvTV/HkwLgnaTA9NnsPpvnrvkKqZdqNnA1sOsgv+e7s99xMSm8\n",
       "HJodn0QKoh/P2v0McDrwnuz83wOnArdl+38DHsvK5+Xa/itSr97eg7ShPxtmP58c5JrVDUdGUrC8\n",
       "j9SDtoDUK3Zsdn47Uki7OKvrg8D/I32vL5K+4/cgqRQMZdLIFUm9YRNJw2KnkAJVzexceQvgE6Rw\n",
       "U9umkMLYVFJvVs9q7ncVaajxO6QhzDNJIa2vWu9YzSJSD9Xk3LF8mFtM6iUayC9Jv+Mk4B7gY7nf\n",
       "aSwpFNV+p++Shjch/X5/G6DOE0m9irXP7UhvyKpXrddts0GvWr3ZffZ/QW8oOw64EFhK+r3WIoXM\n",
       "Wrv/QO9QrqQOM5RJGkh+6Owx4IukcFPb1iEFntmkXqzRddT5LdKw4zTg1aTetb6eIIXEmrVJgefx\n",
       "IbU+ifT2Nj1L6vH7ELBV1u5lWd2132k9eufKzWbVYcCaLYDvAf9IGsqdSAp7Q51kPyu7x9GDXPMi\n",
       "KUjVbNrPNX2HOK8kBbBdSL1gv8iOzyP1CE6j9/ddnzQEK6kEDGWS6vF94COkocpACkqHkYLZzaTe\n",
       "pq+QAsSawL791LEHaYhvLKl3aylp8j2sOiH/HOAkUqhYgzSkehO9Q4d9DRaG+p77K3AJ8K9Zmy8n\n",
       "TbZfl/T34TbAG7JrfwB8kvRkaiAFtM2z3z2SQs6orK07DtKGgUTSUOJ/kB6ImJDVtz+pFxHgrqw9\n",
       "U0mB8bN1/I7LgV+TJvRPBK7IjveQvsfT6e0NnEyaRyepBAxlkvrTt/flNtJ8pG+TJuc/QBrCg/SP\n",
       "/RGk0PIYqffn3bl6anVNIPUwPUdaI20e8NV+rvsjKaicT+o124pV5z31bVvs59hg576atX2T7Oc4\n",
       "0hOWz5HCTK036jxS7+AvSHO1LiCFnHuBrwM3koZRdwT+b5B7DjZZ/3zgGOBkUk/gU6SnMS/Kzl9B\n",
       "6o28mzSf75J+6uuv/l8AB2W/T35Y+dOkhxRuIj0BewWpx1JSCYQY63m4R5IkSUWyp0ySJKkEDGWS\n",
       "JEklYCiTJEkqAUOZJElSCXTLi4Z9GkGSJHWToa5d2DWhDBr45VQaM7JN3WcGfnfdbAZ+f91qBn53\n",
       "3ayhziSHLyVJkkrAUCZJklQChjK1w8xON0ANm9npBqgpMzvdADVsZqcboPbrlhX98y8VliRJKrOG\n",
       "cos9ZZIkSSVgKJMkSSoBQ5kkSVIJGMokSZJKwFAmSZJUAoYySZKkEjCUSZIklYChTJIkqQSKDmU/\n",
       "BJ4G/jzINd8EHgDuAnYruD2SJEmlVHQoOxs4eJDzhwLbAtsBHwLOKLg9kiRJpVR0KLsOmD/I+SOB\n",
       "H2flm4H1gUkFt0mSJKl0xnT4/pOB2bn9OcAU0pDnKvb7QHiqXY2SpLIZ00PcbCFLtniBJVvPZ/F2\n",
       "z7Jkh3ksnvQiy3P/d/0c8ES2PZkrP0/ve/hqP1cAC+mSFyBLI0GnQxm88oWd/f4FccPm9qBJUl/r\n",
       "LoNXLYTRPRBD71+gMfubNQ5S7gnEno+HlStGsXLFKFauHEXPysDKl0azfOkYlvb0pr3AK0Ndfz/z\n",
       "f58/TwqGte2pPvtzgWdjJa5s8o9AGjY6HcoeB6bm9qdkx16h8sbKy+WZj8zkmkevKbZlktQFFq4B\n",
       "s9Zo+OOB9O9AEf8WbEyaLzyYGKrhWeCZbJub+/kiaYrNqKyd+Z8vAQuAF7KffcsLYiW+1OLfRxrM\n",
       "9GxrSmhDz/WWwCXATv2cOxQ4Jfu5D3B69rOv+NHDwxeKaqAkld2SsawxZ102nrs2m8wfzyYL1mDS\n",
       "onFMWjmKxiPZ8LaU3rD2N+Ae0koA9wHLgNH0hr5an+CsWInPt7+pGoYirxwJXK2iQ9k5wBuBjUjz\n",
       "xCrA2OzcmdnPb5Oe0FwEnATc3k89Df1ykjSchWoIwIas+oBUJDeKOcCxCLDvY4w/8GE23vIFNtp4\n",
       "ERuvt5SN13mJV014ib3WWMFOAUaF7FMByJcZYD8APQGeHc/yuydx44Wv4eYLpjFv5Sg2BTaDl39u\n",
       "DGzQmj+JllkBXA1cCFwUK/HJDrdH3auUoaxVDGWS1E4hrAe8ATgw23ah8b+HHyc9af8b4DZimkcW\n",
       "qmEsKVRunG2b5H6uBfRkW8z9jMA4YAKwXvYzv9WOjW6wrTURuIkU0G7P6htNGuqtlXtIPXJLsq3f\n",
       "svPmRiRDmSSpICFsCOwJrEFvWFqZK9e27YCT6X8qCsCzwOXAH4DLifEVT9s33dTUgzieFM42BHYg\n",
       "TaHZCdgmu6xvu9cCdmx1WzIrWDWsrWDVcJcvjyY9RftQbns4V54bK93xD/cIZyiTJJVECNNI4exE\n",
       "Uu/XQG4nBbRLgZuIcUUbWtevUA1TgLcD7yT1Ejbb21aExcCjwDxSwH2uz88ngatjJS7qWAsFhjJJ\n",
       "UumEMA44DDiKNH94sOWNFpFCx9hsG0earH8VteAW2zPPK1TDRsARpLZvQOoVXEnq5aqVRwFrknrl\n",
       "aj/7lsfT/n+/FgHnAz8lBTSHT9vPUCZJKrEQRgE7k8LZIcC+DH05jjuBy0hPUj5IerLy2bIugpsN\n",
       "pY5l1cA2hhTu8gGvVo6k4Lp1n20r0tDrhCE24XHg59k2l97AOza75zPAQodEW85QJknqIulhgjfR\n",
       "G9KmDv6BAb1Ab0Cbw6rrneV/LipreKtHFvAmkv6cNsi2DfuUX0+aQzcUS0l/RnNJKyXcDPwOuDNW\n",
       "Yk9LGj/yGMokSV0qhEB66hJgOWmB2OWkBwcOybYDaG6h2yWkcPYAaWjvV8S4pIn6SicLbrsDJwDH\n",
       "kZakatRTwO+z7YpYiQuab+GIYSiTJA1jIawLHETqDdqWNJy3LbB2gzU+D/wEuJHUQ1TbnuvmHrWa\n",
       "bMmRt5EetngD6d/R5bltHCkIj6+juiXAD4HTYiU+VEiDhxdDmSRphEk9bJNIAW0b0uK0tbXONulT\n",
       "rvftB8vpHcp7CpgF3JptDxKHz5Be1rO2Nr1/RtuSgtwhpOHQvnpIi75/Nlbi4na1swsZyiRJ6lcK\n",
       "b+uQ3ibwDuBDpAn0Q7WAtIxHLaTdBjzSyaU8ihCqYTRpXbrDSH9er+1zySzghFiJf2p327qEoUyS\n",
       "pLqkJ0HfRFr2YjKpt622DfUJR0hPTuZX8q/9vBM4C7iuW4dEs960g4B/I73doWYpcGisxKs70rBy\n",
       "M5RJktS0EMbTG9AmA7sCe2TbYAvhDuZ+4HvAT4jx2VY0s92ycHYycDqp1xHSunLnAjdk2yyf2AQM\n",
       "ZZIkFSgNgU6hN6DtDryONPdqVJ21LAOuIYWZ2oT7FX1+zic9fHA9sXxPPIZq2I704vbJ/Zyutf06\n",
       "4AexEue1s20lYiiTJKntwssLxPZd1X9j0rIUxwPrNlBzD2n+2jXAb4jxupa0twVCNbyatFTGNoNc\n",
       "dl2sxDe0qUllYyiTJKl0QlgHeA/p4YI9m6jpUuBTxHhPS9rVpFAN44C9SG9mqG19h3d3ipVytLfN\n",
       "DGWSJJVaelH7NqSetTH0vvIoX96atK7Yrrzy374e0tDhLNJbDGrbw8S4tA2/wYCyOWfbkJbMeFt2\n",
       "+Brg6BE4jGkokyRp2AhhfWB/4O9IC8AONm8tkl4xdSXwdWL8S/EN7F+ohoOydtQ8CrwjVuIdHWpS\n",
       "JxjKJEkalkLYCfhv0ntC6/Fb4FRi/L/iGjWwUA3/DvxX7tBS4LBYiVd1oj0dYCiTJGlYC2FbYBpp\n",
       "5f38tgX996T9BbgWuB74P+Cxdq2XFqrhSNI7Rmvrvt0VK3HXdty7BAxlkiSNSCGMIz1E8Ang7Qz8\n",
       "b+YcUkD7ATFeOcA1rWtWNWwP3E16zybA1rESHy76viXQUG6pd10VSZJUVjG+RIzXE+M7ST1pPwRe\n",
       "6ufKKcAxwBWEcDoh1Ps+0MaaVYmzWHV+2XuKvF+3s6dMkqThKC3FsTfpYYH9gNfTuxJ/zR3Ae4jx\n",
       "r4U1oxpOJr1qCtK7Q7eLlTi3qPuVhD1lkiQpE+OLxPhHYqwS41uBiaS3EFySu2o34HZC+DghbFhQ\n",
       "S34G1ELfBOBLBd2n69lTJknSSJLeQPAx4Kv0zvWC9Iqn35Em5/+OGJe17JbVcAjpDQCQ/k3fO1bi\n",
       "n1pVfwk50V+SJNUphN1ILxN/dT9n5wM/Bv6XGB9oye2q4RLg8Gz3JmC/YfzycocvJUlSnWK8g/RC\n",
       "9Y8Bt/Q5OxH4F+CvhHAZIRxJCKObvOPH6X34YB/g/U3WN+zYUyZJkiCE7YH3ZtuW/VzxGPBd0nIa\n",
       "zzR0i2r4MvCZbHchsGusxIcaqavkHL6UJElNCmEU8BbgH0jDjX1H1ZYAZwJfI8bHh1R1NawD3A5s\n",
       "lx26GTggVuLyptpcPg5fSpKkJsXYQ4yXEeNRpBeMfwXIv1B8PGlo8yFCOIMQdiOEsXVVXYkvAscD\n",
       "K7JDewOfb13ju5s9ZZIkaXAhrAkcDfw/0jIafb1EeqXTnbntJmLsbwFbQjV8mhT2ap/dKlbiE61u\n",
       "dgfZUyZJkgoQ41Ji/BlpnbPDSE9P5o0jhbWTgP8BrgEeJIQTsuHQvr5K78MF40g9byOePWWSJGlo\n",
       "0lpnbwI+CuxBeiH6QO4AqsBviXHly1VUw1HARdnuQmDzWInPF9PgtrOnTJIktUGMMXtbwNHEuCWw\n",
       "AXAgadmLHwP5pzN3I4WvBwnhU4SwQXb8EuC+rLwu8JG2tL3E7CmTJEmtFcIE4F9Jc9DG9zm7FPg5\n",
       "8K0wg92As7PjTwNbxkpc2rZ2FseeMkmSVAIxLiDGz5GWvjgVeC53dk3gA8CdS77ASWuseLlXbRJp\n",
       "vtqIZSiTJEnFiPFxYvwMMAU4mTS/7GVrruQNn7yBjXOH3tLO5pWNoUySJBUrxiXEeDbp6c39SO/c\n",
       "XAHwlr/1XvaqBZxICHt2oollYCiTJEntkR4QuIEYjyUNbZ61zxxWrp2tZvbEBMY/NJFbCOFiQtih\n",
       "k03tBEOZJElqvxgfIca/X2Mlr9nhGV5+XdMVWwNwBHAXIXyZENbuVBPbzVAmSZI6J8YHb53M12q7\n",
       "l2/z8pmxpJeX30sIe3WgZW1nKJMkSZ12ea1w4Q4sf2S9l1f7B9gcuJwQXtf+ZrWXoUySJHXafWSv\n",
       "boqBsVv/CzNJr2yqLaWxHnAFIezYmea1h4vHSpKkjgvV8Hbgwmw3vXZpBlsAVwMTs+Nzgf2I8cEO\n",
       "NHEoXDxWkiR1rYuB+7Nyeu1SjHcBbwUWZMc3AX5JCOM60L7CGcokSVLHxUrsAb6aO/QvoRrWJMZb\n",
       "gUOBbOEMXkd6wfmwYyiTJEll8XN4eXmMScCJAMR4PfDZ3HWfJoTpbW1ZGxjKJElSKcRKXAZ8I3fo\n",
       "M6Ea1sjKpwNXZuUAXEII72hn+4pmKJMkSWXyPWB+Vt4K+DgAMfYA7weezc6tA1xACF8ghGGRZ4bF\n",
       "LyFJkoaHWIkLgUru0OdCNbwqnYyPAwcCD+fPAxcTwvi2NbIghjJJklQ2ZwB/ycprA195+UyMfwb2\n",
       "ILfgLHAYcFq7GlcUQ5kkSSqVWIkrgH/OHTohVMPrey+Iz5GeyPxa7pqPEMI729PCYrh4rCRJKqVQ\n",
       "DRcAtcn8twJ7Z0tnZBeEAPwSeFd25HlgF2J8rJ3t7IeLx0qSpGHlE8CyrLwHaaJ/r9Sz9CHg0ezI\n",
       "+sDPCGFMm9rXUoYySZJUSrESH2bVBWU/9sqL4vPAccDK7MgBwL8X3rgCGMokSVKZfT1X3ilUw5qv\n",
       "uCLGG4AZuSOfJ4S3FNyuljOUSZKk0oqV+DxQewH5aGDHAS79MjAzK48CzieEnYttXWsZyiRJUtnd\n",
       "niu/rt8rYlwJHA/MyY6sC/yeEKYU27TWMZRJkqSyuyNX3m3Aq2J8grRUxoLsyGRSMFuvuKa1jqFM\n",
       "kiSVXX2hDGqLy74TWJ4d2Qk4J1s+o9QMZZIkqezyoWznUF3Nkhcx/hH4QO7IIcBJBbSrpQxlkiSp\n",
       "1GIlzgUez3bHA9uv/kPxp6z66qXTCGFy61vXOoYySZLUDVbpLavzM/9B75Ob6wHfLfMwpqFMkiR1\n",
       "g4dz5c3q+kSMi1l1GPNw4NgWtqmlDGWSJKkbPJMrb1z3p2K8Fvjf3JFvEcKkVjWqlQxlkiSpGzQW\n",
       "ypLPALWXlG8A/E9LWtRihjJJktQN8qFsoyF9MsaFwAdzR95NCBu0olGtVHQoOxi4H3gA+HQ/5zcC\n",
       "LgXuBO6h79vfJUmSkmZ6yiDGy4Fbs70A7NWCNrVUkaFsNPBtUjCbRppYt0Ofa04hPU2xKzCd9NLR\n",
       "wdcekSRJI1FzoSy5KVfeu4m2FKLIULYX6THUR0ir6p4LHNXnmieBCVl5AvAssKLANkmSpO7UilB2\n",
       "c648okLZZGB2bn9Odizv+8BrgSeAu4B/LrA9kiSpez0H9GTl9UM1jG2gjnwo26tsa5YVGcpiHdf8\n",
       "G2k+2atIQ5jfIb3VXZIk6WWxEntII2o1mzZQzYOkcAewIXBAs+1qpSLnbz0OTM3tTyX1luXtC3wx\n",
       "K/+NtDDc9vROxMubkSvPzDZJkjRyPETv0OU/AZ8a0qdjjIRwGb0LyJ5FCLsS46Im2zU925oSYqyn\n",
       "Q6shY4BZwEGk4clbSH8I9+WuOQ14AagCk4DbSK9OeI5VRdKTEpIkaYQK1fAe4JxsdzkwLVbig4N8\n",
       "pJ9KwlTSig+1Oe3fIcZTWtbIpKHcUuTw5QrS05WXAfcCvyQFsg9nG8CXgD1I88muBP6VVwYySZIk\n",
       "SFnixqw8FvjvIdcQ42xSL1vNPxLCm5tvWvOK7ClrJXvKJEkSoRr2YtUJ+wfGSpw5tEpCAC4CjsyO\n",
       "PApsR4zLW9FGSthTJkmS1FKxEm8BfpY79I1QDaOHVkmMwIfoHZ3bAtinJQ1sgqFMkiR1m88CS7Ly\n",
       "rsD7hlxDjE8DF+SOHNR8s5pjKJMkSV0lVuIcVp1PNiNUG1pz7I+5sqFMkiSpAV8FFmflqcDEBuq4\n",
       "KlfehxDWabpVTTCUSZKkrhMrcRFpya2aTYZeSZwL3J3tjQHe0HzLGmcokyRJ3Wpurjz0UJaUZgjT\n",
       "UCZJkrpVPpQ1+pLyfCh7WxNtaZqhTJIkdatW9JRdAyzNyq8lhB2aa1LjDGWSJKlbNR/KYnwR+F3u\n",
       "yLubaVAzDGWSJKlbtaKnDOBXubKhTJIkaYhaFcp+R+/yGtMIYccm6mqYoUySJHWrZ3LlSQ3XEuMi\n",
       "4Le5I+9quK4mGMokSVK3eipX3qzJus7PlTvyFKahTJIkdas5ufKUBl+1VHMlELPynoTQyBsCmmIo\n",
       "kyRJXSlW4gJgQba7JrBB45XF54Dbsr1RwIFNNa4BhjJJktTN8r1lU5us64pc+c1N1jVkhjJJktTN\n",
       "ZufKU5qsKx/K3tJkXUNmKJMkSd1slXllTdZ1A71LY2xLCFs2Wd+QGMokSVI3y4eyLZqqKcZlwLW5\n",
       "I0c0Vd8QGcokSVI3uz9XPrgF9V2UK7d1dX9DmSRJ6ma/B5Zl5V1DNbymyfouAHqy8v6EMLnJ+upm\n",
       "KJMkSV0rWxYj/0LxY5urMD4DXJU7cnRT9Q2BoUySJHW7c3LlY5tcRBZWfUH5MU3WVTdDmSRJ6na/\n",
       "AxZm5e2A1zVZ34XAyqz8ekLYvMn66mIokyRJXS1W4hJWnaDf7BDmPNJrl2ra8oJyQ5kkSRoOzs2V\n",
       "396C+vIvKN+/BfWtlqFMkiQNB3+kd8hxm1ANazVZ3z25crOvb6qLoUySJHW9WInLgIdyh17dZJWt\n",
       "fFNAXQxlkiRpuMgvJNvsemVP0rte2SRCWKPJ+lbLUCZJkoaL1oWyGFeQgllN4YvIGsokSdJw0cqe\n",
       "MoDHc+XNWlDfoAxlkiRpuGh1KFuWK49uQX2DMpRJkqThIh/Ktg/VUHiQaiVDmSRJGhZiJT4HzM12\n",
       "1wTashJ/qxjKJEnScNLqIcy2MZRJkqThZFauvG3HWtEAQ5kkSRpOFubK4zrWigYYyiRJklZvTNE3\n",
       "MJRJkiT177Fcee+ib2YokyRJ6t9lufKhRd/MUCZJktS/y4CYlV9PCOsXeTNDmSRJUn9inAvcmu2N\n",
       "Bt5U5O0MZZIkSQO7MVfessgbGcokSZIG9myuvEGRNzKUSZKk4Sq0oI7ncmVDmSRJUp0W5MqtmJhv\n",
       "KJMkSWrAU7nypi2oz1AmSZLUgKdzZUOZJElSh+R7yia1oD5DmSRJUgNa3VM2P1c2lEmSJNUpH8om\n",
       "hWpo9gnM5+ld1X89QijsxeSGMkmSNGzESlxM7xOYY4GJzVUYV7LqEOaGTdU3CEOZJEkabvK9ZVu1\n",
       "oL55ufJGLaivX4YySZI03MzKlU8P1aaHHJ/JlTdusq4BGcokSdJw80VgZVbeH/i3Juuzp0ySJGmo\n",
       "YiXeBMzIHaqEativiSrtKZMkSWrQl4Frs/Io4OehGhp97ZI9ZZIkSY2IlbgSeC9pSQuALYDvNFhd\n",
       "vqdsSjPtGoyhTJIkDUuxEmcDH8wdOi5UQyPDj/fmyrs116qBGcokSdKwFSvxPOCe3KEtGqjmtlx5\n",
       "F0IY11yr+mcokyRJw92cXHnykD8d4zzgkWxvHPDa5pv0SoYySZI03D2eK7+qwTr+lCvv0URbBmQo\n",
       "kyRJw10+lA29pyy5NVfes4m2DMhQJkmShrsncuVGQ1l+XtqWjTdlYIYySZI03LVi+HJ5rhyaaMuA\n",
       "DGWSJGm4a8XwZeGKDmUHA/cDDwCfHuCa6cAdpG7BmQW3R5IkjTyt6CkrXJGhbDTwbVIwmwYcC+zQ\n",
       "55r1SavrHgHsCBxdYHskSdLINA9YkZUnhmoY38nGDKTIULYX8CBpXY/lwLnAUX2uOQ44n971Q+Yh\n",
       "SZLUQrESe4Cncoc2baCa/Jyy9ZprUf+KDGWTgdm5/Tm8chx3O2AD4GrSo6YnFNgeSZI0cuWfwGxk\n",
       "CDP/9OWuhNb3thUZymId14wFXgccCrwN+A9SUJMkSWql5kJZWtV/VrY3Fti9BW1axZhWV5jzODA1\n",
       "tz+VVV9zAKknbR6wJNuuBXYhPRjQ14xceSY+FCBJkur3ZK7c6GT/G4Dts/K+wP9l5enZ1pQiQ9mt\n",
       "pF6vLUnp9BjSZP+835AeBhgNrAHsDZw2QH0zimikJEkaEfI9ZZs1WMf1wElZeb/c8Zms2llUaaTy\n",
       "IkPZCuAU4DJS6DoLuA/4cHb+TNJyGZcCdwM9wPeBewtskyRJGpmanVMGqaesZl9CCMRYz3StuoQW\n",
       "1lWkSEGr50qSpOEvVMPhwCXZ7mWxEg8eeiVhFGna1cTsyPbE+Nd+rmwot7iivyRJGgmeyZU3bqiG\n",
       "GHuAG3NHDmimQX0ZyiRJ0kjQfChLrsmVj2uinlcwlEmSpJFglVAWqqHRaVG/IM2DB3gTIWzTXLN6\n",
       "1RPK9geuIC1T8XC2PdSqBkiSJLXBi8CyrLwmsHZDtcQ4B/hD7sjJzTWrVz2h7CzSMhX7A3tm216t\n",
       "aoAkSVLRYiVGWjeE+f1c+SRCaMlqFvWEsudJifBp0hMHtU2SJKmbtCqU/Z7ed2luBuzWRF0vqyeU\n",
       "XQ18FXg96ZVItU2SJKmbtCaUxbgcuCN3pNHFaFdRT3fbPqT1Nvboc/zAVjRAkiSpTVrVUwYwt4V1\n",
       "AfWFsumtuJEkSVKHPZ0rb9pkXflQtkmTdQH1DV+uD3wDuC3bvg6s14qbS5IktdHsXHlqk3V1JJT9\n",
       "EFgAvAt4N7AQOLsVN5ckSWqjOblys6GslUOhQH3Dl9sA78ztzwDuasXNJUmS2ijfUzalybo60lO2\n",
       "hFXf7bQ/sLgVN5ckSWqjVg5fPpsrb9BkXUB9PWUfAX5C7zyy+cD7WnFzSZKkNnoaWEHKPxuFahgf\n",
       "K3FJg3WtzJUbfWXTKurpKbsT2BnYKdt2xeFLSZLUZWIlrgSeyB3aoVNt6c9gPWUnAD8FPkFap6wm\n",
       "ZPunFdguSZKkItwFbJ6Vvx6q4U3ZK5g6brCesrWyn+sOsEmSJHWbGfQOPU6nhS8Ub1aI5QiHqxNp\n",
       "0XitJEka2UI1/DfwqWz3eWCHWIlPDfKRfioJuwO3Znt3EGP+FZQN5ZZ65pT9NzABGAv8kfQy8hOG\n",
       "eiNJkqTvj24EAAAUcUlEQVSSmAE8lJXXB77ZQB3P5crTCGH9ZhtVTyh7G2nx2MOBR0jrln1qsA9I\n",
       "kiSVVazExcCHc4feFarhyKFVEh8mPQwJsAZpkf2m1BPKag8DHA6cB7zAqhP/JUmSukqsxCuBH+UO\n",
       "/W+ohglDrObHufKJzbapnlB2CXA/sDtp+HITYGmzN5YkSeqwT9L7uqTJwBeG+Plz6H1oYH9C2KaZ\n",
       "xtQ70X9D0kS4lcDapKcvhzYhrjlO9JckSS0XquE9pHAFsAjYNFbii/VXEC4hjSYCzCDGKgVM9D8o\n",
       "+/l3wBuBo7LywcC+Q72RJElSCf0SuDcrrw28Y4if/0mufCIhNNyJNFgoe0P284jcdni2HdHoDSVJ\n",
       "ksoiWzj2p7lDQ11h4hLSA5EAWwNbNNoW1ymTJEkjWqiGqcCj9L61aGqsxMfrryD8Cdgj29uXGG+g\n",
       "oHXKvkRaw6NmIvBfQ72RJElSGcVKnA1cne0G4PghVvF0rjyp0XbUE8oOJU3yr5kPHNboDSVJkkpo\n",
       "lblhoTqkuWFtC2WjgDVz++OBcY3eUJIkqYQuABZn5dcC+w/hs20LZT8nrU/2AeDvgStZNU1KkiR1\n",
       "tViJC+ldGgPgc0P4eNtC2amkOWQ7AK8B/jM7JkmSNJycCvRk5beGatirzs/lQ9nkRm9eTygDuA+4\n",
       "jLTy7XWkxWMlSZKGjViJDwDn5g7V21t2f658QKP3ryeUfQj4NfDdbH8KcFGjN5QkSSqxL9L7ju8j\n",
       "QjXsWsdn7gKezMobNHrjekLZP5Imu9UWRvsr6f2XkiRJw0qsxHuB83OH/n31H4oR+H2z964nlC3L\n",
       "tpox9CZISZKk4eaLufLfhWqYVsdn2hLKriGlxLWAt5CGMi9p9saSJEllFCvxTnqzTqCe3jK4vtn7\n",
       "1vOapVGkpTDemu1fBvyA9vaW+ZolSZLUNtmTlzdnuyuACbESlwz8gTCaNLI4mpSthpxbVhfKxgD3\n",
       "kJbC6CRDmSRJaqtQDY8BU7PdbWIlPjT4B8ITwGaNhrLVDV+uAGbRxBvPJUmSutSTufJmdVz/VDM3\n",
       "G1PHNRsAfwFuARZlxyJwZDM3liRJKrl8yCpFKKstnJbvhvPpS0mSNNzle8o2reP6wkLZeOAjwLbA\n",
       "3cAPgeXN3EySJKmLtLWnbLA5ZT8GdicFskOBrzVzI0mSpC4z1J6y24ELGr3ZYD1lOwA7ZeWzgD81\n",
       "ehNJkqQuNLSJ/jGeB5xHg9O8BuspWzFAWZIkaSTID0e+JlRDoctzDRbKdgYW5radcuUFg3xOkiRp\n",
       "OPgLKfcAbAW8ucibDRbKRgPr5rYxufKEIhslSZLUabESFwFn5w79U5H3q+fdl5IkSSPVt3Plw0I1\n",
       "bFPUjQxlkiRJA4iV+ADw+2w3AP9Y1L0MZZIkSYP7Zq78gVANaxVxE0OZJEnS4K4AHsjKE4A3FnET\n",
       "Q5kkSdIgYiX2AL/NHXpLEfcxlEmSJK3eFbmyoUySJKlDrqX3HeA7hmqo57VLQ2IokyRJWo1szbIb\n",
       "codavpCsoUySJKk++SFMQ5kkSVKHXJUrH9Dqyg1lkiRJ9bkdWJaVt271vDJDmSRJUh1iJS4D/pQ7\n",
       "tG8r6zeUSZIk1e/6XHm/VlZsKJMkSapfPpTZUyZJktQhN+bKu4dqGN+qig1lkiRJdYqVOA+Yle2O\n",
       "BXZuVd2GMkmSpKF5Kle2p0ySJGk4KTqUHQzcDzwAfHqQ6/YEVgDvLLg9kiRJpVRkKBsNfJsUzKYB\n",
       "xwI7DHDdqcClQCiwPZIkSaVVZCjbC3gQeIT0VvVzgaP6ue5jwHnAMwW2RZIkqdSKDGWTgdm5/TnZ\n",
       "sb7XHAWcke3HAtsjSZJUWkWGsnoC1unAZ7JrAw5fSpKk8luUK2/eqkrHtKqifjwOTM3tTyX1luXt\n",
       "ThrWBNgIOIQ01HlxP/XNyJVnZpskSVK7XQ8cmpXfBDwGTG+20hBjYSOGY0iLqx0EPAHcQprsf98A\n",
       "158NXAJc0M+5Wk+aJElSR4Vq2Ifelf1nA1vEyiqBqqHcUuTw5QrgFOAy4F7gl6RA9uFskyRJ6ka3\n",
       "Aguz8lRg21ZUWmRPWSvZUyZJkkojVMMlwOHZ7kdiJZ6ZO126njJJkqTh6qpc+aBWVGgokyRJGro/\n",
       "5spvC9WwRrMVGsokSZKG7s/Aw1l5Ai3oLTOUSZIkDVH2tOX5uUNHN1unoUySJKkx+VB2VKiGsc1U\n",
       "ZiiTJElqzC2kxfIBNgAOaKYyQ5kkSVIDYiX2AL/NHdqrmfoMZZIkSY27PVfepZmKDGWSJEmNuzNX\n",
       "3rWZigxlkiRJjbsH6MnKrw7VsG6jFRnKJEmSGhQrcTEpmEHKVe9ttC5DmSRJUnPOypX/pdFKDGWS\n",
       "JEnNORtYkJVf3WglhjJJkqQmxEpcCPyg2XoMZZIkSc27sdkKDGWSJEnNi81WYCiTJEkqAUOZJElS\n",
       "CRjKJEmSmvdwsxUYyiRJkpp3N7C4mQoMZZIkSU2KlbgCuKWZOgxlkiRJrdHUshiGMkmSpNa4Hri/\n",
       "0Q+HGJteVqMdIhA63QhJkqQ6NJRb7CmTJEkqAUOZJElSCRjKJEmSSsBQJkmSVAKGMkmSpBIwlEmS\n",
       "JJWAoUySJKkEDGWSJEklYCiTJEkqAUOZJElSCRjKJEmSSsBQJkmSVAKGMkmSpBIwlEmSJJWAoUyS\n",
       "JKkEDGWSJEklYCiTJEkqAUOZJElSCRjKJEmSSsBQJkmSVAKGMkmSpBIwlEmSJJWAoUySJKkEDGWS\n",
       "JEklYCiTJEkqAUOZJElSCRjKJEmSSsBQJkmSVAKGMkmSpBIwlEmSJJWAoUySJKkEDGWSJEklYCiT\n",
       "JEkqAUOZJElSCRjKJEmSSsBQJkmSVAKGMkmSpBIwlEmSJJWAoUySJKkEDGWSJEklYCiTJEkqgXaE\n",
       "soOB+4EHgE/3c/544C7gbuB6YOc2tEmSJKlUQoyxyPpHA7OANwOPA38CjgXuy13zeuBe4AVSgJsB\n",
       "7NOnngiEIhsqSZLUIg3llqJ7yvYCHgQeAZYD5wJH9bnmRlIgA7gZmFJwmyRJkkqn6FA2GZid25+T\n",
       "HRvIB4DfF9oiSZKkEhpTcP1DGRs9EDgZ2K+gtkiSJJVW0aHscWBqbn8qqbesr52B75PmlM0foK4Z\n",
       "ufLMbJMkSeq06dnWlKIn+o8hTfQ/CHgCuIVXTvTfHLgKeC9w0wD1ONFfkiR1i4ZyS9E9ZSuAU4DL\n",
       "SE9inkUKZB/Ozp8JfB6YCJyRHVtOekBAkiRpxCi6p6xV7CmTJEndopRLYkiSJKkOhjJJkqQSMJRJ\n",
       "kiSVgKFMkiSpBAxlkiRJJWAokyRJKgFDmSRJUgkYyiRJkkrAUCZJklQChjJJkqQSMJRJkiSVgKFM\n",
       "kiSpBAxlkiRJJWAokyRJKgFDmSRJUgkYyiRJkkrAUCZJklQChjJJkqQSMJRJkiSVgKFMkiSpBAxl\n",
       "kiRJJWAokyRJKgFDmSRJUgmM6XQDJElS8UIIsdNtGI5ijKFVdRnKJEkaIVoZINT6oOvwpSRJUgkY\n",
       "yiRJkkrAUCZJklQChjJJklRaIYQzQgifa0E97w8hXNeKNhXFif6SJKmjQgiPAJsAK4HlwA3AR2KM\n",
       "c2KMH+1k29rJnjJJktRpETg8xrgusBnwNPCtzjapVwhhdDvuYyiTJEmlEWNcBpwPTAMIIfwohPCF\n",
       "rDw9hDAnhPDZEMIzIYSHQwjH1T4bQlgvhPCTEMLcEMIjIYR/DyH0uwxICOF/QgiPhRBeCCHcGkLY\n",
       "P3duRgjhvBDCT0MILwDvK/SXzhjKJElSGQSAEMJawDHAjdnxmG01k4ANgVeRwtL3Qgivzs59C1gX\n",
       "2Ap4I3AicNIA97sF2AWYCPwC+HUIYVzu/JHAr2OM62XnC2cokyRJEEJs6TbEuwMXhRDmA88DBwFf\n",
       "63M+7z9ijMtjjNcCvwPenQ0xHgN8Nsa4KMb4KPB14IT+bhhj/HmMcX6MsSfGeBqwBrB97pIbYowX\n",
       "Z9cuHeLv0xBDmSRJ6rQIHBVjnEgKRx8DrgkhTOrn2vkxxiW5/UdJ89A2BMZm+zWPAZP7u2EI4ZMh\n",
       "hHtDCM9nYXA9YKPcJXMa/m0aZCiTJEmlEZMLSU9i7t/PJROzIc6aLYAngHmkJze3zJ3bnH7CVQjh\n",
       "AOBTwLtijOtnYfAFVu2Ra/u7Qg1lkiQJYgwt3YauNqcshBCOAtYH7uWVQ5cA1RDC2CxcHUaa+9UD\n",
       "/Ar4YghhnRDCFsDHgZ/18/l1gRXAvBDCuBDC54EJDbS5pVynTJIklcElIYSVpB6qR4D3xRjvy176\n",
       "ne+1egqYT+odWwR8OMb41+zcx0iT/R8ClgLfA87OzuXruTTb/prV8Q3SUCf9XNs2Ica237MRkf6T\n",
       "siRJqkMIIcbGerBKI4QwHfhpjHFqp9sCg/6ZNpRbHL6UJEkqAUOZJEnqJl0xxNcIhy8lSRoBhsPw\n",
       "Zdk4fClJkjQMGcokSZJKwFAmSZJUAoYySZKkEjCUSZIklYChTJIkqQQMZZIkqaNCCI+EEBaHEBaG\n",
       "EJ4LIfw2hDAlO/ejEMKy7NyzIYTLQwjbZ+dmhBCWhxAWZNusEMK3QgibdvY3aoyhTJIkdVoEDo8x\n",
       "rgtsBjxNeodl7dyp2bkpwFzgR7nPnhNjnABMBN4BbArc1o3BzFAmSZJKI8a4DDgfmJYdCrlzS4Bz\n",
       "gB1zHwnZuZUxxnuBY4BngE+0pcEtZCiTJEllEABCCGuRgtWN2fGYO7cOcDxw+0CVxBh7gN8ABxTZ\n",
       "2CKM6XQDJElS54VqaOl7F2NlSK90CsBFIYQVwNqkIcqDc+c+GUI4BVgK3Ay8fzX1PQlsMKQGl4Ch\n",
       "TJIkdVoEjooxXhVCCMDbgWtCCNOyc1+NMX5+CPVNBp4toJ2FcvhSkiSVRkwuBFYC+2eHB+p1e0Xv\n",
       "XghhFHAEcF0xLSyOPWWSJGmow41FqM0bC8CRwPrAvaSANehnss+NAbYDZgCbAKcV1dCiGMokSVIZ\n",
       "XBJCWEnq/XoEeF+M8b4QQqSfHrFMBI4JIbydFNCeAC4Hdo8xPtWGNrdUiLGl8/qK8vKTF5IkaehC\n",
       "CDHGjveGDSuD/Jk2lFucUyZJklQChjJJkqQSMJRJkiSVgKFMkiSpBAxlkiRJJWAokyRJKgFDmSRJ\n",
       "Ugm4eKwkSSNEthCrSqroUHYwcDowGvgBcGo/13wTOARYTHrr+x0Ft0mSpBHHhWPLr8jhy9HAt0nB\n",
       "bBpwLLBDn2sOBbYlvavqQ8AZBbZHnTO90w1Qw6Z3ugFqyvRON0ANm97pBqj9igxlewEPkt5ftRw4\n",
       "FziqzzVHAj/OyjeTXj46qcA2qTOmd7oBatj0TjdATZne6QaoYdM73QC1X5GhbDIwO7c/Jzu2umum\n",
       "FNgmSZKkUioylNU7mbDvGLeTECVJ0ohT5ET/x4Gpuf2ppJ6wwa6Zkh3rj2Gtu1U63QA1zO+uu/n9\n",
       "dS+/uxGmyFB2K2kC/5bAE8AxpMn+eRcDp5Dmm+0DPA883U9dPjEiSZKGtSJD2QpS4LqM9CTmWcB9\n",
       "wIez82cCvyc9gfkgsAg4qcD2SJIklVaI0VFBSZKkTivba5YOBu4HHgA+PcA138zO3wXs1qZ2afVW\n",
       "990dT/rO7gauB3ZuX9NUh3r+2wPYk9QL/s52NEp1qee7m05amPseYGZbWqV6re772wi4FLiT9P29\n",
       "v20t0+r8kDTl6s+DXDOkzFKmUOZis92rnu/uIeANpDD2BeB77WygBlXP91e77lTSPxDO8yyHer67\n",
       "9YHvAEcAOwJHt7OBGlQ9398ppEC9Kylcfx1fkVgWZ5O+u4EMObOUKZS52Gz3que7uxF4ISvfjOvR\n",
       "lUk93x/Ax4DzgGfa1jKtTj3f3XHA+fQ+/T6vXY3TatXz/T0JTMjKE4BnSb3V6rzrgPmDnB9yZilT\n",
       "KHOx2e5Vz3eX9wHSQx4qh3r/2zuK3v/TczJqOdTz3W0HbABcTXoq/oT2NE11qOf7+z7wWtIqBncB\n",
       "/9yepqkFhpxZytQF6mKz3Wso38GBwMnAfgW1RUNXz/d3OvCZ7NqAw5dlUc93NxZ4HXAQsBap1/om\n",
       "0jwXdVY939+/keaTTQe2Aa4AdgEWFtcstdCQMkuZQlmrF5tV+9Tz3UGaT/Z90hj8YF2+aq96vr/d\n",
       "SUMrkCYeH0Iabrm48NZpMPV8d7NJQ5ZLsu1a0j/qhrLOq+f72xf4Ylb+G/AwsD2p11PlNuTMUqbh\n",
       "y/xis+NIi832/Qv/YuDErDzYYrNqr3q+u82BC4D3kuZQqDzq+f62BrbKtvOAj/Zzjdqvnu/uN8D+\n",
       "pEnlawF7A/e2r4kaRD3f3/3Am7PyJFIge6hN7VNzhpxZytRT5mKz3aue7+7zwER65yQtJ01yVefV\n",
       "8/2pnOr57u4nPTF7N9BD6q02lJVDPd/fl0hP+d1F6kj5V+C5trdU/TkHeCNp9GA26bVYY7NzDWUW\n",
       "F4+VJEkqgTINX0qSJI1YhjJJkqQSMJRJkiSVgKFMkiSpBAxlkiRJJWAokyRJKgFDmaThYCVwB2kt\n",
       "rguAdVpc/yOk90cCvNjiuiUJMJRJGh4WA7uRXuW1gN7FN1slDlCWpJYxlEkabm4kvbiZ7OcfSK+z\n",
       "uZb0ihpIr6u5kPSi5ztJr0AhO3YrcA/wwTa1V5KAcr1mSZKaNRp4K/DHbP97pF6zB0nvfPxf4CDg\n",
       "m8DVwDtI/3NaG+48GZgPjAduIb3nc36b2i5phPM1S5KGgxXAn4HJpPlf+5Bevj0XmJW7bhzw2uz4\n",
       "ZNI7WPNmAG/PyluSAt4twMPA7qR3Di4E1m35byBpxLOnTNJwsIQ0p2w86eXORwFXAs9nx/sT+uxP\n",
       "J/Wi7QMsJfWkrVlAWyWpX84pkzScLAH+Cfgi6SnJh4Gjs3OB9CAApOHNj2bl0cCEbJtPCmSvoXee\n",
       "mSS1haFM0nCQn4dxJ2kO2buB44EPZMfuAY7Mrvln4EDSEhq3AjsAl5JGD+4Fvkx6YGB195KklnFO\n",
       "mSRJUgnYUyZJklQChjJJkqQSMJRJkiSVgKFMkiSpBAxlkiRJJWAokyRJKgFDmSRJUgkYyiRJkkrg\n",
       "/wPXrnVg7eSWaQAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4a26f6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mental illness', 'health', 'medication', 'depression http', 'mental', 'mentalhealth', 'helpful', 'hospital', 'health issues', 'meds', 'manic', 'depression', 'know feeling', 'effects', 'dealing', 'harm', 'help', 'depressive', 'cope', 'natasha_tracy', 'mindfulness', 'thoughts', 'diagnosed bipolar', 'ptsd', 'unstable', 'anxiety', 'psychiatrist', 'thanks', 'reading', 'suffering', 'supportive', 'living bipolar', 'game', 'triggered', 'hard time', 'illness', 'nurse_w_glasses', 'people bipolar', 'bipolar disorder', 'sorry hear', 'bipolardisorder', 'link', 'triggers', 'psychologist', 'insomnia', 'bipolarblogger', 'stigma', 'panic attacks', 'totally', 'keeping']\n",
      "['disorder', 'anxiety', 'mental', 'suicidal', 'meds', 'personality', 'health', 'bpd', 'bit', 'patients', 'helps', 'trigger', 'bpdffs', 'diagnosis', 'try', 'therapy', 'psych', 'borderline', 'bpd http', 'triggers', 'blog', 'self', 'illness', 'selfharm', 'harm', 'thoughts', 'help', 'sufferers', 'girl_interrupt_', 'bpdchat', 'overwhelming', 'exhausted', 'help feel', 'recovery', 'unstable', 'dollar', 'help need', 'nurses', 'hugs', 'mindfulness', 'mindful', 'mentally ill', 'recently', 'lovely', 'new meds', 'new blog', 'suffer', 'hosp', 'self soothe', 'just realised']\n"
     ]
    }
   ],
   "source": [
    "#Tf-iDF performance, really robust\n",
    "\n",
    "\n",
    "\n",
    "def XYGenerator(labeled_data):\n",
    "    X, Y = labeled_data[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for x, y in labeled_data[1:]:\n",
    "        try:\n",
    "            \n",
    "            X = np.concatenate((X, x))\n",
    "        except:\n",
    "            X = vstack([X, x])\n",
    "            \n",
    "        Y = np.concatenate((Y, y))\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "class TextFeatureGroups(object):\n",
    "    \n",
    "    def __init__(self, base, groups, model = \"Tf-iDF\"):\n",
    "        self.base = base\n",
    "        self.groups = groups\n",
    "        self.name_to_index = {group.getName(): index for index, group in enumerate(groups)}\n",
    "       \n",
    "        base_texts = base.getTexts()\n",
    "        self.text_models = []\n",
    "        self.classifiers = []\n",
    "        for group in groups:\n",
    "            group_texts = group.getTexts()\n",
    "            total_texts = base_texts  + group_texts\n",
    "            if model == \"Tf-iDF\":\n",
    "                text_model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "            text_model.fit(total_texts)    \n",
    "            X = text_model.transform(total_texts)  \n",
    "            Y = np.array([0] * base.getSize() + [1]*group.getSize(), dtype=int)\n",
    "            classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "            classifier.fit(X,Y)\n",
    "            self.classifiers.append(classifier)\n",
    "            self.text_models.append(text_model)\n",
    "    \n",
    "    \n",
    "    def getTopFeatures(self, name, k=50):\n",
    "        index = self.name_to_index[name]\n",
    "        text_model = self.text_models[index]\n",
    "        classifier = self.classifiers[index]\n",
    "        feature_indicies = np.argsort(classifier.feature_importances_)\n",
    "        vector2word = text_model.get_feature_names()\n",
    "        top_words = []\n",
    "        for i in range(k):\n",
    "            vector = feature_indicies[-i-1]insomnia\n",
    "            word = vector2word[vector]\n",
    "            top_words.append(word)\n",
    "        return top_words\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def getGroup(self,name):\n",
    "        index = self.name_to_index[name]\n",
    "        return self.group[index]\n",
    "            \n",
    "    def classify(self,name, group):\n",
    "        index = self.name_to_index[name]\n",
    "        classifier = self.classifiers[index]\n",
    "        text_model = self.text_models[index]\n",
    "        X = text_model.transform(group.getTexts())\n",
    "        return classifier.predict(X) \n",
    "        \n",
    "    \n",
    "            \n",
    "    def renderPrecisionRecall(self,colors=['r','g','b',\"y\",\"w\"], n_fold = 30):\n",
    "        classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.spines['bottom'].set_color('white')\n",
    "        ax.spines['top'].set_color('white') \n",
    "        ax.spines['right'].set_color('white')\n",
    "        ax.spines['left'].set_color('white')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        ax.title.set_color('white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.xaxis.label.set_color('white')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall Curve')\n",
    "\n",
    "        for g, group in enumerate(self.groups):\n",
    "\n",
    "            SCORES = np.array([])\n",
    "            LABELS = np.array([])\n",
    "            \n",
    "            \n",
    "            X = self.text_models[g].transform(self.base.getTexts()+group.getTexts())\n",
    "            Y =  np.zeros(self.base.getSize() + group.getSize(), dtype=int)\n",
    "            Y[self.base.getSize():] = 1\n",
    "            \n",
    "            sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "\n",
    "            for train_index, test_index in sss:\n",
    "\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "                classifier.fit(X_train, Y_train)\n",
    "                score = classifier.predict_proba(X_test)[:,1]\n",
    "                SCORES = np.concatenate((SCORES, score))\n",
    "                LABELS = np.concatenate((LABELS, Y_test))\n",
    "\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "            average_precision = average_precision_score(LABELS, SCORES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            positive_train = np.count_nonzero(Y_train)\n",
    "            positive_test = np.count_nonzero(Y_test)\n",
    "            normal_train = Y_train.shape[0] - positive_train\n",
    "            normal_test = Y_test.shape[0] - positive_test\n",
    "\n",
    "\n",
    "            print(\"{} Normal and {} {} in Training Data\".format(normal_train,positive_train,group.getName()))\n",
    "            print(\"{} Normal and {} {} in Test Data\\n\".format(normal_test,positive_test,group.getName()))\n",
    "            plt.plot(recall, precision, label=group.getName() , color=colors[g],linewidth=3)\n",
    "\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "tfidf_groups = TextFeatureGroups(regular_group, [bipolar_all,BPD_all])\n",
    "tfidf_groups.renderPrecisionRecall()\n",
    "bipolar_words = tfidf_groups.getTopFeatures(\"Bipolar\")\n",
    "BPD_words = tfidf_groups.getTopFeatures(\"BPD\")\n",
    "\n",
    "print(bipolar_words)\n",
    "print(BPD_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The Tf-iDF false positive experiment on disorder experts =====\n",
      "\n",
      "9 out of 13 BPD Expert experts are mis-calssified as patients. Accuracy 0.3076923076923077\n",
      "10 out of 11 bipolar Expert experts are mis-calssified as patients. Accuracy 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "#Expert experiemnt of Tf-iDF kinda sucks\n",
    "bipolar_experts =  Group(\"BPD Expert\", dbName= \"patients\", collectionName=\"bipolar_experts\")\n",
    "BPD_experts =  Group(\"bipolar Expert\", dbName=\"idea\", collectionName=\"coach_tweets_emotion\")\n",
    "\n",
    "\n",
    "\n",
    "bipolar_result = tfidf_groups.classify(\"Bipolar\", bipolar_experts)\n",
    "BPD_result = tfidf_groups.classify(\"BPD\", BPD_experts )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== The Tf-iDF false positive experiment on disorder experts =====\\n\")\n",
    "\n",
    "misclassified = np.sum(bipolar_result)\n",
    "size = bipolar_experts.getSize()\n",
    "name = bipolar_experts.getName()\n",
    "\n",
    "print(\"{} out of {} {} experts are mis-calssified as patients. Accuracy {}\".format(np.sum(bipolar_result), size, name, (size-misclassified)/size))\n",
    "misclassified = np.sum(BPD_result)\n",
    "size = BPD_experts.getSize()\n",
    "name = BPD_experts.getName()\n",
    "print(\"{} out of {} {} experts are mis-calssified as patients. Accuracy {}\".format(np.sum(BPD_result), size, name, (size-misclassified)/size))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The cross-valadation experiment on disorder experts =====\n",
      "\n",
      "142 out of 203 BPD experts are mis-calssified as patients. Accuracy 0.30049261083743845\n",
      "152 out of 278 Bipolar experts are mis-calssified as patients. Accuracy 0.45323741007194246\n"
     ]
    }
   ],
   "source": [
    "#Cross classification of Tf-iDF\n",
    "#tfidf_groups_clean = TextFeatureGroups(regular_group, [bipolar_group_clean,BPD_group_clean])\n",
    "#tfidf_groups_clean.renderPrecisionRecall()\n",
    "\n",
    "bipolar_result = tfidf_groups_clean.classify(\"Bipolar\", BPD_group_clean)\n",
    "BPD_result = tfidf_groups_clean.classify(\"BPD\", bipolar_group_clean)\n",
    "\n",
    "#print(bipolar_result)\n",
    "#print(BPD_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== The cross-valadation experiment on disorder experts =====\\n\")\n",
    "\n",
    "misclassified = np.sum(bipolar_result)\n",
    "size = BPD_group_clean.getSize()\n",
    "name = BPD_group_clean.getName()\n",
    "\n",
    "print(\"{} out of {} {}  are mis-calssified as {} Accuracy {}\".format(np.sum(bipolar_result), size, name, (size-misclassified)/size))\n",
    "misclassified = np.sum(BPD_result)\n",
    "size = bipolar_group_clean.getSize()\n",
    "name = bipolar_group_clean.getName()\n",
    "print(\"{} out of {} {}  are mis-calssified as {} Accuracy {}\".format(np.sum(BPD_result), size, name, (size-misclassified)/size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Individual pattern of life feature performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class POLFeaturerGroups(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform(self, ):\n",
    "        features_dict = getAllFeature(group)\n",
    "        \n",
    "    \n",
    "    def __init__(self, base, groups):\n",
    "        feature2categories = getCategories(base)\n",
    "        self.categories = set(feature2categories.values())\n",
    "        self.name_to_index = {group.getName(): index for index, group in enumerate(groups)}\n",
    "        self.base_name = base.getName()\n",
    "        self.features = []\n",
    "        self.index2feature = []\n",
    "        base_feature_dict = getAllFeature(base)\n",
    "        self.base_feature = np.zeros((base.getSize(),len(base_feature_dict)), dtype = float)\n",
    "        for name, values in base_feature_dict.items():\n",
    "            cursor = len(self.index2feature)\n",
    "          \n",
    "            self.base_feature[:,cursor] = values\n",
    "            category = feature2categories[name]\n",
    "            self.index2feature.append((name, category))\n",
    "        \n",
    "\n",
    "        for group in groups:\n",
    "            features_dict = getAllFeature(group)\n",
    "            feature = np.zeros((group.getSize(),len(self.index2feature)),dtype=float)\n",
    "            for cursor, (name,category) in enumerate(self.index2feature):\n",
    "                value = features_dict[name]\n",
    "                feature[:,cursor] = value\n",
    "            self.features.append(feature)\n",
    "            \n",
    "    def getLabeledData(self,name, selection = \"all\"):\n",
    "        if selection is \"all\":\n",
    "            selection = range(len(self.index2feature))\n",
    "        \n",
    "        base_feature = self.base_feature[:,selection]\n",
    "        base_length = base_feature.shape[0]\n",
    "        index = self.name_to_index[name]\n",
    "        feature = self.features[index][:,selection]\n",
    "\n",
    "        X = np.zeros((base_length  + feature.shape[0],len(selection)),dtype=float)\n",
    "        X[0:base_length ,:] = base_feature\n",
    "        X[base_length:,:] = feature\n",
    "        Y = np.array([0]*base_length  + [1]*feature.shape[0])\n",
    "        \n",
    "        return X, Y\n",
    "        \n",
    "    \n",
    "    def getFeatureImportance_all(self,name):\n",
    "        X, Y = self.getLabeledData(name)\n",
    "        classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        classifier.fit(X,Y)\n",
    "        features_indicies = np.argsort(classifier.feature_importances_)\n",
    "        feature_names = [self.index2feature[i] for i in features_indicies ]\n",
    "        return feature_names[::-1]\n",
    "    \n",
    "    def getFeature(self, name, selection=\"all\"):\n",
    "        if selection is \"all\":\n",
    "            selection = list(range(len(self.index2feature)))\n",
    "        index = self.name_to_index[name]\n",
    "        feature =  self.features[index]\n",
    "        return feature[:, selection]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def classify(self,name, target_name, selection=\"all\"):\n",
    "        X, Y = self.getLabeledData(name, selection)\n",
    "        classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        classifier.fit(X,Y)\n",
    "        test_x = self.getFeature(target_name,selection)\n",
    "        return classifier.predict(test_x)\n",
    "    \n",
    "    def renderPrecisionRecall_all(self, name,colors=['r','g','b',\"y\",\"w\", \"m\"], n_fold = 30):\n",
    "        classifier  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.spines['bottom'].set_color('white')\n",
    "        ax.spines['top'].set_color('white') \n",
    "        ax.spines['right'].set_color('white')\n",
    "        ax.spines['left'].set_color('white')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "        ax.title.set_color('white')\n",
    "        ax.yaxis.label.set_color('white')\n",
    "        ax.xaxis.label.set_color('white')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall Curve: {}'.format(name))\n",
    "        \n",
    "        \n",
    "        categories = list(self.categories) + [\"All\"]\n",
    "        for g,category in enumerate(categories):\n",
    "            if category == \"All\":\n",
    "                selection = range(len(self.index2feature))\n",
    "            else:\n",
    "                selection = [i for i, (feature_name, feature_category) in enumerate(self.index2feature) if category == feature_category]\n",
    "        \n",
    "\n",
    "            SCORES = np.array([])\n",
    "            LABELS = np.array([])\n",
    "            \n",
    "            \n",
    "            X, Y = self.getLabeledData(name, selection)\n",
    "            \n",
    "            sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "\n",
    "            for train_index, test_index in sss:\n",
    "\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "                classifier.fit(X_train, Y_train)\n",
    "                score = classifier.predict_proba(X_test)[:,1]\n",
    "                SCORES = np.concatenate((SCORES, score))\n",
    "                LABELS = np.concatenate((LABELS, Y_test))\n",
    "\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "            average_precision = average_precision_score(LABELS, SCORES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            positive_train = np.count_nonzero(Y_train)\n",
    "            positive_test = np.count_nonzero(Y_test)\n",
    "            normal_train = Y_train.shape[0] - positive_train\n",
    "            normal_test = Y_test.shape[0] - positive_test\n",
    "\n",
    "\n",
    "           \n",
    "            plt.plot(recall, precision, label=category , color=colors[g],linewidth=3)\n",
    "            \n",
    "        print(\"{} Normal and {} {} in Training Data\".format(normal_train,positive_train,name))\n",
    "        print(\"{} Normal and {} {} in Test Data\\n\".format(normal_test,positive_test,name))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def GA_evaluate(self,name, target_name, gene, selection_all, classifier):\n",
    "          \n",
    "\n",
    "        selection = selection_all[gene]\n",
    "        X,Y = self.getLabeledData(name, selection)\n",
    "        #sss = list(StratifiedShuffleSplit(Y, 1, random_state=randint(0,65536) ))\n",
    "        #train_index, test_index = sss[0]\n",
    "        #X_train, X_test = X[train_index], X[test_index]\n",
    "        #Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        classifier.fit(X, Y)\n",
    "        #score = classifier.score(X_test, Y_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        target_X = self.getFeature(target_name, selection)\n",
    "        \n",
    "        score =  - np.sum(classifier.predict_proba(target_X)[1])\n",
    "\n",
    "        return score, selection\n",
    "\n",
    "\n",
    "      \n",
    "    def GA_crossover_mutate(self, genes, scores):\n",
    "        \n",
    "         \n",
    "        gene_length = genes.shape[1]\n",
    "        population = genes.shape[0]\n",
    "        new_genes = np.zeros((population, gene_length),dtype=bool)\n",
    "        \n",
    "        for i in range(population):\n",
    "            choice = np.random.choice(population, population/10, replace=False)\n",
    "            compete_rank = np.argsort(scores[choice])\n",
    "            top = genes[compete_rank[-1],:]\n",
    "            second = genes[compete_rank[-2],:]\n",
    "\n",
    "            #crossover\n",
    "            selected_chromosoe = np.random.choice(gene_length, population/10, replace=False)\n",
    "            kid_a = top\n",
    "            kid_a[selected_chromosoe] = second\n",
    "            #mutate\n",
    "            if randint(0,100) == 0:\n",
    "                random_index = randint(0, gene_length-1)\n",
    "                kid_a[random_index] = -kid_a[random_index]\n",
    "            \n",
    "            new_genes[i,:] = kid_a\n",
    "        \n",
    "        \n",
    "        \n",
    "        return new_genes\n",
    "    \n",
    "    def GA_evolve(self,name, target_name, genes, classifier, selection_all):\n",
    "        best_score = -100\n",
    "        population = genes.shape[0]\n",
    "        scores = np.zeros(population,dtype=int)\n",
    "        for i in range(population):\n",
    "            gene = genes[i,:]\n",
    "            score, selection = self.GA_evaluate(name, target_name,gene, selection_all, classifier)\n",
    "            \n",
    "            scores[i] = score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_selection = selection\n",
    "            \n",
    "        genes = self.GA_crossover_mutate(genes, scores)\n",
    "            \n",
    "            \n",
    "            \n",
    "        return genes, best_score, best_selection\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GA_optmize(self, name, target_name, population=100,generations=100):\n",
    "\n",
    "        best_selection = range(len(self.index2feature))\n",
    "        best_score = -100\n",
    "\n",
    "        tf = np.array([True,False], dtype=bool)\n",
    "        genes = np.random.choice(tf, (population,len(self.index2feature))) #init\n",
    "        classifier = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=4)\n",
    "        selection_all = np.array(best_selection,dtype=int)\n",
    "\n",
    "\n",
    "        gene2selection = lambda gene: list(selection_all[gene])\n",
    "        for i in range(generations):\n",
    "            genes, score, selection = self.GA_evolve(name, target_name, genes, classifier,selection_all)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_selection = selection\n",
    "                print(best_score)\n",
    "                print(best_selection)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return best_score, best_selection\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "#pol_groups = POLFeaturerGroups(regular_group, [bipolar_all,BPD_all, bipolar_experts,BPD_experts])\n",
    "best_bipolar_selection = pol_groups.GA_optmize(\"Bipolar\", \"bipolar Expert\")\n",
    "best_BPD_selection = pol_groups.GA_optmize(\"BPD\", \"BPD Expert\")\n",
    "\n",
    "#pol_groups.renderPrecisionRecall_all(\"Bipolar\",n_fold=10)\n",
    "#pol_groups.renderPrecisionRecall_all(\"BPD\",n_fold=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_bipolar_selection_1 = [ 2,  7, 10, 13, 21, 22, 23, 26, 28, 29, 34, 35, 38, 40, 41, 42, 46,\n",
    "        51, 52, 56, 58, 62, 67, 68, 71, 76, 77, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 83)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_selection = range(len(pol_groups.index2feature))\n",
    "best_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 34)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pol_groups.features[0]\n",
    "x[:,best_bipolar_selection].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BPD': 1, 'BPD Expert': 2, 'Bipolar': 0, 'bipolar Expert': 3}"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_groups.name_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 positive_ratio\n",
      "1 LIWC_achieve\n",
      "2 LIWC_humans\n",
      "3 tweets_rate\n",
      "4 LIWC_excl\n",
      "5 LIWC_death\n",
      "6 LIWC_time\n",
      "7 joy\n",
      "8 age\n",
      "9 positive_combos\n",
      "10 LIWC_tentat\n",
      "11 LIWC_leisure\n",
      "12 LIWC_verb\n",
      "13 LIWC_shehe\n",
      "14 LIWC_adverb\n",
      "15 LIWC_work\n",
      "16 LIWC_quant\n",
      "17 LIWC_home\n",
      "18 LIWC_posemo\n",
      "19 LIWC_relativ\n",
      "20 LIWC_nonfl\n",
      "21 LIWC_assent\n",
      "22 LIWC_we\n",
      "23 negative_combos\n",
      "24 frequent_mentions\n",
      "25 LIWC_family\n",
      "26 LIWC_swear\n",
      "27 gender\n",
      "28 LIWC_preps\n",
      "29 LIWC_future\n",
      "30 LIWC_affect\n",
      "31 LIWC_health\n",
      "32 LIWC_see\n",
      "33 sadness\n",
      "34 fear\n",
      "35 LIWC_cogmech\n",
      "36 LIWC_filler\n",
      "37 LIWC_ipron\n",
      "38 LIWC_present\n",
      "39 disgust\n",
      "40 flips\n",
      "41 LIWC_negemo\n",
      "42 LIWC_conj\n",
      "43 LIWC_negate\n",
      "44 LIWC_pronoun\n",
      "45 LIWC_bio\n",
      "46 LIWC_sexual\n",
      "47 LIWC_incl\n",
      "48 LIWC_relig\n",
      "49 LIWC_discrep\n",
      "50 mention_rate\n",
      "51 LIWC_money\n",
      "52 LIWC_number\n",
      "53 LIWC_friend\n",
      "54 LIWC_auxverb\n",
      "55 LIWC_insight\n",
      "56 LIWC_you\n",
      "57 LIWC_social\n",
      "58 negative_ratio\n",
      "59 LIWC_percept\n",
      "60 trust\n",
      "61 LIWC_certain\n",
      "62 LIWC_cause\n",
      "63 LIWC_they\n",
      "64 LIWC_past\n",
      "65 LIWC_ppron\n",
      "66 LIWC_body\n",
      "67 LIWC_feel\n",
      "68 LIWC_anger\n",
      "69 surprise\n",
      "70 LIWC_i\n",
      "71 unique_mentions\n",
      "72 LIWC_motion\n",
      "73 LIWC_article\n",
      "74 LIWC_hear\n",
      "75 LIWC_ingest\n",
      "76 LIWC_funct\n",
      "77 anger\n",
      "78 LIWC_sad\n",
      "79 LIWC_space\n",
      "80 LIWC_anx\n",
      "81 anticipation\n",
      "82 LIWC_inhib\n"
     ]
    }
   ],
   "source": [
    "index2feature = pol_groups.index2feature\n",
    "for i, (feature_name, category) in enumerate(index2feature):\n",
    "    print(i, feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================\n",
      "\n",
      "0.272727272727 0.615384615385\n",
      "0.363636363636 0.769230769231\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selection = []\n",
    "\n",
    "removed_features = []\n",
    "for i in range(len(index2feature)):\n",
    "    name,category =  index2feature[i]\n",
    "    if  category == \"Social\":\n",
    "        removed_features.append(name)\n",
    "    else:                   \n",
    "        selection.append(i)\n",
    "#print(removed_features)\n",
    "\n",
    "bipolar_result_all = 1 - np.mean(pol_groups.classify(\"Bipolar\", \"bipolar Expert\"))\n",
    "BPD_result_all =  1 -np.mean(pol_groups.classify(\"BPD\", \"BPD Expert\"))\n",
    "\n",
    "bipolar_result_selected = 1 - np.mean(pol_groups.classify(\"Bipolar\", \"bipolar Expert\",best_bipolar_selection ))\n",
    "BPD_result_selected =  1 -np.mean(pol_groups.classify(\"BPD\", \"BPD Expert\",best_bipolar_selection ))\n",
    "print(\"\\n=======================\\n\")\n",
    "print(bipolar_result_all, BPD_result_all)\n",
    "print(bipolar_result_selected, BPD_result_selected)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== The POL false positive experiment on disorder experts =====\n",
      "\n",
      "8 out of 13 BPD Expert experts are mis-calssified as patients. Accuracy 0.38461538461538464\n",
      "6 out of 11 bipolar Expert experts are mis-calssified as patients. Accuracy 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "#Expert experiment to product best combinations of POL\n",
    "\n",
    "bipolar_result = pol_groups.classify(\"Bipolar\", \"bipolar Expert\")\n",
    "BPD_result = pol_groups.classify(\"BPD\", \"BPD Expert\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== The POL false positive experiment on disorder experts =====\\n\")\n",
    "\n",
    "misclassified = np.sum(bipolar_result)\n",
    "size = bipolar_experts.getSize()\n",
    "name = bipolar_experts.getName()\n",
    "\n",
    "print(\"{} out of {} {} experts are mis-calssified as patients. Accuracy {}\".format(np.sum(bipolar_result), size, name, (size-misclassified)/size))\n",
    "misclassified = np.sum(BPD_result)\n",
    "size = BPD_experts.getSize()\n",
    "name = BPD_experts.getName()\n",
    "print(\"{} out of {} {} experts are mis-calssified as patients. Accuracy {}\".format(np.sum(BPD_result), size, name, (size-misclassified)/size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross Classification of POL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Show the LIWC correlations \n",
    "def loads_LIWC():\n",
    "    f = open(\"../4_prediction_visualization/categories.json\", 'r')\n",
    "    categories = json.load(f)\n",
    "    f = open(\"../4_prediction_visualization/words.json\", 'r')\n",
    "    words = json.load(f)\n",
    "    patterns_categories = {}\n",
    "    for word, category in words.items():\n",
    "        if word[-1] == '*':\n",
    "            current_dict = patterns_categories\n",
    "            for char in word:\n",
    "                if char == '*':\n",
    "                   \n",
    "                    #current_dict = category\n",
    "                    pre_dict[word[-2]] = category\n",
    "                    break\n",
    "                \n",
    "                elif char not in current_dict:\n",
    "                    current_dict[char] = {}\n",
    "                pre_dict = current_dict\n",
    "                current_dict = current_dict[char]\n",
    "                \n",
    "    return categories, words, patterns_categories\n",
    "LIWC_categories, LIWC_words, LIWC_patterns_categories = loads_LIWC() \n",
    "\n",
    "def getLIWC_user(timeSeries):\n",
    " \n",
    "    LIWC_counts = {}\n",
    "    \n",
    " \n",
    "    features = np.zeros(len(LIWC_categories),dtype=int)\n",
    "    \n",
    "    texts= timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        words = text.strip().split()\n",
    "        for word in words:\n",
    "            category_indices = LIWC_words.get(word,[])\n",
    "            if category_indices == []:\n",
    "                current_dict = LIWC_patterns_categories\n",
    "                for char in word[:-1]:\n",
    "                    current_dict = current_dict.get(char,[])\n",
    "                    if isinstance(current_dict,list):\n",
    "                        category_indices = current_dict                            \n",
    "                        break\n",
    "\n",
    "\n",
    "            for category_index in category_indices:\n",
    "\n",
    "                category_name = LIWC_categories[category_index]\n",
    "\n",
    "                LIWC_counts[category_name] = LIWC_counts.get(category_name,0) + 1\n",
    "\n",
    "    for category_name in LIWC_counts:\n",
    "        LIWC_counts[category_name] /= timeSeries.shape[0]\n",
    "    \n",
    "    return LIWC_counts\n",
    "\n",
    "def getLIWC_group(group):\n",
    "    LIWC_counts = {}\n",
    "    users_num = len(group)\n",
    "    for category in LIWC_categories.values():\n",
    "        LIWC_counts[category] = []\n",
    "    for timeSeries in group:\n",
    "        user_LIWC_count = getLIWC_user(timeSeries)\n",
    "      \n",
    "        for category in LIWC_categories.values():\n",
    "          \n",
    "            LIWC_counts[category].append(user_LIWC_count.get(category,0))\n",
    "            \n",
    "    \n",
    "    \n",
    "    return LIWC_counts\n",
    "\n",
    "#polarity\n",
    "def getFlipsCount(timeSeries, upperbound=60, lowerbound = 0):\n",
    "    flips = getFlips(timeSeries)\n",
    "    durations = getFlipsDuration(timeSeries, flips)\n",
    "    return np.sum((durations > lowerbound) & (durations < upperbound) )\n",
    "\n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    filtered_timeSeries = timeSeries['dt'][flips].index.values\n",
    "    dt = np.zeros(filtered_timeSeries.shape[0],dtype=float)\n",
    "    dt[:-1] = (filtered_timeSeries[1:] - filtered_timeSeries[:-1]).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return dt\n",
    "\n",
    "def getCombosCount(timeSeries, matcher = -1, lowerbound = 2):\n",
    "    combos = comboTracker(timeSeries)\n",
    "    combos_count = sum([hit for element, hit in combos if element == matcher and hit > lowerbound])\n",
    "    return combos_count\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "def getNegativeRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1) / total_tweets\n",
    "\n",
    "\n",
    "def getPositiveRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == 1) / total_tweets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getPolarity(group):\n",
    "    polarity = {\"flips\":[],\"negative_combos\":[],\"positive_combos\":[], \"positive_ratio\":[], \"negative_ratio\":[]}\n",
    "    for timeSeries in group:\n",
    "        tweets_length = timeSeries.shape[0]\n",
    "        flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "        negative_combos_ratio = getCombosCount(timeSeries,matcher=-1) / tweets_length\n",
    "        positive_combos_ratio = getCombosCount(timeSeries,matcher=1) / tweets_length\n",
    "        positive_ratio = getPositiveRatio(timeSeries)\n",
    "        negative_ratio = getNegativeRatio(timeSeries)\n",
    "        \n",
    "        polarity[\"flips\"].append(flips_ratio)\n",
    "        polarity[\"negative_combos\"].append(negative_combos_ratio)\n",
    "        polarity[\"positive_combos\"].append(positive_combos_ratio)\n",
    "        polarity[\"positive_ratio\"].append(positive_ratio)\n",
    "        polarity[\"negative_ratio\"].append(negative_ratio)\n",
    "        \n",
    "    return polarity\n",
    "\n",
    "\n",
    "#Emotions Table\n",
    "\n",
    "\n",
    "def getUsersEmotions(timeSeries):\n",
    "    non_ambiguous = np.invert(timeSeries[\"ambiguous\"].values)\n",
    "    filtered_emotions = timeSeries[\"emotion\"][non_ambiguous].values\n",
    "    emotions_count = {\"joy\":0,\"sadness\": 0,\"fear\":0,\\\n",
    "                \"anticipation\": 0, \"anger\":0, \"trust\": 0, \"disgust\": 0 ,\"surprise\" : 0}\n",
    "    for emotion in emotions_count:\n",
    "        emotions_count[emotion] = np.sum(filtered_emotions == emotion) / filtered_emotions.shape[0]\n",
    "    return emotions_count\n",
    "        \n",
    "\n",
    "\n",
    "def getGroupEmotions(group):\n",
    "    emotions_counts = {\"joy\":[],\"sadness\": [],\"fear\":[],\\\n",
    "                \"anticipation\": [], \"anger\":[], \"trust\": [], \"disgust\": [] ,\"surprise\" : []}\n",
    "    for timeSeries in group:\n",
    "        emotions_count = getUsersEmotions(timeSeries)\n",
    "        for emotion, count in emotions_count.items():\n",
    "            emotions_counts[emotion].append(count)\n",
    "    return emotions_counts\n",
    "#social features\n",
    "def getTweetRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    delta_time = np.max(timeSeries.index.values) - np.min(timeSeries.index.values)\n",
    "    totla_duration = (delta_time).astype('timedelta64[h]') / np.timedelta64(24, 'h')\n",
    "    return total_tweets / totla_duration\n",
    "def getMentioRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    total_mentions = np.sum(seriesContains(timeSeries))\n",
    "    return total_mentions / total_tweets\n",
    "\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def seriesContains(timeSeries):\n",
    "    match_function = np.vectorize(thirdPronuonDetect)\n",
    "\n",
    "\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n",
    "\n",
    "\n",
    "def getUniqueMentions(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_set = set()\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_set.add(word)\n",
    "    return len(friends_set)\n",
    "\n",
    "def getFrequentMentions(timeSeries, lowerbound = 3):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_mentions = {}\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_mentions[word] = friends_mentions.get(word, 0) +1\n",
    "    frequent_frients = [screen_name for screen_name, mentions in friends_mentions.items() if mentions >= lowerbound]\n",
    "    return len(frequent_frients)\n",
    " \n",
    "\n",
    "def getSocialFeature_group(group):\n",
    "    social_features = {\"tweets_rate\": [],\"mention_rate\": [],\"unique_mentions\": [],\"frequent_mentions\": []}\n",
    "    for timeSeries in group:\n",
    "        social_features[\"tweets_rate\"].append(getTweetRate(timeSeries))\n",
    "        social_features[\"mention_rate\"].append(getMentioRate(timeSeries))\n",
    "        social_features[\"unique_mentions\"].append(getUniqueMentions(timeSeries))\n",
    "        social_features[\"frequent_mentions\"].append(getFrequentMentions(timeSeries))\n",
    "    return social_features\n",
    "\n",
    "def getAllFeature(group):\n",
    "    feature_set = {}\n",
    "    methods = [getSocialFeature_group, getGroupEmotions, getPolarity, getLIWC_group, getAgeGender]\n",
    "    for method in methods:\n",
    "        if method == getLIWC_group:\n",
    "            LIWC_feature_set = {\"LIWC_\"+key: value for key, value in method(group.group).items()}\n",
    "            feature_set.update(LIWC_feature_set)\n",
    "        else:\n",
    "            feature_set.update(method(group.group))\n",
    "    return feature_set\n",
    "\n",
    "#Age and gender Distribution\n",
    "\n",
    "def getAgeGender(group):\n",
    "    features = {\"age\":[],\"gender\":[]}\n",
    "    for timeSeries in group:\n",
    "        \n",
    "        features[\"age\"].append(getAge(timeSeries))\n",
    "        features[\"gender\"].append(getGender(timeSeries))\n",
    "\n",
    "    return features\n",
    "def getAge(timeSeries):\n",
    "    texts = \"\"\n",
    "    for text in timeSeries[\"text\"].values:\n",
    "        texts += text + \"\\n\"\n",
    "    return age_gender_predictor.get_age(texts)\n",
    "\n",
    "def getGender(timeSeries):\n",
    "    texts = \"\"\n",
    "    for text in timeSeries[\"text\"].values:\n",
    "        texts += text + \"\\n\"\n",
    "    return age_gender_predictor.get_gender(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCategories(group):\n",
    "    name2category = {}\n",
    "    names = [\"Social\", \"Emotions\", \"Polarity\", \"LIWC\", \"AgeGender\"]\n",
    "    methods = [getSocialFeature_group, getGroupEmotions, getPolarity, getLIWC_group, getAgeGender]\n",
    "    for i, method in enumerate(methods):\n",
    "        if method == getLIWC_group:\n",
    "            feature = {\"LIWC_\"+key: value for key, value in method([group.group[0]]).items()}\n",
    "\n",
    "        else:\n",
    "            feature = method([group.group[0]])\n",
    "        for key in feature:\n",
    "            name2category[key] = names[i]\n",
    "    return name2category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AgeGender', 'Emotions', 'LIWC', 'Polarity', 'Social'}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(getCategories(regular_group).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('b', 2)\n",
      "1 ('a', 1)\n"
     ]
    }
   ],
   "source": [
    "x  = [(\"b\",2),(\"a\",1)]\n",
    "for i,k in enumerate(x):\n",
    "    print(i,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "a is \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
