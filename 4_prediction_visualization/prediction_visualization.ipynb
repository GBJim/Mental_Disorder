{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from random import randint\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from Levenshtein import *\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_stopwords(file_location=\"../SmartStoplist\"):\n",
    "    f = open(file_location)\n",
    "    stopwords = [line.strip() for line in f]\n",
    "    return stopwords + [\"http\",\"https\", \"don\", \"thi\",\"http \", \"co\",\"dont\",\"im\"]\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "stemmer = PorterStemmer()\n",
    "stopSet = set(stopwords)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = []\n",
    "    global stopSet\n",
    "    \n",
    "    for word in text.lower().split():\n",
    "        word = re.sub('[!@#$?\\'\\\"]|&amp', '',word)\n",
    "        stemmedWord = stemmer.stem_word(word)\n",
    "        condition_Word = word not in stopSet  and \"http\" not in word\n",
    "        condition_StemmedWord = stemmedWord not in stopSet\n",
    "        \n",
    "        if  condition_Word and  condition_StemmedWord:\n",
    "            words.append(stemmedWord)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Extraction Functions\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getTFIDF(usersDataList):\n",
    "    textsList = []\n",
    "    for usersData in usersDataList:\n",
    "        texts = \"\"\n",
    "        for data in usersData.values():\n",
    "            for text in data[\"texts\"]:\n",
    "                texts += preprocessor(text) + \"\\n\"\n",
    "        textsList.append(texts)\n",
    " \n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    tfidf.fit(textsList)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "    \n",
    "def getEmotionFeature(usersDataList):\n",
    "    \n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    featuresList = []\n",
    "\n",
    "    \n",
    "    for usersData in usersDataList:\n",
    "        features = []\n",
    "        for data in usersData.values():\n",
    "            feature = np.array([])\n",
    "            emotionCompositions = np.zeros(len(emotions),dtype=float)\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                feature = np.concatenate((feature, (data[\"emotions\"].get(emotion, emptyHourTable()))))\n",
    "                emotionCompositions[i] = np.sum(data[\"emotions\"].get(emotion, emptyHourTable()))\n",
    "            feature = np.concatenate((feature,emotionCompositions))\n",
    "            features.append(feature)\n",
    "            \n",
    "        featuresList.append(np.array(features))\n",
    "        \n",
    "    return featuresList\n",
    "\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "       model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    \n",
    "    getText = lambda data : \"\\n\".join(data[\"texts\"])\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData.values():\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "def featureCombination(emotion_features, text_features):\n",
    "    featuresList = []\n",
    "    for i in range(len(emotion_features)):\n",
    "        new_features = csr_matrix(hstack((emotion_features[i], text_features[i])))\n",
    "        \n",
    "        featuresList.append(new_features)\n",
    "    return featuresList\n",
    "    \n",
    "\n",
    "    \n",
    "def featureExtraction(usersDataList, emotion_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    \n",
    "    featuresList = []\n",
    " \n",
    "    \n",
    "    if emotion_extraction:\n",
    "        emotion_features = getEmotionFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = emotion_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if emotion_extraction:\n",
    "            featuresList = featureCombination(emotion_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XYGenerator(featuresList,verbose=True):\n",
    "    X, Y = featuresList[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for x, y in featuresList[1:]:\n",
    "        try:\n",
    "            \n",
    "            X = np.concatenate((X, x))\n",
    "        except:\n",
    "            X = vstack([X, x])\n",
    "            \n",
    "        Y = np.concatenate((Y, y))\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showImportantFeatures(features_importance, text_model, n_features = 10):\n",
    "    \n",
    "    if text_model is None:\n",
    "        print(\"The top {} most important features:\\n\".format(n_features))\n",
    "        emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "        for vector in features_importance[:n_features]:\n",
    "            \n",
    "            if vector >= 192:\n",
    "                \n",
    "                emotion = emotions[vector - 192]\n",
    "                print(\"The {} ratio of users\".format(emotion))\n",
    "            \n",
    "            else:\n",
    "                emotion = emotions[int(vector/24)]\n",
    "                hour = vector % 24\n",
    "                print(\"The daily {} ratio at {} o'clock\".format(emotion, hour))\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"The following are the top {} most important features:\\n\".format(n_features))\n",
    "        vector2word = text_model.get_feature_names()\n",
    "        for vector in features_importance[:n_features]:\n",
    "            word = vector2word[vector]\n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting functions\n",
    "def plot_precision_recall(classifier, featuresList, n_fold=20):\n",
    "    \n",
    "    \n",
    "    \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "        \n",
    "    precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "    average_precision = average_precision_score(LABELS, SCORES)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    " \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(recall, precision, label='area = %0.2f' %  average_precision , color=\"green\",linewidth=3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier\n",
    "def plot_multiple_precision_recall(classifier, featuresLists,names,colors=['r','g','b',\"y\"] , n_fold=20):\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    \n",
    "    for g,featuresList in enumerate(featuresLists):\n",
    "\n",
    "        SCORES = np.array([])\n",
    "        LABELS = np.array([])\n",
    "\n",
    "        X, Y = XYGenerator(featuresList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "\n",
    "        for train_index, test_index in sss:\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            classifier.fit(X_train, Y_train)\n",
    "            score = classifier.predict_proba(X_test)[:,1]\n",
    "            SCORES = np.concatenate((SCORES, score))\n",
    "            LABELS = np.concatenate((LABELS, Y_test))\n",
    "\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "        average_precision = average_precision_score(LABELS, SCORES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        BPD_train = np.count_nonzero(Y_train)\n",
    "        BPD_test = np.count_nonzero(Y_test)\n",
    "        normal_train = Y_train.shape[0] - BPD_train\n",
    "        normal_test = Y_test.shape[0] - BPD_test\n",
    "\n",
    "       \n",
    "\n",
    "        plt.plot(recall, precision, label=names[g] , color=colors[g],linewidth=3)\n",
    "      \n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "\n",
    "    plt.show()\n",
    "def plot_ROC(classifier, featuresList, n_fold=20):\n",
    "      \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "      \n",
    "    fpr, tpr, _ = roc_curve(LABELS, SCORES, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "       \n",
    "   \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "    \n",
    "  \n",
    "   \n",
    " \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, color=\"green\",linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', color=\"w\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#John Hopkins Patten of lifes:\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def tweetRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    delta_time = np.max(timeSeries.index.values) - np.min(timeSeries.index.values)\n",
    "    totla_duration = (delta_time).astype('timedelta64[h]') / np.timedelta64(24, 'h')\n",
    "    return total_tweets / totla_duration\n",
    "def mentioRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    total_mentions = np.sum(seriesContains(timeSeries, method=\"third\"))\n",
    "    return total_mentions / total_tweets\n",
    "\n",
    "def uniqueMentions(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_set = set()\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_set.add(word)\n",
    "    return len(friends_set)\n",
    "\n",
    "def frequentMentions(timeSeries, lowerbound = 3):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_mentions = {}\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_mentions[word] = friends_mentions.get(word, 0) +1\n",
    "    frequent_frients = [screen_name for screen_name, mentions in friends_mentions.items() if mentions >= lowerbound]\n",
    "    return len(frequent_frients)\n",
    "\n",
    "def getNegativeRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1) / total_tweets\n",
    "\n",
    "\n",
    "def getPositiveRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == 1) / total_tweets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new functions\n",
    "\n",
    "def getNegativeCount(timeSeries):\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1)\n",
    "\n",
    "\n",
    "def getUsersPolarities(collectionName):\n",
    "    collection = MongoClient(\"localhost\", 27017)['idea'][collectionName]\n",
    "    usersPolarties = {}\n",
    "    for tweet in collection.find():\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        #Processing emotions from Carlos' API\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]\n",
    "            \n",
    "        ambiguous = True if tweet['emotion']['ambiguous'] == 'yes' else False\n",
    "       \n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]    \n",
    "        else:\n",
    "            emotion_2 = None\n",
    "        if tweet[\"polarity\"] == \"positive\":\n",
    "            polarity = 1\n",
    "        elif tweet[\"polarity\"] == \"negative\":\n",
    "            polarity = -1\n",
    "        else:\n",
    "            polarity = 0\n",
    "   \n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "\n",
    "        if userID not in usersPolarties:\n",
    "            usersPolarties[userID] = {}\n",
    "        if date not in usersPolarties[userID]:\n",
    "            usersPolarties[userID][date] = {}\n",
    "        usersPolarties[userID][date]['text'] = text\n",
    "        usersPolarties[userID][date]['polarity'] =  polarity\n",
    "        usersPolarties[userID][date]['emotion'] =  emotion\n",
    "        usersPolarties[userID][date]['emotion_2'] =  emotion_2\n",
    "        usersPolarties[userID][date]['ambiguous'] =  ambiguous\n",
    "    return usersPolarties\n",
    "\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "\n",
    "\n",
    "def userVerify(timeSeries, threshold = 0.5, lowerbound = 50):\n",
    "    http_rows = getHTTPRows(timeSeries)\n",
    "    average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "    duration = np.max(timeSeries.index.values) -  np.min(timeSeries.index.values)\n",
    "    duration = duration.astype('timedelta64[s]') / np.timedelta64(604800, 's')\n",
    "    return (average_http_count < threshold) and (timeSeries.shape[0] > lowerbound) and duration > 1\n",
    "\n",
    "\n",
    "def groupFilter(group):\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        if userVerify(timeSeries):\n",
    "            new_group.append(cleanPost(timeSeries))\n",
    "    return new_group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "\n",
    "\n",
    "def cleanPost(timeSeries):\n",
    "    left_text = timeSeries['text'].values[:-1]\n",
    "    right_text = timeSeries['text'].values[1:]\n",
    "    conditions = np.ones(timeSeries.shape[0],dtype=bool)\n",
    "    edit_distance = np.vectorize(distance)\n",
    "    conditions[:-1] =  conditions[:-1] & (edit_distance(left_text, right_text) > 5)\n",
    "    patterns = ['http://','https://']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        conditions = conditions & np.logical_not(timeSeries['text'].str.contains(pattern).values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "\n",
    "    return timeSeries\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def seriesContains(timeSeries,method =\"first\"):\n",
    "    if method == \"first\":\n",
    "        match_function = np.vectorize(firstPronuonDetect)\n",
    "    elif method == \"second\":\n",
    "        match_function = np.vectorize(secondPronuonDetect)\n",
    "    elif method == \"third\":\n",
    "            match_function = np.vectorize(thirdPronuonDetect)\n",
    "\n",
    "\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n",
    "    \n",
    "\n",
    "def firstPronuonDetect(words, matchers=[\"i\",\"we\",\"i'd\",\"i'm\"]):\n",
    "    for matcher in matchers:\n",
    "        if matcher in words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def getFirstPronounCount(timeSeries):\n",
    "    return np.sum(seriesContains(timeSeries))\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getCombosCount(timeSeries, matcher = -1, lowerbound = 2):\n",
    "    combos = comboTracker(timeSeries)\n",
    "    combos_count = sum([hit for element, hit in combos if element == matcher and hit > lowerbound])\n",
    "    return combos_count\n",
    "    \n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def getFlipsCount(timeSeries, upperbound=30, lowerbound = 0):\n",
    "    flips = getFlips(timeSeries)\n",
    "    durations = getFlipsDuration(timeSeries, flips)\n",
    "    return np.sum((durations > lowerbound) & (durations < upperbound) )\n",
    "\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "        model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "        to_train = True\n",
    "    else:\n",
    "        model = text_model\n",
    "        to_train = False\n",
    "        \n",
    "    getText = lambda data : \"\\n\".join(data[\"text\"].values)\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData:\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    \n",
    "    if to_train:\n",
    "        model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "def getPolarityFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),4),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            negative_ratio = getNegativeCount(timeSeries) / tweets_length\n",
    "            feature[i][0] = flips_ratio \n",
    "            feature[i][1] = combos_ratio\n",
    "            feature[i][2] = negative_ratio\n",
    "            feature[i][3] = first_pronoun_ratio\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def getEmotionFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),4+8),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            negative_ratio = getNegativeCount(timeSeries) / tweets_length\n",
    "            feature[i][0] = flips_ratio \n",
    "            feature[i][1] = combos_ratio\n",
    "            feature[i][2] = negative_ratio\n",
    "            feature[i][3] = first_pronoun_ratio\n",
    "            #emotions come in\n",
    "            \n",
    "            feature[i][4:] = getEmotionRatio(timeSeries)\n",
    "            \n",
    "            \n",
    "            \n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def getEmotionRatio(timeSeries):\n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    emotion_ratios = []\n",
    "    conditions = np.logical_not(timeSeries[\"ambiguous\"].values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        total = np.sum((timeSeries[\"emotion\"].values == emotion))\n",
    "        emotion_ratio = total / timeSeries.shape[0]\n",
    "        emotion_ratios.append(emotion_ratio)\n",
    "        \n",
    "    return emotion_ratios\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getJHFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),6),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "         \n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = negative_ratio\n",
    "            feature[i][5] = positive_ratio\n",
    "          \n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def getCombinedFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),9+8),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = positive_ratio\n",
    "            feature[i][5] = negative_ratio\n",
    "            feature[i][6] = flips_ratio\n",
    "            feature[i][7] = combos_ratio\n",
    "            feature[i][8] = first_pronoun_ratio\n",
    "            feature[i][9:] = getEmotionRatio(timeSeries)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def featureExtraction(usersDataList, polarity_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    featuresList = []\n",
    " \n",
    "\n",
    "    if polarity_extraction:\n",
    "        polarity_features = getPolarityFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = polarity_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if polarity_extraction:\n",
    "            featuresList = featureCombination(polarity_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "\n",
    "def emotionExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList = getEmotionFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "    \n",
    "def JHfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList = getJHFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) \n",
    "\n",
    "    \n",
    "def CombinedfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList =getCombinedFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Fetching the data of BPD and random sampled Twitter user\n",
    "\n",
    "BPD_polarities = getUsersPolarities(\"BPD_581_emotion\")\n",
    "regular_polarities = getUsersPolarities(\"regularUser_en_fixed_emotion\")\n",
    "#Transform raw tweets into timeSeries data and filter and clean the timeSeries data\n",
    "BPD_timeSeries = groupFilter(timeSeriesTransform(BPD_polarities))\n",
    "regular_timeSeries = groupFilter(timeSeriesTransform(regular_polarities))\n",
    "groups = [regular_timeSeries, BPD_timeSeries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 Normal and 275 BPD in Training Data\n",
      "59 Normal and 31 BPD in Test Data\n",
      "Number of features: 3867910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4HFWZx/Hvm7AECHuQYZKwo7KD7BokGED2KPs+oCCj\n",
       "4jgzqFFROz0qoCjyKIrAsKmsBtSALANCAJFdQoAsEDZDWEOAkIQlIWf+eE/bVZ2+fev27erqe+/v\n",
       "8zz13Nq66u2C1NvnnKpzLISAiIhIxaCiAxARkc6ixCAiIilKDCIikqLEICIiKUoMIiKSosQgIiIp\n",
       "SgzS6Y4Gbsmw33nAd3KOpV2OB+5OLC8BNiwmFBmIlBikN54DFgJvAy8DlwArtfgclwOfzrDfF4Ef\n",
       "tPjcAOOBRfh3fBO4D9g1h/P0xqeBu4B5wKvAJOCAIgOSvk2JQXojAPsDKwMfA7an/q/2ZdoZVIsF\n",
       "4Er8O64J3AZMKDSitEOAa4BLgeHAh4Dv0VxisDjJAKfEIK3yInAzsHlcXgJ8CXgKmBHX7Q9MBt4A\n",
       "7gG2THx+JHAd/ot3DvCLuP54qtUqBvwMeAV4C5gCbBa3XQp8P3G8k+K5Xwf+BKyT2LYEOBl4MsZy\n",
       "boPvlbxZfgBcAawFDIvrVgUuit//hRhD8t/VScBU/Nf8E8C2cf03gZmJ9Z9pEEOj2M4G/ge4GC/V\n",
       "gJcevhDnxwO/TXxmffz7V2KchJe07gEWAF8HHqw5z3/h1xBgeeAnwPN4KfE8YEgTsUsHU2KQ3qrc\n",
       "NEcC+wCPJLaNBXbAb97b4jfQk4A1gPOBicCywGDgBuBZYD38l++Vdc61F16Nswl+Qz4UmBu3hTgB\n",
       "fAo4PW5fB7+JXVVzrP3wEs5WwGFkq65aDjgOeBpPXuAJ6X1go/gd9wJOjNsOBUrAscAqwIF4ogJP\n",
       "CqPi+jLwO2DtDDEkfQQYQeMSTJY+b46JMQ8Ffh2Pu3Fi+1F4lR7AmXHb1vHvcLyEIv2IEoP0hgF/\n",
       "xH91343/+jw9sf0MvF7+PfwX7Pn4r9EA/Cau3wXYEb+Bfx14J67/W53zLcKrdDbF/9+dgf9qrXU0\n",
       "noQm4zftb8XzrJvY50z81/os4A5gmwbf87D4HRfiN9B94/q18WT4XzHu14BzgCPi9hOBHwEPx+Wn\n",
       "gX/E+QmJ2K/BSzc7NYihnjXj35ca7NNd1VDAk9s0vCQxDy8dHBm3b4InionxWCcB/43/d52P/zc+\n",
       "AulXlBikNwJeKlgdr6I4Bb+pV8xKzK8HnIrfYCvTCDwhjMR/1S/p5ny349U+v8Srk87HE0WtSimh\n",
       "YgH+S314Yl0yoSzEfy135Wr8O64NPA58JfGdlsVvzJXv9Gu8qgn8+z3dxTGPw0tXlc9tQfVGn1Wl\n",
       "9LFOw726N6tm+QqqieEo4A/Au/j3WhFPdJW4b6JarSb9hBKD5ClZjfEP4If4DbYyDcVvurPwX/OD\n",
       "MxzzF3gV0GbAh/FSRq0X8URVsRJ+053do+hdoPqr+3W85PMFYIMY93vx2JXvtCrVtpNZpKtkKtYD\n",
       "LgC+jFerrY4nnJ42/M6I5zikwT7z8Zt5xb/U2ae2uuk2PAlsjZcGrojr5+Alo82oft/V8Oow6UeU\n",
       "GKRdLgT+Ha82MvxmvR+eHO7Hf3Wfid/EhgAfr3OM7fHqlmXxX/nv4g3CkG4kvhI4Ab+xLY9Xb91H\n",
       "tRqnVqMbcu22J4HrgW/EmP8PbwBeGf/3tBHwybjv/wJfw5/YMjxJrBu/e8BvtINirFs0iKErAa/W\n",
       "+S7eSL9KPN4ovDQF8GiMZySetL6V4TsuAn6PNzKvDtwa1y/B/zueQ7VUNBxvV5F+RIlB8lL7K/Rh\n",
       "vH76XLzB+Cm8OgX8hnMAfuP8B/4r+LDEcSrHWgX/pT0Xf4diDnBWnf3+gt8sr8VLDxuQrgevjS3U\n",
       "Wddo21kx9g/Fv8vhTx7NxW+olV/lE/BS0hV43f11+I12KvBT4F68SmsL4K8NztmoAfla4HDgc3iJ\n",
       "6GX8KaU/xu234qWyKXj7zvV1jlfv+FcAY+L3SVbxjcMbzu/Dnwy7FS+5ST9iGqhHRESSVGIQEZEU\n",
       "JQYREUlRYhARkRQlBhERSekrnZuphVxEpDk97hixL5UYTBOG96tTdAydMula6FroWjSemtKXEoOI\n",
       "iLSBEoOIiKQoMfQ9k4oOoINMKjqADjKp6AA6yKSiA+jr+sqbz8mOzEREJJum7p15lxguxrtHfqzB\n",
       "Pj/H+815lOroViIiUpC8E8MlwN4Ntu+Ld5y2Cd6V8Xk5xyMiIt3I+z2Gu4H1G2w/ELgszt+P9+2+\n",
       "Nl7KSLGy7dnq4ERkKfOAR0IpvF90IFKcol9wG0569KgX8FGvlkoMeL/3IpK/BVa2Sfi/ucdIPxc/\n",
       "qGY54AMYvQy8ooTSPxSdGGDphpG6reGl3Ur/nJ/03CTufP7OPGMSGcgqgyjt19MPWtnmAq/iYzhU\n",
       "ksigmvm3gWfiNJtqA2nAx9mYDsxUkmnK6Dj1SjueSlofHxxkyzrbfo0/WnZVXJ4O7MbSJYZgZbst\n",
       "p/hEpGojfGCjoi3Ghzt9Ah9RbnCcKklmNjAZ+FsohZlFBdkHNPVUUtGJYV98APl9gZ3xIQN3rrOf\n",
       "HlcVaRMr20bAnvgIbmtRHVEu4CWB5PIgYBg+at3aFPNu1L3ApcA1oRTeLOD8nawjE8OVeAlgGF4K\n",
       "KOHj9UJ1TNpz8SeXFuBj3/69znGUGEQ6nJVtMP5vfVhcVUkitX/XBDbESydrx/VL8PG5NwY2xcfG\n",
       "7qnF+BCmD+DDmD4ATAul8EHDT/VvHZkYWkWJQWQAsbKtBmyDJ5AAfBCnStvFJsAOwF40bitdQBx7\n",
       "O5TCU3nG3KGUGERkYLGyrQUcCfwb8LEGuy4BfgecAbwELAylsCj/CAunxCAiA1csZWwH7BinHfBH\n",
       "4uv5AH8q6klgBvAQ3kbR36qdlBhERJKsbB/H2zb3yrD7DcDYUApL8o2qrZQYRETqsbJ9AvgO8Em8\n",
       "Wmlog93/DnwplML97YgtZ0oMIiJZWNlWxBuwPwwcBBxRs8s7wGnALfiTTX3iRlmHEoOISE9Z2ZYF\n",
       "rgUO6GKX14DbgdNCKTzdtsBaQ4lBRKRZVrYVqL6Qu1GdXV4CtgilMLedcfWSEoOISG9Z2VYFjgN2\n",
       "x9sk1kxsviKUwtGFBNYcJQYRkVaysg0CDgeuSKz+ciiFXxUUUk8pMYiI5MHKdin+El3FyaEULigo\n",
       "nJ7oyKE9RUT6g6/gfS9VnGtl+9eigsmbEoOISDdCKbwNfBrvBhy8M9CDi4soX0oMIiIZxC69f5lY\n",
       "dWhRseRNiUFEJLvr8DenAUZZ2XYpMpi8KDGIiGQUSuEV4Na4aMDE2MNrv6LEICLSM6cAc+L8MODz\n",
       "BcaSCyUGEZEeiGNMn5pYdaKVrV89Tq/EICLScxOAt+L8RsCoAmNpOSUGEZEeCqWwEO94r2L7omLJ\n",
       "gxKDiEhzpibmNy4sihwoMYiINGdmYr5eb6x9lhKDiEhzkmMzqMQgIiI8m5gfGXti7Rf6zRcREWmn\n",
       "UAoLgDfi4nJAv3nRTYlBRKR5LyTmRxQWRYspMYiING9WYn5kYVG0mBKDiEjzkiWGU6xsKxYWSQsp\n",
       "MYiING9iYn4McH1/SA5KDCIiTQql8Gfge4lVnwL+bGVbqaCQWkKJQUSkF0IpfB84LbFqNHCjlW2V\n",
       "YiLqPQshFB1DFk0NaC0i0i5WtnHAmYlVDwL7hFJ4vaCQoMl7pxKDiEiLWNlOBX6SWDUV2DOUwosF\n",
       "haTEICJSNCvbycB5VO9ZzwCfCKXwcgHhNHXvzLuNYW9gOvAUMK7O9mHAzcBk4HHg+JzjERHJVSiF\n",
       "84GjgcVx1YbA94uLqOfyLDEMBmYAewCz8fq2I4FpiX3GA8sD38KTxAxgbaoXtEIlBhHpU6xshwC/\n",
       "j4sfAB+No7+1U8eVGHbEu6V9DlgEXAWMrdnnJaDScr8K8DpLJwURkT4nlMIE4I64OJj0Y60dLc/E\n",
       "MJz06+IvxHVJFwKbAy8CjwJfzTEeEZF2+25i/hgr26aFRdIDeSaGLHVU38bbF/4V2Ab4JbByjjGJ\n",
       "iLRNKIV78HZU8CqdbxUYTmbL5Hjs2aQ7lRpJul8RgI8DP4zzT+P9m38EeKjO8cYn5ifFSUSk05Xw\n",
       "B3EADreyfS2Uwqs5nWt0nHolz8bnZfDG5DF4VdEDLN34fDbwFlDGG50fBrYC5tYcS43PItJnWdnu\n",
       "BXaOi98NpfCDNp264xqfFwOnALfgL3lcjSeFk+MEcDqwPd6+cBvwDZZOCiIifd0vEvNfsrItV1gk\n",
       "GegFNxGRnMVE8DzwL3HVuFAKP27DqTuuxCAiIkAohfeBsxKrxlvZNigqnu4oMYiItMcv8GpzgBWA\n",
       "X1nZOrImRIlBRKQNQikswttXK/X3ewOHFhdR15QYRETaJJTC/cCvEqtKRcXSiBKDiEh7JTvUW7ew\n",
       "KBpQYhARaa+FRQfQHSUGEZHiLGtlG1J0ELWUGERE2msB8FqcXx7vM66jKDGIiLRRKIUleDdAFd+0\n",
       "sm1eVDz1KDGIiLTfecC9cX5Z4EIrW8fcjzsmEBGRgSKWGk7CBzED2AU4sbiI0pQYREQKEErhCeCM\n",
       "xKovFBVLLSUGEZHi/Ax4P85vZ2XbqMhgKpQYREQKEkrhTXxogoqO6CJDiUFEpFjXJOYPKyyKBCUG\n",
       "EZFiXU+1OmlbK9tmRQYDSgwiIoUKpfAW8OfEqvEFhfJPSgwiIsVLPp10qJVtu8IiQYlBRKRwoRQe\n",
       "BK5NrDq9qFhAiUFEpFN8B1gS5/eysn2yqECUGEREOkAohenApYlVJxcUihKDiEgH+UVi/iAr26pF\n",
       "BKHEICLSIUIpTAYmx8UhwOFFxKHEICLSWS5NzJ9QRABKDCIineVyqr2u7mxl+2i7A1BiEBHpIKEU\n",
       "5gA3JFYd3+4YlBhERDrPJYn546xsy7Tz5EoMIiKd52bglTi/DnBQO0+uxCAi0mFCKSwCzk+s+pqV\n",
       "zdp1fiUGEZHO9EvgvTi/AzCqXSdWYhAR6UChFF4FfptYdWq7zq3EICLSuc5OzB9oZduwHSdVYhAR\n",
       "6VChFKZRHfrTgD3acV4lBhGRznZbYn7ndpwwS2IYBdwKPAU8G6dnMh5/b2B6/Oy4LvYZDTwCPA5M\n",
       "ynhcEZGB4v7E/E7tOKGFELrbZwbwn8DfgQ8S6+d087nB8bN7ALOBB4EjgWmJfVYD7gE+DbwADOvi\n",
       "uAEvRomIDChWthWBefg9NQCrx+FAs2jq3pmlxPAmcBP+ssWcxNSdHYGZwHN4vx9XAWNr9jkKH7Xo\n",
       "hbic5bgiIgNGKIWFwJS4aPijq7nKkhjuAM4CdgE+lpi6MxyYlVh+Ia5L2gRYI57jIeDYDMcVERlo\n",
       "pibma++jLZel/42d8eLI9jXrd+/mc93WUQHL4klmDLAicC9wH94mUWt8Yn4Sao8QkYHjg+53AbzN\n",
       "dnRvT5YlMTR7ktnAyMTySKpVRhWz8Oqjd+J0F7A13ScGERFZ2iTSP5pLzRwkS1XSasDPgIfj9FMg\n",
       "y3BzD+FVResDy+EjEU2s2edP+FNPg/ESw06ki0wiIgKLE/O75X2yLInhYrxF/FDgMOBt0l3CdmUx\n",
       "cAr+csZU4Gr8iaSTqQ5yPR3vRXAK/kjWhSgxiIjUuikxf4KV7ag8T5blcdVH8eqd7tblSY+risiA\n",
       "FXtWvQI4Iq6aD2wXSuHJbj6a2+Oq7wC7JpZHAQt7eiIREWlOKIWA17TMjKuGApfn1RV3lhLDNsBv\n",
       "qLYrvAH8G15qaBeVGERkwLOybYs/vbl8XLVlKIXHG3wktxLDZGArYMs4bUN7k4KIiAChFB4hPR70\n",
       "p/I4T6PHVY/F+wI/lfQ7CRaXz673IRERydXtwMFxfgzw81afoFGJYcX4d+UuJhERab+/JOZHW9my\n",
       "vI/WI1naGDqB2hhERPjnE0qzqHaNsWMohQe72D23NoYfA6vg3Vf8BX9TWX0aiYgUID6hdGdi1a5d\n",
       "7dusLInh0/gLbvvjPaVuBHy91YGIiEhmycTwyVYfPEtiqNRf7Q9MAN4iWwd5IiKSj7sS87ta2Vo6\n",
       "GmeWg12Pd12xHV6V9CHg3VYGISIiPTIDeC3OrwEc2MqDZ218XhMfsOcDYCX8qaSXWxlIN9T4LCKS\n",
       "YGW7EDgxLs4FtgmlMKtmt6bunY0Swxi8hHAw1aqjygkCcF1PT9YLSgwiIglWtmH4C8iVp5P+Cuwe\n",
       "SiHZE2vLn0qqNGgckJj2j9MBPT2RiIi0TiiFOcCRwJK4ahTeC3av6T0GEZE+zMp2JjAuLv4+lMJh\n",
       "ic25vcdwOj5YT8XqwA96eiIREcnFbxPz+1jZlu9yz4yyJIZ98YbnijeA/Xp7YhERaYmpwDNxfiiw\n",
       "e28PmCUxDAKGJJZXwIfqFBGRgsU3oW9OrNqqt8fM0vnS5fjTSRfjdVUn4OMziIhIZ3g7Md/rl92y\n",
       "JIYf4WMyj4nL/4OP4ywiIv1Q1u5apwGLgVvx7rhXJp2hRESkOPMT872uSspS5PgC8Hvg13F5BPDH\n",
       "3p5YRERa5vrE/GetbKt1uWcGWRLDl/EXJ+bF5Sfx/pJERKQDhFJ4FHgkLg4BjujN8bIkhvfiVLEM\n",
       "6l1VRKTTXJqYP7I3B8qSGO4ETsPbFvbEq5Wub/gJERFpt6updo+xq5VtRLMHypIYxuHduz4GnAzc\n",
       "CHyn2ROKiEjrhVJ4Bbg9LhpwWIPdG+ruqaRlgMeBjwIXNHsSERFpi2uAPeJ8029Ad1diWIwPCLFe\n",
       "sycQEZG2+WtifrtmD5LlPYY1gCeAB4AFcV2gxSMGiYhIrz2J36dXAtZp9iBZEkOlPSHZdaueShIR\n",
       "6TChFD6wsj0PbNab4zRKDCsA/w5sjHeJcTGwqDcnExGR3PX6h3ujNobL8DqqKXjX2z/p7clERKTz\n",
       "NSoxbApsGecvAh7MPxwRESlaoxLD4i7mRUSkH2uUGLbCe1CtTFsm5uc1+FzS3sB04CmqY5LWswOe\n",
       "fA7KeFwREclJo6qkwb089mDgXPxli9l4VdREvAvv2v1+hI9A1ONBq0VEpLV6PdJPAzsCM4Hn8KeZ\n",
       "rgLG1tnvK8AEvNsNEREpWJ6JYTgwK7H8QlxXu89Y4Ly4rPcjREQKlmdiyHKTPwf4ZtzXUFWSiEjh\n",
       "sg7t2YzZwMjE8ki81JC0HV7FBDAM2AevdppY53jjE/OT4iQiIlWjz9rzrLXmvz+/+z0bsBByq71Z\n",
       "Bu+AbwzwIt7X0pEs3fhccQk+zsN1dbZVShQiItKAle0q4HCAUArQxL0zz6qkxcApwC3AVHwQiWn4\n",
       "mA4n53heEZGB7KbeHiDPqiTwAGuDPL+LfU/IORYRkYGg10945lliEBGRPkiJQUREUpQYRET6lwXd\n",
       "79KYEoOISP9yP9n7s6tLiUFEpB8JpfAu9d8Fy0yJQUSk/7kM78S0KXm+4NZKesFNRKTnmrp3qsQg\n",
       "IiIpSgwiIpKixCAiIilKDCIikqLEICIiKUoMIiKSosQgIiIpSgwiIpKixCAiIilKDCIikqLEICIi\n",
       "KUoMIiKSosQgIiIpSgwiIpKixCAiIilKDCIikqLEICIiKUoMIiKSosQgIiIpSgwiIpKixCAiIilK\n",
       "DCIikqLEICIiKUoMIiKSosQgIiIpSgwiIpKixCAiIintSAx7A9OBp4BxdbYfDTwKTAHuAbZqQ0wi\n",
       "ItIFCyHkefzBwAxgD2A28CBwJDAtsc8uwFTgLTyJjAd2rjlOACzPQEVE+qGm7p15lxh2BGYCzwGL\n",
       "gKuAsTX73IsnBYD7gRE5xyQiIg3knRiGA7MSyy/EdV35PHBjrhGJiEhDy+R8/J7UU+0OfA74RBfb\n",
       "xyfmJ8VJRESqRsepV/JODLOBkYnlkXipodZWwIV4G8MbXRxrfEsjExHpfyaR/tFcauYgeVclPQRs\n",
       "AqwPLAccDkys2Wdd4DrgGLw9QkRECpR3iWExcApwC/6E0kX4E0knx+3nA98DVgfOi+sW4Y3WIiJS\n",
       "gLwfV20VPa4qItJzHfm4qoiI9DFKDCIikqLEICIiKUoMIiKSosQgIiIpSgwiIpKixCAiIilKDCIi\n",
       "kqLEICIiKXl3iSEiHczM+kTXB9K9EELLeodQYhAZ4Fp5Q5FitDrBqypJRERSlBhERCRFiUFERFKU\n",
       "GEREJEWJQUREUpQYRERaxMzWN7M7zGyBmU0zszEN9l3NzC4zs1fitNT4zGb2VTN7xszmm9lUM9sk\n",
       "32/glBhEpM8zs8FFxxBdCTwMrAGcBkwws2Fd7PszYAiwHj6c8bFmdnxlo5mdCHwO2DeEMBTYD5iT\n",
       "X+hVSgwi0pHM7JtmNtPM5pnZE2b2mcS2483sHjM728zmACUzW87MfmJmz5vZy2Z2npkNifuvZmY3\n",
       "mNmrZjbXzK43s+EtjvfDwLZAKYTwXgjhOmAKcHAXH9kfOCuE8G4I4XngIjwRYGaDgBLwnyGE6QAh\n",
       "hGdDCG+0Muau6AU3EanLyq19aSqUevwi3UxgVAjhZTM7DPidmW0UQnglbt8RuAL4ELAc8CNgA2Br\n",
       "YHHc9j3g2/iP4IuAQ/D73sXAucBn653YzG4APtFFXHeHEA6ss35z4JkQwoLEukfj+q4kr8kgYIs4\n",
       "PwIYDmxpZpfF7/MboBxCyP1tdZUYRKQjhRAmhBBejvPXAE8BOyV2eTGE8MsQwhLgPeAk4L9DCG+G\n",
       "EOYDZwBHxM/PDSH8If46nw+cDuzW4Nz7hxBW72KqlxQAhgJv1aybB6zcxf43A+PMbKiZbYyXFlaI\n",
       "20bEv3viyWJ34Ejg813F3EpKDCLSkczsODN7xMzeMLM38BvkmoldZiXm1wJWBB5O7H8TMCwea0Uz\n",
       "O9/MnjOzt4A7gVXNrJXdgcwHVqlZtxqeHOr5D+BdPOH9AS/hzI7b3ol/fxxCmBerms4H9m1hvF1S\n",
       "VZKI1NVE1U/LmNl6wAXAp4B7QwjBzB4hXfWSrFKZg99MNwshvFTnkKcCHwZ2DCG8ambbAH+Px1uq\n",
       "asbMbgJGdRHeXSGE/eqsfwLY0MyGxlIJeLXWb+sdJLYXHJM45+nA/XFxBvB+vY91EVNLqcQgIp1o\n",
       "JfwmOAcYZGYnUK1/X0qsTroQOMfM1gIws+FmtlfcZSieON4yszXwht0uhRD2CSGs3MVULykQQngS\n",
       "mIw3hA8xs4NizNfW29/MNjSzNc1ssJntg1eF/SAeayFwNfCNWNU0Im6/oVHcraLEICIdJ4QwFfgp\n",
       "cC/wMn6D/WtyF5b+9TwOb7C+L1YX3YqXEgDOwevv5wB/w6uZ8vj1fQSwPTAX+CFwcAjhdQAz29XM\n",
       "3k7sux3+1NK8uO9RIYRpie2n4NVTL8aYLw8hXJJDzEuxNjRwt0IgXYQUkRYws6But/u+Bv8dm7p3\n",
       "qsQgIiIpSgwiIpKixCAiIilKDCIikqLEICIiKUoMIiKSojefRQY4s9Z2lid9X96JYW/8xZLBwP/i\n",
       "vR/W+jmwD7AQOB54JOeYRCTSOwxST55VSYPxbm33BjbDewbctGaffYGNgU2ALwDn5RhPfzG66AA6\n",
       "yOiiA+ggo4sOoIOMLjqAvi7PxLAj/nr6c8Ai4CpgbM0+BwKXxfn78Z4I184xpv5gdNEBdJDRRQfQ\n",
       "QUYXHUAHGV10AH1dnolhOOlucV+I67rbZwQiIlKYPBND1gat2jpONYSJiBQoz8bn2cDIxPJIvETQ\n",
       "aJ8RVAeqqKWEUdWwy+ABRteiSteiSteiF/JMDA/hjcrr493GHo43QCdNxLuWvQrYGXgTeIWl6ckJ\n",
       "EZE2yTMxLMZv+rfgTyhdBEwDTo7bzwduxJ9MmgksAE7IMR4REcmgr4zHICIibdJpXWLsDUzHB8ce\n",
       "18U+P4/bHwW2bVNcRejuWhyNX4MpwD3AVu0Lre2y/H8BsANeUj2oHUEVIMt1GI2/JPo4MKktURWj\n",
       "u2sxDLgZH2rzcfzl2f7qYrwK/rEG+/TsvhlC6JRpcAhhZghh/RDCsiGEySGETWv22TeEcGOc3ymE\n",
       "cF8HxF3UtdglhLBqnN97gF+Lyn63hxBuCCEc3AFxF3EdVgshPBFCGBGXh3VA3EVdi/EhhDMS1+H1\n",
       "EMIyHRB7HtOuIYRtQwiPdbG9x/fNTiox6IW4qizX4l7grTh/P/33/Y8s1wLgK8AE4LW2RdZeWa7D\n",
       "UfjA85Wn/+a0K7g2y3ItXgJWifOrAK/jpcn+6G7gjQbbe3zf7KTEoBfiqrJci6TP4w35/VHW/y/G\n",
       "Uu1SpT82nGW5DpsAawB34E8FHtue0Nouy7W4ENgcfyLyUeCr7QmtI/X4vtlJvavqhbiqnnyn3YHP\n",
       "AZ/IKZaiZbkW5wDfpDrweX98vDnLdVgW+BgwBlgRL1Xeh9ct9ydZrsW38faF0cBGwK3A1sDb+YXV\n",
       "0Xp03+ykxNDqF+L6sizXArzB+UK8Ia5RUbIvy3IttsOrE8AbHffBqxgm5h5d+2S5DrPw6qN34nQX\n",
       "fjPsb4khy7X4OPDDOP808CzwEbwkNdD0+L7ZSVVJyRfilsNfiKv9hz0ROC7ON3ohrq/Lci3WBa4D\n",
       "jsHrW/urLNdiQ2CDOE0Avlhnn74uy3X4EzAKf29oRWAnYGr7QmybLNdiOrBHnF8bTwrPtCm+TtPj\n",
       "+2YnlRj0QlxVlmvxPWB1qvXqi/BGuf4my7UYCLJch+n4I5pTgCV4abI/JoYs1+J04BK8fWEQ8A1g\n",
       "btsjbY8rgd3w0vIsvDuQZeO2pu6besFNRERSOqkqSUREOoASg4iIpCgxiIhIihKDiIikKDGIiEiK\n",
       "EoOIiKQoMYi4D/DuqqfgLw4ObfHxn8P7MQKY3+Jji7SUEoOIW4j3U78VMI/qy1KtErqYF+k4Sgwi\n",
       "S7sX73iN+PcmvBuGu/CuFcC7WfgD3lHbZLyrAeK6h/DBYU5qU7wiLdVJXWKIdILBwF7AX+LyBXjp\n",
       "YSbe99Cv8N5Lf453b/1Z/AdWperpc3iHhisAD+B9N/XXDg6ln1KXGCJuMT404nC8PWBnvCO6V4EZ\n",
       "if2Ww/v5fzXuu6jmOOOBz8T59fEk8wDeu+d2eH89bwMrt/wbiLSISgwi7h28jWEFvHO2scBteE+U\n",
       "XY2RW9vH/Wi8NLEz8C5eohiSQ6wiuVIbg0jaO8B/4H35z8d/6R8StxneOA1e1fTFOD8YHz5yFbza\n",
       "6F3go1SqIgQXAAAAb0lEQVTbHUT6FCUGEZesU52MtykcBhyND506GW9QPjDu81V89LwpeGPzpniX\n",
       "18vgXV2fgTdid3cukY6jNgYREUlRiUFERFKUGEREJEWJQUREUpQYREQkRYlBRERSlBhERCRFiUFE\n",
       "RFKUGEREJOX/AbO+dlMw9rDOAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1bc913aba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The prediction performance of Tf-iDF and pattern of life in precision-recall curve\n",
    "\n",
    "featuresList, text_model = featureExtraction(groups)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The prediction performance of pattern of life in precision-recall curve\n",
    "\n",
    "featuresList = featureExtraction(groups, text_extraction=False)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "classifer.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"GB\",\"GB_Carlos\",\"JH\",\"JH+GB_Carlos\"]\n",
    "new_feature_lists = [featureExtraction(groups, text_extraction=False),emotionExtraction(groups), JHfeatureExtraction(groups),CombinedfeatureExtraction(groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The prediction performance of Tf-iDF model in precision-recall curve and top-50 relevant n-grams\n",
    "\n",
    "featuresList, text_model = featureExtraction(groups, polarity_extraction=False)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The false-positive test of experts and therapists. \n",
    "\n",
    "coach_polarities = getUsersPolarities(\"coach_tweets_emotion\")\n",
    "coach_timeSeries = groupFilter(timeSeriesTransform(coach_polarities))\n",
    "coach_feature, _ = getTextFeature([coach_timeSeries], text_model)\n",
    "coach_X = coach_feature[0]\n",
    "print(classifer.score(coach_X, np.zeros(coach_X.shape[0],dtype=int)))\n",
    "classifer.predict(coach_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performance comparison of different features \n",
    "plot_multiple_precision_recall(RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=1024),new_feature_lists,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
