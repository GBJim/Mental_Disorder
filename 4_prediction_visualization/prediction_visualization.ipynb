{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from random import randint\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from Levenshtein import *\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_stopwords(file_location=\"../SmartStoplist\"):\n",
    "    f = open(file_location)\n",
    "    stopwords = [line.strip() for line in f]\n",
    "    return stopwords + [\"http\",\"https\", \"don\", \"thi\",\"http \", \"co\",\"dont\",\"im\"]\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "stemmer = PorterStemmer()\n",
    "stopSet = set(stopwords)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = []\n",
    "    global stopSet\n",
    "    \n",
    "    for word in text.lower().split():\n",
    "        word = re.sub('[!@#$?\\'\\\"]|&amp', '',word)\n",
    "        stemmedWord = stemmer.stem_word(word)\n",
    "        condition_Word = word not in stopSet  and \"http\" not in word\n",
    "        condition_StemmedWord = stemmedWord not in stopSet\n",
    "        \n",
    "        if  condition_Word and  condition_StemmedWord:\n",
    "            words.append(stemmedWord)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Feature Extraction Functions\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getTFIDF(usersDataList):\n",
    "    textsList = []\n",
    "    for usersData in usersDataList:\n",
    "        texts = \"\"\n",
    "        for data in usersData.values():\n",
    "            for text in data[\"texts\"]:\n",
    "                texts += preprocessor(text) + \"\\n\"\n",
    "        textsList.append(texts)\n",
    " \n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    tfidf.fit(textsList)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "    \n",
    "def getEmotionFeature(usersDataList):\n",
    "    \n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    featuresList = []\n",
    "\n",
    "    \n",
    "    for usersData in usersDataList:\n",
    "        features = []\n",
    "        for data in usersData.values():\n",
    "            feature = np.array([])\n",
    "            emotionCompositions = np.zeros(len(emotions),dtype=float)\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                feature = np.concatenate((feature, (data[\"emotions\"].get(emotion, emptyHourTable()))))\n",
    "                emotionCompositions[i] = np.sum(data[\"emotions\"].get(emotion, emptyHourTable()))\n",
    "            feature = np.concatenate((feature,emotionCompositions))\n",
    "            features.append(feature)\n",
    "            \n",
    "        featuresList.append(np.array(features))\n",
    "        \n",
    "    return featuresList\n",
    "\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "       model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    \n",
    "    getText = lambda data : \"\\n\".join(data[\"texts\"])\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData.values():\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "def featureCombination(emotion_features, text_features):\n",
    "    featuresList = []\n",
    "    for i in range(len(emotion_features)):\n",
    "        new_features = csr_matrix(hstack((emotion_features[i], text_features[i])))\n",
    "        \n",
    "        featuresList.append(new_features)\n",
    "    return featuresList\n",
    "    \n",
    "\n",
    "    \n",
    "def featureExtraction(usersDataList, emotion_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    \n",
    "    featuresList = []\n",
    " \n",
    "    \n",
    "    if emotion_extraction:\n",
    "        emotion_features = getEmotionFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = emotion_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if emotion_extraction:\n",
    "            featuresList = featureCombination(emotion_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XYGenerator(featuresList,verbose=True):\n",
    "    X, Y = featuresList[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for x, y in featuresList[1:]:\n",
    "        try:\n",
    "            \n",
    "            X = np.concatenate((X, x))\n",
    "        except:\n",
    "            X = vstack([X, x])\n",
    "            \n",
    "        Y = np.concatenate((Y, y))\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showImportantFeatures(features_importance, text_model, n_features = 10):\n",
    "    \n",
    "    if text_model is None:\n",
    "        print(\"The top {} most important features:\\n\".format(n_features))\n",
    "        emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "        for vector in features_importance[:n_features]:\n",
    "            \n",
    "            if vector >= 192:\n",
    "                \n",
    "                emotion = emotions[vector - 192]\n",
    "                print(\"The {} ratio of users\".format(emotion))\n",
    "            \n",
    "            else:\n",
    "                emotion = emotions[int(vector/24)]\n",
    "                hour = vector % 24\n",
    "                print(\"The daily {} ratio at {} o'clock\".format(emotion, hour))\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"The following are the top {} most important features:\\n\".format(n_features))\n",
    "        vector2word = text_model.get_feature_names()\n",
    "        for vector in features_importance[:n_features]:\n",
    "            word = vector2word[vector]\n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting functions\n",
    "def plot_precision_recall(classifier, featuresList, n_fold=20):\n",
    "    \n",
    "    \n",
    "    \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "        \n",
    "    precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "    average_precision = average_precision_score(LABELS, SCORES)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    " \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(recall, precision, label='area = %0.2f' %  average_precision , color=\"green\",linewidth=3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier\n",
    "def plot_multiple_precision_recall(classifier, featuresLists,names,colors=['r','g','b',\"y\"] , n_fold=20):\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    \n",
    "    for g,featuresList in enumerate(featuresLists):\n",
    "\n",
    "        SCORES = np.array([])\n",
    "        LABELS = np.array([])\n",
    "\n",
    "        X, Y = XYGenerator(featuresList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "\n",
    "        for train_index, test_index in sss:\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            classifier.fit(X_train, Y_train)\n",
    "            score = classifier.predict_proba(X_test)[:,1]\n",
    "            SCORES = np.concatenate((SCORES, score))\n",
    "            LABELS = np.concatenate((LABELS, Y_test))\n",
    "\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "        average_precision = average_precision_score(LABELS, SCORES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        BPD_train = np.count_nonzero(Y_train)\n",
    "        BPD_test = np.count_nonzero(Y_test)\n",
    "        normal_train = Y_train.shape[0] - BPD_train\n",
    "        normal_test = Y_test.shape[0] - BPD_test\n",
    "\n",
    "       \n",
    "\n",
    "        plt.plot(recall, precision, label=names[g] , color=colors[g],linewidth=3)\n",
    "      \n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "\n",
    "    plt.show()\n",
    "def plot_ROC(classifier, featuresList, n_fold=20):\n",
    "      \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "      \n",
    "    fpr, tpr, _ = roc_curve(LABELS, SCORES, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "       \n",
    "   \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "    \n",
    "  \n",
    "   \n",
    " \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, color=\"green\",linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', color=\"w\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#John Hopkins Patten of lifes:\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def tweetRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    delta_time = np.max(timeSeries.index.values) - np.min(timeSeries.index.values)\n",
    "    totla_duration = (delta_time).astype('timedelta64[h]') / np.timedelta64(24, 'h')\n",
    "    return total_tweets / totla_duration\n",
    "def mentioRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    total_mentions = np.sum(seriesContains(timeSeries, method=\"third\"))\n",
    "    return total_mentions / total_tweets\n",
    "\n",
    "def uniqueMentions(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_set = set()\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_set.add(word)\n",
    "    return len(friends_set)\n",
    "\n",
    "def frequentMentions(timeSeries, lowerbound = 3):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_mentions = {}\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_mentions[word] = friends_mentions.get(word, 0) +1\n",
    "    frequent_frients = [screen_name for screen_name, mentions in friends_mentions.items() if mentions >= lowerbound]\n",
    "    return len(frequent_frients)\n",
    "\n",
    "def getNegativeRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1) / total_tweets\n",
    "\n",
    "\n",
    "def getPositiveRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == 1) / total_tweets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new functions\n",
    "\n",
    "def getNegativeCount(timeSeries):\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1)\n",
    "\n",
    "\n",
    "def getUsersPolarities(collectionName):\n",
    "    collection = MongoClient(\"localhost\", 27017)['idea'][collectionName]\n",
    "    usersPolarties = {}\n",
    "    for tweet in collection.find():\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        #Processing emotions from Carlos' API\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]\n",
    "            \n",
    "        ambiguous = True if tweet['emotion']['ambiguous'] == 'yes' else False\n",
    "       \n",
    "        if len(tweet[\"emotion\"][\"groups\"]) > 1:\n",
    "            emotion_2 = tweet[\"emotion\"][\"groups\"][1][\"name\"]    \n",
    "        else:\n",
    "            emotion_2 = None\n",
    "        if tweet[\"polarity\"] == \"positive\":\n",
    "            polarity = 1\n",
    "        elif tweet[\"polarity\"] == \"negative\":\n",
    "            polarity = -1\n",
    "        else:\n",
    "            polarity = 0\n",
    "   \n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "\n",
    "        if userID not in usersPolarties:\n",
    "            usersPolarties[userID] = {}\n",
    "        if date not in usersPolarties[userID]:\n",
    "            usersPolarties[userID][date] = {}\n",
    "        usersPolarties[userID][date]['text'] = text\n",
    "        usersPolarties[userID][date]['polarity'] =  polarity\n",
    "        usersPolarties[userID][date]['emotion'] =  emotion\n",
    "        usersPolarties[userID][date]['emotion_2'] =  emotion_2\n",
    "        usersPolarties[userID][date]['ambiguous'] =  ambiguous\n",
    "    return usersPolarties\n",
    "\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "\n",
    "\n",
    "def userVerify(timeSeries, threshold = 0.5, lowerbound = 50):\n",
    "    http_rows = getHTTPRows(timeSeries)\n",
    "    average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "    duration = np.max(timeSeries.index.values) -  np.min(timeSeries.index.values)\n",
    "    duration = duration.astype('timedelta64[s]') / np.timedelta64(604800, 's')\n",
    "    return (average_http_count < threshold) and (timeSeries.shape[0] > lowerbound) and duration > 1\n",
    "\n",
    "\n",
    "def groupFilter(group):\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        if userVerify(timeSeries):\n",
    "            new_group.append(cleanPost(timeSeries))\n",
    "    return new_group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "\n",
    "\n",
    "def cleanPost(timeSeries):\n",
    "    left_text = timeSeries['text'].values[:-1]\n",
    "    right_text = timeSeries['text'].values[1:]\n",
    "    conditions = np.ones(timeSeries.shape[0],dtype=bool)\n",
    "    edit_distance = np.vectorize(distance)\n",
    "    conditions[:-1] =  conditions[:-1] & (edit_distance(left_text, right_text) > 5)\n",
    "    patterns = ['http://','https://']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        conditions = conditions & np.logical_not(timeSeries['text'].str.contains(pattern).values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "\n",
    "    return timeSeries\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def seriesContains(timeSeries,method =\"first\"):\n",
    "    if method == \"first\":\n",
    "        match_function = np.vectorize(firstPronuonDetect)\n",
    "    elif method == \"second\":\n",
    "        match_function = np.vectorize(secondPronuonDetect)\n",
    "    elif method == \"third\":\n",
    "            match_function = np.vectorize(thirdPronuonDetect)\n",
    "\n",
    "\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n",
    "    \n",
    "\n",
    "def firstPronuonDetect(words, matchers=[\"i\",\"we\",\"i'd\",\"i'm\"]):\n",
    "    for matcher in matchers:\n",
    "        if matcher in words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def getFirstPronounCount(timeSeries):\n",
    "    return np.sum(seriesContains(timeSeries))\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getCombosCount(timeSeries, matcher = -1, lowerbound = 2):\n",
    "    combos = comboTracker(timeSeries)\n",
    "    combos_count = sum([hit for element, hit in combos if element == matcher and hit > lowerbound])\n",
    "    return combos_count\n",
    "    \n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def getFlipsCount(timeSeries, upperbound=30, lowerbound = 0):\n",
    "    flips = getFlips(timeSeries)\n",
    "    durations = getFlipsDuration(timeSeries, flips)\n",
    "    return np.sum((durations > lowerbound) & (durations < upperbound) )\n",
    "\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "        model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "        to_train = True\n",
    "    else:\n",
    "        model = text_model\n",
    "        to_train = False\n",
    "        \n",
    "    getText = lambda data : \"\\n\".join(data[\"text\"].values)\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData:\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    \n",
    "    if to_train:\n",
    "        model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "def getPolarityFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),4),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            negative_ratio = getNegativeCount(timeSeries) / tweets_length\n",
    "            feature[i][0] = flips_ratio \n",
    "            feature[i][1] = combos_ratio\n",
    "            feature[i][2] = negative_ratio\n",
    "            feature[i][3] = first_pronoun_ratio\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def getEmotionFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),4+8),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            negative_ratio = getNegativeCount(timeSeries) / tweets_length\n",
    "            feature[i][0] = flips_ratio \n",
    "            feature[i][1] = combos_ratio\n",
    "            feature[i][2] = negative_ratio\n",
    "            feature[i][3] = first_pronoun_ratio\n",
    "            #emotions come in\n",
    "            \n",
    "            feature[i][4:] = getEmotionRatio(timeSeries)\n",
    "            \n",
    "            \n",
    "            \n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def getEmotionRatio(timeSeries):\n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    emotion_ratios = []\n",
    "    conditions = np.logical_not(timeSeries[\"ambiguous\"].values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        total = np.sum((timeSeries[\"emotion\"].values == emotion))\n",
    "        emotion_ratio = total / timeSeries.shape[0]\n",
    "        emotion_ratios.append(emotion_ratio)\n",
    "        \n",
    "    return emotion_ratios\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getJHFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),6),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "         \n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = negative_ratio\n",
    "            feature[i][5] = positive_ratio\n",
    "          \n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def getCombinedFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),9+8),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = positive_ratio\n",
    "            feature[i][5] = negative_ratio\n",
    "            feature[i][6] = flips_ratio\n",
    "            feature[i][7] = combos_ratio\n",
    "            feature[i][8] = first_pronoun_ratio\n",
    "            feature[i][9:] = getEmotionRatio(timeSeries)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def featureExtraction(usersDataList, polarity_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    featuresList = []\n",
    " \n",
    "\n",
    "    if polarity_extraction:\n",
    "        polarity_features = getPolarityFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = polarity_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if polarity_extraction:\n",
    "            featuresList = featureCombination(polarity_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "\n",
    "def emotionExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList = getEmotionFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "    \n",
    "def JHfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList = getJHFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) \n",
    "\n",
    "    \n",
    "def CombinedfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList =getCombinedFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Fetching the data of BPD and random sampled Twitter user\n",
    "\n",
    "BPD_polarities = getUsersPolarities(\"BPD_581_emotion\")\n",
    "regular_polarities = getUsersPolarities(\"regularUser_en_fixed_emotion\")\n",
    "#Transform raw tweets into timeSeries data and filter and clean the timeSeries data\n",
    "BPD_timeSeries = groupFilter(timeSeriesTransform(BPD_polarities))\n",
    "regular_timeSeries = groupFilter(timeSeriesTransform(regular_polarities))\n",
    "groups = [regular_timeSeries, BPD_timeSeries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The prediction performance of Tf-iDF and pattern of life in precision-recall curve\n",
    "\n",
    "featuresList, text_model = featureExtraction(groups)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The prediction performance of pattern of life in precision-recall curve\n",
    "\n",
    "featuresList = featureExtraction(groups, text_extraction=False)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "classifer.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"GB\",\"GB_Carlos\",\"JH\",\"JH+GB_Carlos\"]\n",
    "new_feature_lists = [featureExtraction(groups, text_extraction=False),emotionExtraction(groups), JHfeatureExtraction(groups),CombinedfeatureExtraction(groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The prediction performance of Tf-iDF model in precision-recall curve and top-50 relevant n-grams\n",
    "\n",
    "featuresList, text_model = featureExtraction(groups, polarity_extraction=False)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The false-positive test of experts and therapists. \n",
    "\n",
    "coach_polarities = getUsersPolarities(\"coach_tweets_emotion\")\n",
    "coach_timeSeries = groupFilter(timeSeriesTransform(coach_polarities))\n",
    "coach_feature, _ = getTextFeature([coach_timeSeries], text_model)\n",
    "coach_X = coach_feature[0]\n",
    "print(classifer.score(coach_X, np.zeros(coach_X.shape[0],dtype=int)))\n",
    "classifer.predict(coach_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performance comparison of different features \n",
    "plot_multiple_precision_recall(RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=1024),new_feature_lists,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
