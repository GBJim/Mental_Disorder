{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from random import randint\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from Levenshtein import *\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_stopwords(file_location=\"SmartStoplist\"):\n",
    "    f = open(file_location)\n",
    "    stopwords = [line.strip() for line in f]\n",
    "    return stopwords + [\"http\",\"https\", \"don\", \"thi\",\"http \", \"co\",\"dont\",\"im\"]\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "stemmer = PorterStemmer()\n",
    "stopSet = set(stopwords)\n",
    "\n",
    "def preprocessor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = []\n",
    "    global stopSet\n",
    "    \n",
    "    for word in text.lower().split():\n",
    "        word = re.sub('[!@#$?\\'\\\"]|&amp', '',word)\n",
    "        stemmedWord = stemmer.stem_word(word)\n",
    "        condition_Word = word not in stopSet  and \"http\" not in word\n",
    "        condition_StemmedWord = stemmedWord not in stopSet\n",
    "        \n",
    "        if  condition_Word and  condition_StemmedWord:\n",
    "            words.append(stemmedWord)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emptyHourTable():\n",
    "    return np.zeros(24,dtype=float)\n",
    "\n",
    "\n",
    "def getEmotion(tweet):\n",
    "    emotion = tweet[\"groups\"][0][\"name\"]\n",
    "    return emotion\n",
    "\n",
    "def getLocalTime(time, zone):\n",
    "    zoneTable = {\"International Date Line West\":\"Pacific/Midway\",\"Midway Island\":\"Pacific/Midway\",\"American Samoa\":\"Pacific/Pago_Pago\",\"Hawaii\":\"Pacific/Honolulu\",\"Alaska\":\"America/Juneau\",\"Pacific Time (US & Canada)\":\"America/Los_Angeles\",\"Tijuana\":\"America/Tijuana\",\"Mountain Time (US & Canada)\":\"America/Denver\",\"Arizona\":\"America/Phoenix\",\"Chihuahua\":\"America/Chihuahua\",\"Mazatlan\":\"America/Mazatlan\",\"Central Time (US & Canada)\":\"America/Chicago\",\"Saskatchewan\":\"America/Regina\",\"Guadalajara\":\"America/Mexico_City\",\"Mexico City\":\"America/Mexico_City\",\"Monterrey\":\"America/Monterrey\",\"Central America\":\"America/Guatemala\",\"Eastern Time (US & Canada)\":\"America/New_York\",\"Indiana (East)\":\"America/Indiana/Indianapolis\",\"Bogota\":\"America/Bogota\",\"Lima\":\"America/Lima\",\"Quito\":\"America/Lima\",\"Atlantic Time (Canada)\":\"America/Halifax\",\"Caracas\":\"America/Caracas\",\"La Paz\":\"America/La_Paz\",\"Santiago\":\"America/Santiago\",\"Newfoundland\":\"America/St_Johns\",\"Brasilia\":\"America/Sao_Paulo\",\"Buenos Aires\":\"America/Argentina/Buenos_Aires\",\"Montevideo\":\"America/Montevideo\",\"Georgetown\":\"America/Guyana\",\"Greenland\":\"America/Godthab\",\"Mid-Atlantic\":\"Atlantic/South_Georgia\",\"Azores\":\"Atlantic/Azores\",\"Cape Verde Is.\":\"Atlantic/Cape_Verde\",\"Dublin\":\"Europe/Dublin\",\"Edinburgh\":\"Europe/London\",\"Lisbon\":\"Europe/Lisbon\",\"London\":\"Europe/London\",\"Casablanca\":\"Africa/Casablanca\",\"Monrovia\":\"Africa/Monrovia\",\"UTC\":\"Etc/UTC\",\"Belgrade\":\"Europe/Belgrade\",\"Bratislava\":\"Europe/Bratislava\",\"Budapest\":\"Europe/Budapest\",\"Ljubljana\":\"Europe/Ljubljana\",\"Prague\":\"Europe/Prague\",\"Sarajevo\":\"Europe/Sarajevo\",\"Skopje\":\"Europe/Skopje\",\"Warsaw\":\"Europe/Warsaw\",\"Zagreb\":\"Europe/Zagreb\",\"Brussels\":\"Europe/Brussels\",\"Copenhagen\":\"Europe/Copenhagen\",\"Madrid\":\"Europe/Madrid\",\"Paris\":\"Europe/Paris\",\"Amsterdam\":\"Europe/Amsterdam\",\"Berlin\":\"Europe/Berlin\",\"Bern\":\"Europe/Berlin\",\"Rome\":\"Europe/Rome\",\"Stockholm\":\"Europe/Stockholm\",\"Vienna\":\"Europe/Vienna\",\"West Central Africa\":\"Africa/Algiers\",\"Bucharest\":\"Europe/Bucharest\",\"Cairo\":\"Africa/Cairo\",\"Helsinki\":\"Europe/Helsinki\",\"Kyiv\":\"Europe/Kiev\",\"Riga\":\"Europe/Riga\",\"Sofia\":\"Europe/Sofia\",\"Tallinn\":\"Europe/Tallinn\",\"Vilnius\":\"Europe/Vilnius\",\"Athens\":\"Europe/Athens\",\"Istanbul\":\"Europe/Istanbul\",\"Minsk\":\"Europe/Minsk\",\"Jerusalem\":\"Asia/Jerusalem\",\"Harare\":\"Africa/Harare\",\"Pretoria\":\"Africa/Johannesburg\",\"Kaliningrad\":\"Europe/Kaliningrad\",\"Moscow\":\"Europe/Moscow\",\"St. Petersburg\":\"Europe/Moscow\",\"Volgograd\":\"Europe/Volgograd\",\"Samara\":\"Europe/Samara\",\"Kuwait\":\"Asia/Kuwait\",\"Riyadh\":\"Asia/Riyadh\",\"Nairobi\":\"Africa/Nairobi\",\"Baghdad\":\"Asia/Baghdad\",\"Tehran\":\"Asia/Tehran\",\"Abu Dhabi\":\"Asia/Muscat\",\"Muscat\":\"Asia/Muscat\",\"Baku\":\"Asia/Baku\",\"Tbilisi\":\"Asia/Tbilisi\",\"Yerevan\":\"Asia/Yerevan\",\"Kabul\":\"Asia/Kabul\",\"Ekaterinburg\":\"Asia/Yekaterinburg\",\"Islamabad\":\"Asia/Karachi\",\"Karachi\":\"Asia/Karachi\",\"Tashkent\":\"Asia/Tashkent\",\"Chennai\":\"Asia/Kolkata\",\"Kolkata\":\"Asia/Kolkata\",\"Mumbai\":\"Asia/Kolkata\",\"New Delhi\":\"Asia/Kolkata\",\"Kathmandu\":\"Asia/Kathmandu\",\"Astana\":\"Asia/Dhaka\",\"Dhaka\":\"Asia/Dhaka\",\"Sri Jayawardenepura\":\"Asia/Colombo\",\"Almaty\":\"Asia/Almaty\",\"Novosibirsk\":\"Asia/Novosibirsk\",\"Rangoon\":\"Asia/Rangoon\",\"Bangkok\":\"Asia/Bangkok\",\"Hanoi\":\"Asia/Bangkok\",\"Jakarta\":\"Asia/Jakarta\",\"Krasnoyarsk\":\"Asia/Krasnoyarsk\",\"Beijing\":\"Asia/Shanghai\",\"Chongqing\":\"Asia/Chongqing\",\"Hong Kong\":\"Asia/Hong_Kong\",\"Urumqi\":\"Asia/Urumqi\",\"Kuala Lumpur\":\"Asia/Kuala_Lumpur\",\"Singapore\":\"Asia/Singapore\",\"Taipei\":\"Asia/Taipei\",\"Perth\":\"Australia/Perth\",\"Irkutsk\":\"Asia/Irkutsk\",\"Ulaanbaatar\":\"Asia/Ulaanbaatar\",\"Seoul\":\"Asia/Seoul\",\"Osaka\":\"Asia/Tokyo\",\"Sapporo\":\"Asia/Tokyo\",\"Tokyo\":\"Asia/Tokyo\",\"Yakutsk\":\"Asia/Yakutsk\",\"Darwin\":\"Australia/Darwin\",\"Adelaide\":\"Australia/Adelaide\",\"Canberra\":\"Australia/Melbourne\",\"Melbourne\":\"Australia/Melbourne\",\"Sydney\":\"Australia/Sydney\",\"Brisbane\":\"Australia/Brisbane\",\"Hobart\":\"Australia/Hobart\",\"Vladivostok\":\"Asia/Vladivostok\",\"Guam\":\"Pacific/Guam\",\"Port Moresby\":\"Pacific/Port_Moresby\",\"Magadan\":\"Asia/Magadan\",\"Srednekolymsk\":\"Asia/Srednekolymsk\",\"Solomon Is.\":\"Pacific/Guadalcanal\",\"New Caledonia\":\"Pacific/Noumea\",\"Fiji\":\"Pacific/Fiji\",\"Kamchatka\":\"Asia/Kamchatka\",\"Marshall Is.\":\"Pacific/Majuro\",\"Auckland\":\"Pacific/Auckland\",\"Wellington\":\"Pacific/Auckland\",\"Nuku'alofa\":\"Pacific/Tongatapu\",\"Tokelau Is.\":\"Pacific/Fakaofo\",\"Chatham Is.\":\"Pacific/Chatham\",\"Samoa\":\"Pacific/Apia\"}\n",
    "    offset = timezone(zoneTable[zone]).utcoffset(datetime.now())\n",
    "    return time + offset\n",
    "\n",
    "def getTweetHour(tweet):\n",
    "    zone = tweet[\"user\"][\"time_zone\"]\n",
    "    return getLocalTime(tweet[\"created_at\"], zone).hour\n",
    "\n",
    "\n",
    "\n",
    "def getTweetSentimentHour(tweet):\n",
    "    zone = tweet[\"user\"][\"time_zone\"]\n",
    "    return getLocalTime(tweet[\"created_at\"], zone).hour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getUserEmotionFrequency(collectionName, timeZone=True, regularization=True, percentage = False):\n",
    "\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    collection = client['idea'][collectionName]\n",
    "    userFrequency = {}\n",
    "\n",
    "    for tweet in collection.find():\n",
    "        zone = tweet[\"user\"][\"time_zone\"]\n",
    "        emotion =  tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "        if zone is not None:\n",
    "            userID = tweet[\"user\"][\"id\"]\n",
    "            hour = getTweetHour(tweet)\n",
    "            \n",
    "            if userID not in userFrequency:\n",
    "                userFrequency[userID] = {}\n",
    "            if emotion not in userFrequency[userID]:\n",
    "                    userFrequency[userID][emotion] = emptyHourTable()\n",
    "                    \n",
    "            userFrequency[userID][emotion][hour] += 1\n",
    "    \n",
    "   \n",
    "    if regularization:\n",
    "        for userID in userFrequency:\n",
    "            for emotion in userFrequency[userID]:\n",
    "                total = sum(userFrequency[userID][emotion])\n",
    "                if percentage:\n",
    "                    total /= 100\n",
    "                for i in range(24):\n",
    "                    userFrequency[userID][emotion][i] /=  total\n",
    "            \n",
    "    return userFrequency\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def getUserFrequency(collectionName, timeZone=True, regularization=True, percentage = False):\n",
    "\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    collection = client['idea'][collectionName]\n",
    "    userFrequency = {}\n",
    "\n",
    "    for tweet in collection.find():\n",
    "        zone = tweet[\"user\"][\"time_zone\"]\n",
    "        if zone is not None:\n",
    "            userID = tweet[\"user\"][\"id\"]\n",
    "            hour = getTweetHour(tweet)\n",
    "            \n",
    "            if userID not in userFrequency:\n",
    "                userFrequency[userID] = emptyHourTable()\n",
    "            userFrequency[userID][hour] += 1\n",
    "    \n",
    "    if regularization:\n",
    "        for userID in userFrequency:\n",
    "            total = sum(userFrequency[userID])\n",
    "            if percentage:\n",
    "                total /= 100\n",
    "            for i in range(24):\n",
    "                userFrequency[userID][i] /=  total\n",
    "            \n",
    "    return userFrequency\n",
    "\n",
    "\n",
    "def frequencyPlot(userFrequency):\n",
    "   \n",
    "    X = []\n",
    "    Y = []\n",
    "    for hourTable in userFrequency.values():\n",
    "        for i in range(24):\n",
    "            if hourTable[i] < 0.3:\n",
    "                X.append(i)\n",
    "                Y.append(hourTable[i])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    fig, ax = plt.subplots()\n",
    "    fit = np.polyfit(X, Y, deg=1)\n",
    "    ax.plot(X, fit[0] * X + fit[1], color='red')\n",
    "    ax.scatter(X, Y)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "def averagePlot(userFrequency, title):\n",
    "    \n",
    "    X = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "    Y = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for hourTable in userFrequency.values():\n",
    "        for i in range(24): \n",
    "            Y[i] += hourTable[i]\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y /= len(userFrequency)\n",
    "    fig, ax = plt.subplots()\n",
    "    fit = np.polyfit(X, Y, deg=3)\n",
    "    ax.plot(X, fit[0] * X**3+ + fit[1]*X**2+fit[2]*X+fit[3], color='red')\n",
    "    ax.scatter(X, Y)\n",
    "    ax.set_xlabel(\"Hour\",  fontsize=14)\n",
    "    ax.set_ylabel(\"Probablity to Post\",  fontsize=14)\n",
    "    fig.suptitle(\"{} {} people\".format(len(userFrequency), title),  fontsize=18)\n",
    "    \n",
    "    fig.show()   \n",
    "\n",
    "  \n",
    "\n",
    "def personalPlot(userHour):  \n",
    "        \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(24):\n",
    "        if userHour[i] > 0:\n",
    "            X.append(i)\n",
    "            Y.append(userHour[i])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    fig, ax = plt.subplots()\n",
    "   \n",
    "    fit = np.polyfit(X, Y, deg=2)\n",
    "    ax.plot(X, fit[0] * X**2+ + fit[1]*X+fit[2], color='red')\n",
    "    ax.scatter(X, Y)\n",
    "    plt.axis((0,24,0,0.2))\n",
    "    fig.show()\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def getUsersData(collectionName, time= True):\n",
    "    \n",
    "    \n",
    "    client = MongoClient('localhost', 27017)\n",
    "    collection = client['idea'][collectionName]\n",
    "    usersData = {}\n",
    "    \n",
    "    if time:\n",
    "\n",
    "        for tweet in collection.find():\n",
    "            zone = tweet[\"user\"][\"time_zone\"]\n",
    "            if zone is not None:\n",
    "                userID = tweet[\"user\"][\"id\"]\n",
    "                hour = getTweetHour(tweet)\n",
    "                text = tweet[\"text\"]\n",
    "                emotion = tweet[\"emotion\"][\"groups\"][0][\"name\"]\n",
    "\n",
    "                if userID not in usersData:\n",
    "                    usersData[userID] = {\"emotions\":{}, \"texts\": []}\n",
    "                if emotion not in usersData[userID][\"emotions\"]:\n",
    "                    usersData[userID][\"emotions\"][emotion] = emptyHourTable()\n",
    "                usersData[userID][\"emotions\"][emotion][hour] += 1\n",
    "                usersData[userID][\"texts\"].append(text)\n",
    "\n",
    "        for userID, data in usersData.items():\n",
    "            total = len(data[\"texts\"])\n",
    "            for emotion in data[\"emotions\"]:\n",
    "                usersData[userID][\"emotions\"][emotion] /= total\n",
    "                \n",
    "    else:\n",
    "         for tweet in collection.find():\n",
    "            userID = tweet[\"user\"][\"id\"]\n",
    "            text = tweet[\"text\"]\n",
    "\n",
    "            if userID not in usersData:\n",
    "                usersData[userID] = {\"emotions\":{}, \"texts\": []}\n",
    "            \n",
    "            usersData[userID][\"texts\"].append(text)\n",
    "            \n",
    "    return usersData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getTFIDF(usersDataList):\n",
    "    textsList = []\n",
    "    for usersData in usersDataList:\n",
    "        texts = \"\"\n",
    "        for data in usersData.values():\n",
    "            for text in data[\"texts\"]:\n",
    "                texts += preprocessor(text) + \"\\n\"\n",
    "        textsList.append(texts)\n",
    " \n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    tfidf.fit(textsList)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "    \n",
    "def getEmotionFeature(usersDataList):\n",
    "    \n",
    "    emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "    featuresList = []\n",
    "\n",
    "    \n",
    "    for usersData in usersDataList:\n",
    "        features = []\n",
    "        for data in usersData.values():\n",
    "            feature = np.array([])\n",
    "            emotionCompositions = np.zeros(len(emotions),dtype=float)\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                feature = np.concatenate((feature, (data[\"emotions\"].get(emotion, emptyHourTable()))))\n",
    "                emotionCompositions[i] = np.sum(data[\"emotions\"].get(emotion, emptyHourTable()))\n",
    "            feature = np.concatenate((feature,emotionCompositions))\n",
    "            features.append(feature)\n",
    "            \n",
    "        featuresList.append(np.array(features))\n",
    "        \n",
    "    return featuresList\n",
    "\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "       model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "    \n",
    "    getText = lambda data : \"\\n\".join(data[\"texts\"])\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData.values():\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "def featureCombination(emotion_features, text_features):\n",
    "    featuresList = []\n",
    "    for i in range(len(emotion_features)):\n",
    "        new_features = csr_matrix(hstack((emotion_features[i], text_features[i])))\n",
    "        \n",
    "        featuresList.append(new_features)\n",
    "    return featuresList\n",
    "    \n",
    "\n",
    "    \n",
    "def featureExtraction(usersDataList, emotion_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    \n",
    "    featuresList = []\n",
    " \n",
    "    \n",
    "    if emotion_extraction:\n",
    "        emotion_features = getEmotionFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = emotion_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if emotion_extraction:\n",
    "            featuresList = featureCombination(emotion_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainTestGenerator(featuresList, randomState, verbose=True):\n",
    "    x, y = featuresList[0]\n",
    "    X_TRAIN,X_TEST, Y_TRAIN,Y_TEST = train_test_split(x,y, random_state=randomState)\n",
    "    \n",
    "    normal_train = X_TRAIN.shape[0]\n",
    "    normal_test = X_TEST.shape[0]\n",
    "    \n",
    "    for x, y in featuresList[1:]:\n",
    "        x_train,x_test, y_train, y_test = train_test_split(x,y, random_state=randomState)\n",
    "        \n",
    "        X_TRAIN = vstack([X_TRAIN, x_train])    \n",
    "        X_TEST = vstack([X_TEST, x_test]) \n",
    "        Y_TRAIN = np.concatenate((Y_TRAIN, y_train))   \n",
    "        Y_TEST = np.concatenate((Y_TEST, y_test))\n",
    "        \n",
    "        BPD_train = x_train.shape[0]\n",
    "        BPD_test = x_test.shape[0]\n",
    "        \n",
    " \n",
    "    \n",
    "    return X_TRAIN, X_TEST, Y_TRAIN, Y_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XYGenerator(featuresList,verbose=True):\n",
    "    X, Y = featuresList[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for x, y in featuresList[1:]:\n",
    "        try:\n",
    "            \n",
    "            X = np.concatenate((X, x))\n",
    "        except:\n",
    "            X = vstack([X, x])\n",
    "            \n",
    "        Y = np.concatenate((Y, y))\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_ROC(classifier, featuresList, n_fold=20):\n",
    "      \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "      \n",
    "    fpr, tpr, _ = roc_curve(LABELS, SCORES, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "       \n",
    "   \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "    \n",
    "  \n",
    "   \n",
    " \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, color=\"green\",linewidth=3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', color=\"w\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showImportantFeatures(features_importance, text_model, n_features = 10):\n",
    "    \n",
    "    if text_model is None:\n",
    "        print(\"The top {} most important features:\\n\".format(n_features))\n",
    "        emotions = ['surprise', 'fear', 'sadness', 'disgust', 'trust', 'anticipation', 'anger','joy']\n",
    "        for vector in features_importance[:n_features]:\n",
    "            \n",
    "            if vector >= 192:\n",
    "                \n",
    "                emotion = emotions[vector - 192]\n",
    "                print(\"The {} ratio of users\".format(emotion))\n",
    "            \n",
    "            else:\n",
    "                emotion = emotions[int(vector/24)]\n",
    "                hour = vector % 24\n",
    "                print(\"The daily {} ratio at {} o'clock\".format(emotion, hour))\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"The following are the top {} most important features:\\n\".format(n_features))\n",
    "        vector2word = text_model.get_feature_names()\n",
    "        for vector in features_importance[:n_features]:\n",
    "            word = vector2word[vector]\n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(classifier, featuresList, n_fold=20):\n",
    "    \n",
    "    \n",
    "    \n",
    "    SCORES = np.array([])\n",
    "    LABELS = np.array([])\n",
    "    \n",
    "    X, Y = XYGenerator(featuresList)\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss:\n",
    "      \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        score = classifier.predict_proba(X_test)[:,1]\n",
    "        SCORES = np.concatenate((SCORES, score))\n",
    "        LABELS = np.concatenate((LABELS, Y_test))\n",
    "        \n",
    "        \n",
    "    precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "    average_precision = average_precision_score(LABELS, SCORES)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    BPD_train = np.count_nonzero(Y_train)\n",
    "    BPD_test = np.count_nonzero(Y_test)\n",
    "    normal_train = Y_train.shape[0] - BPD_train\n",
    "    normal_test = Y_test.shape[0] - BPD_test\n",
    "    \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    " \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    \n",
    "    plt.plot(recall, precision, label='area = %0.2f' %  average_precision , color=\"green\",linewidth=3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    try:\n",
    "        print(\"Number of features: {}\".format(len(classifier.feature_importances_)))\n",
    "    except:\n",
    "        print(\"Number of features: {}\".format(len(classifier.coef_[0])))\n",
    "    plt.show()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ef551ee17f11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#John Hopkins Patten of lifes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mthirdPronuonDetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"@[a-z]+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"@\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "#John Hopkins Patten of lifes:\n",
    "\n",
    "def thirdPronuonDetect(words, matcher=re.compile(\"@[a-z]+\")):\n",
    "    for word in words:\n",
    "        if word == \"@\":\n",
    "            continue\n",
    "        elif matcher.search(word):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def tweetRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    delta_time = np.max(timeSeries.index.values) - np.min(timeSeries.index.values)\n",
    "    totla_duration = (delta_time).astype('timedelta64[h]') / np.timedelta64(24, 'h')\n",
    "    return total_tweets / totla_duration\n",
    "def mentioRate(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    total_mentions = np.sum(seriesContains(timeSeries, method=\"third\"))\n",
    "    return total_mentions / total_tweets\n",
    "\n",
    "def uniqueMentions(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_set = set()\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_set.add(word)\n",
    "    return len(friends_set)\n",
    "\n",
    "def frequentMentions(timeSeries, lowerbound = 3):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    friends_mentions = {}\n",
    "    texts = timeSeries[\"text\"].values\n",
    "    for text in texts:\n",
    "        terms = text.strip().split()\n",
    "        for word in terms:\n",
    "            if word[0] == '@' and len(word) > 1:\n",
    "                friends_mentions[word] = friends_mentions.get(word, 0) +1\n",
    "    frequent_frients = [screen_name for screen_name, mentions in friends_mentions.items() if mentions >= lowerbound]\n",
    "    return len(frequent_frients)\n",
    "\n",
    "def getNegativeRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1) / total_tweets\n",
    "\n",
    "\n",
    "def getPositiveRatio(timeSeries):\n",
    "    total_tweets = timeSeries.shape[0]\n",
    "    return np.sum(timeSeries[\"polarity\"].values == 1) / total_tweets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = regular_timeSeries[89]\n",
    "\n",
    "frequentMentions(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BPDUsersData = getUsersData(\"BPD_fixed_emotion\")\n",
    "regularUsersData = getUsersData(\"regularUser_en_fixed_emotion\")\n",
    "featuresList = featureExtraction([regularUsersData, BPDUsersData], text_extraction=False)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "\n",
    "classifer = plot_ROC(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))[::-1]\n",
    "showImportantFeatures(featureScores, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))[::-1]\n",
    "#showImportantFeatures(featureScores, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BPDUsersData_text = getUsersData(\"BPD_fixed_emotion\",time=False)\n",
    "regularUsersData_text = getUsersData(\"regularUser_en_fixed_emotion\",time=False)\n",
    "featuresList, text_model = featureExtraction([regularUsersData_text, BPDUsersData_text], emotion_extraction=False)\n",
    "X_train, X_test, Y_train, Y_test = trainTestGenerator(featuresList, randomState =90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifer = plot_ROC(RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128), featuresList)\n",
    "classifer = plot_ROC(forest, featuresList)\n",
    "\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#classifer = plot_precision_recall(RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128), X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "classifer = plot_precision_recall(forest,featuresList)\n",
    "\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "#showImportantFeatures(featureScores, text_model, n_features=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regularUsersData_text = getUsersData(\"regularUser_en_fixed_emotion\", time=False)\n",
    "BPDUsersData_text =getUsersData(\"BPD_581\",time=False)\n",
    "featuresList, text_model = featureExtraction([regularUsersData_text, BPDUsersData_text], emotion_extraction=False)\n",
    "X_train, X_test, Y_train, Y_test = trainTestGenerator(featuresList, randomState =90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresList[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_ROC(forest, featuresList)\n",
    "\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest,featuresList)\n",
    "\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new functions\n",
    "\n",
    "def getNegativeCount(timeSeries):\n",
    "    return np.sum(timeSeries[\"polarity\"].values == -1)\n",
    "\n",
    "\n",
    "def getUsersPolarities(collectionName):\n",
    "    collection = MongoClient(\"localhost\", 27017)['idea'][collectionName]\n",
    "    usersPolarties = {}\n",
    "    for tweet in collection.find():\n",
    "        userID = tweet[\"user\"][\"id\"]\n",
    "        if tweet[\"polarity\"] == \"positive\":\n",
    "            polarity = 1\n",
    "        elif tweet[\"polarity\"] == \"negative\":\n",
    "            polarity = -1\n",
    "        else:\n",
    "            polarity = 0\n",
    "   \n",
    "            \n",
    "        date = tweet[\"created_at\"]\n",
    "        text = tweet['text']\n",
    "\n",
    "        if userID not in usersPolarties:\n",
    "            usersPolarties[userID] = {}\n",
    "        if date not in usersPolarties[userID]:\n",
    "            usersPolarties[userID][date] = {}\n",
    "        usersPolarties[userID][date]['text'] = text\n",
    "        usersPolarties[userID][date]['polarity'] =  polarity\n",
    "    return usersPolarties\n",
    "\n",
    "\n",
    "def timeSeriesTransform(usersEmotions):\n",
    "    for userID in usersEmotions:\n",
    "        usersEmotions[userID] = pd.DataFrame.from_dict(usersEmotions[userID], orient='index').fillna(0)\n",
    "        usersEmotions[userID]['dt'] = np.zeros(usersEmotions[userID].shape[0],dtype=float)\n",
    "        usersEmotions[userID].loc[:-1,'dt'] = (usersEmotions[userID].index[1:].values - usersEmotions[userID].index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return list(usersEmotions.values())\n",
    "\n",
    "\n",
    "\n",
    "def userVerify(timeSeries, threshold = 0.5, lowerbound = 50):\n",
    "    http_rows = getHTTPRows(timeSeries)\n",
    "    average_http_count = np.sum(http_rows) / timeSeries.shape[0]\n",
    "    duration = np.max(timeSeries.index.values) -  np.min(timeSeries.index.values)\n",
    "    duration = duration.astype('timedelta64[s]') / np.timedelta64(604800, 's')\n",
    "    return (average_http_count < threshold) and (timeSeries.shape[0] > lowerbound) and duration > 1\n",
    "\n",
    "\n",
    "def groupFilter(group):\n",
    "    new_group = []\n",
    "    for timeSeries in group:\n",
    "        if userVerify(timeSeries):\n",
    "            new_group.append(cleanPost(timeSeries))\n",
    "    return new_group\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "def getHTTPRows(timeSeries):\n",
    "    count = 0\n",
    "    patterns = ['http://','https://']\n",
    "    conditions = timeSeries['text'].str.contains(patterns[0])\n",
    "    for pattern in patterns[1:]:\n",
    "        conditions = conditions | timeSeries['text'].str.contains(pattern)\n",
    "\n",
    "    return conditions.values\n",
    "\n",
    "\n",
    "def cleanPost(timeSeries):\n",
    "    left_text = timeSeries['text'].values[:-1]\n",
    "    right_text = timeSeries['text'].values[1:]\n",
    "    conditions = np.ones(timeSeries.shape[0],dtype=bool)\n",
    "    edit_distance = np.vectorize(distance)\n",
    "    conditions[:-1] =  conditions[:-1] & (edit_distance(left_text, right_text) > 5)\n",
    "    patterns = ['http://','https://']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        conditions = conditions & np.logical_not(timeSeries['text'].str.contains(pattern).values)\n",
    "    timeSeries = timeSeries[conditions]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "\n",
    "    return timeSeries\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def seriesContains(timeSeries,method =\"first\"):\n",
    "    if method == \"first\":\n",
    "        match_function = np.vectorize(firstPronuonDetect)\n",
    "    elif method == \"second\":\n",
    "        match_function = np.vectorize(secondPronuonDetect)\n",
    "    elif method == \"third\":\n",
    "            match_function = np.vectorize(thirdPronuonDetect)\n",
    "\n",
    "\n",
    "    return match_function(timeSeries[\"text\"].str.lower().str.split().values)\n",
    "    \n",
    "\n",
    "def firstPronuonDetect(words, matchers=[\"i\",\"we\",\"i'd\",\"i'm\"]):\n",
    "    for matcher in matchers:\n",
    "        if matcher in words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def getFirstPronounCount(timeSeries):\n",
    "    return np.sum(seriesContains(timeSeries))\n",
    "\n",
    "def comboTracker(timeSeries, attribute= \"polarity\"):\n",
    "    array = timeSeries[attribute]\n",
    "    starter = array[0]\n",
    "    combo = 1\n",
    "    result = []\n",
    "    for cursor in array[1:]:\n",
    "        if starter == cursor:\n",
    "            combo += 1\n",
    "        else:\n",
    "            if combo > 1:\n",
    "                result.append((starter, combo))\n",
    "            starter = cursor\n",
    "            combo = 1\n",
    "    if combo > 1:\n",
    "         result.append((starter, combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getCombosCount(timeSeries, matcher = -1, lowerbound = 2):\n",
    "    combos = comboTracker(timeSeries)\n",
    "    combos_count = sum([hit for element, hit in combos if element == matcher and hit > lowerbound])\n",
    "    return combos_count\n",
    "    \n",
    "\n",
    "\n",
    "def getFlips(timeSeries, attribute= 'polarity'):\n",
    "    flips = np.zeros(timeSeries.shape[0],dtype=bool)\n",
    "    polarity = timeSeries[attribute].values[:-1]\n",
    "    right_elements = timeSeries[attribute].values[1:]\n",
    "    flips[:-1] = (polarity * right_elements) < 0\n",
    "    return flips\n",
    "\n",
    "def getFlipsCount(timeSeries, upperbound=30, lowerbound = 0):\n",
    "    flips = getFlips(timeSeries)\n",
    "    durations = getFlipsDuration(timeSeries, flips)\n",
    "    return np.sum((durations > lowerbound) & (durations < upperbound) )\n",
    "\n",
    "\n",
    "\n",
    "def getFlipsDuration(timeSeries, flips):\n",
    "    timeSeries = timeSeries[flips]\n",
    "    timeSeries.loc[:,'dt'] = np.zeros(timeSeries.shape[0],dtype=float)\n",
    "    timeSeries.loc[:-1,'dt'] = (timeSeries.index[1:].values - timeSeries.index[:-1].values).astype('timedelta64[s]') / np.timedelta64(60, 's')\n",
    "    return timeSeries['dt'][:-1].values\n",
    "\n",
    "\n",
    "def getTextFeature(usersDataList, text_model):\n",
    "    if text_model == \"tfidf\":\n",
    "        model = TfidfVectorizer(stop_words=\"english\",ngram_range = (1,2))\n",
    "        to_train = True\n",
    "    else:\n",
    "        model = text_model\n",
    "        to_train = False\n",
    "        \n",
    "    getText = lambda data : \"\\n\".join(data[\"text\"].values)\n",
    "    usersTextsList = []\n",
    "    totalTexts = []\n",
    "    for usersData in usersDataList:\n",
    "        usersTexts = []\n",
    "        for data in usersData:\n",
    "            text = getText(data)\n",
    "            totalTexts.append(text)\n",
    "            usersTexts.append(text)\n",
    "        usersTextsList.append(usersTexts)\n",
    "    \n",
    "    if to_train:\n",
    "        model.fit(totalTexts)\n",
    "    \n",
    "    featuresList = []\n",
    "    for usersTexts in usersTextsList:\n",
    "        featuresList.append(model.transform(usersTexts))\n",
    "        \n",
    "    return featuresList, model\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "def getPolarityFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),4),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            negative_ratio = getNegativeCount(timeSeries) / tweets_length\n",
    "            feature[i][0] = flips_ratio \n",
    "            feature[i][1] = combos_ratio\n",
    "            feature[i][2] = negative_ratio\n",
    "            feature[i][3] = first_pronoun_ratio\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def getJHFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),6),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "         \n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = negative_ratio\n",
    "            feature[i][5] = positive_ratio\n",
    "          \n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def getCombinedFeature(groups):\n",
    "    features = []\n",
    "    for group in groups:\n",
    "        feature = np.zeros((len(group),9),dtype=float)\n",
    "        for i, timeSeries in enumerate(group):\n",
    "            tweets_length = timeSeries.shape[0]\n",
    "            tweets_rate = tweetRate(timeSeries)\n",
    "            mentio_rate = mentioRate(timeSeries)\n",
    "            unique_mentions = uniqueMentions(timeSeries)\n",
    "            frequent_mentions = frequentMentions(timeSeries)\n",
    "            negative_ratio = getNegativeRatio(timeSeries)\n",
    "            positive_ratio = getPositiveRatio(timeSeries)\n",
    "            flips_ratio = getFlipsCount(timeSeries) / tweets_length\n",
    "            combos_ratio = getCombosCount(timeSeries) / tweets_length\n",
    "            first_pronoun_ratio = getFirstPronounCount(timeSeries) / tweets_length\n",
    "            \n",
    "            feature[i][0] = tweets_rate \n",
    "            feature[i][1] = mentio_rate\n",
    "            feature[i][2] = unique_mentions\n",
    "            feature[i][3] = frequent_mentions \n",
    "            feature[i][4] = negative_ratio\n",
    "            feature[i][5] = positive_ratio\n",
    "            feature[i][6] = flips_ratio\n",
    "            feature[i][7] = combos_ratio\n",
    "            feature[i][8] = first_pronoun_ratio\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def featureExtraction(usersDataList, polarity_extraction=True, text_extraction=True, text_model=\"tfidf\"):\n",
    "    featuresList = []\n",
    " \n",
    "\n",
    "    if polarity_extraction:\n",
    "        polarity_features = getPolarityFeature(usersDataList)\n",
    "        \n",
    "        if not text_extraction:\n",
    "            featuresList = polarity_features\n",
    "        \n",
    "        \n",
    "    if text_extraction:\n",
    "        \n",
    "        text_features, model = getTextFeature(usersDataList, text_model)\n",
    "        \n",
    "        \n",
    "        if polarity_extraction:\n",
    "            featuresList = featureCombination(polarity_features, text_features)\n",
    "        else:\n",
    "            featuresList = text_features\n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(usersDataList):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "    if text_extraction:\n",
    "        return list(zip(featuresList, Ylist)), model \n",
    "    else:\n",
    "        return list(zip(featuresList, Ylist)) \n",
    "    \n",
    "    \n",
    "    \n",
    "def JHfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList = getJHFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) \n",
    "\n",
    "    \n",
    "def CombinedfeatureExtraction(groups):\n",
    "\n",
    " \n",
    "\n",
    "    featuresList =getCombinedFeature(groups)\n",
    "        \n",
    "  \n",
    "        \n",
    "    Ylist = []\n",
    "    for label, usersData in enumerate(groups):\n",
    "        Y = np.zeros(len(usersData),dtype=int)\n",
    "        Y[:] = label\n",
    "        Ylist.append(Y)\n",
    "        \n",
    "  \n",
    "    return list(zip(featuresList, Ylist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BPD_polarities = getUsersPolarities(\"BPD_581_polarity\")\n",
    "BPD_timeSeries = groupFilter(timeSeriesTransform(BPD_polarities))\n",
    "regular_polarities = getUsersPolarities(\"regularUser_en_fixed_emotion\")\n",
    "regular_timeSeries = groupFilter(timeSeriesTransform(regular_polarities))\n",
    "groups = [regular_timeSeries, BPD_timeSeries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "featuresList, text_model = featureExtraction(groups)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresList, text_model = featureExtraction(groups, polarity_extraction=False)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featuresList = featureExtraction(groups, text_extraction=False)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "classifer.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifer.feature_importances_#1 flip 2.combo 3.negative ratio 4.first pronoun ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JH_featuresList = JHfeatureExtraction(groups)\n",
    "classifer = plot_precision_recall(forest, JH_featuresList)\n",
    "classifer.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_featuresList = CombinedfeatureExtraction(groups)\n",
    "classifer = plot_precision_recall(forest, com_featuresList)\n",
    "classifer.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_text = x['text'].values[:-1]\n",
    "right_text = x['text'].values[1:]\n",
    "\n",
    "edit_distance = np.vectorize(distance)\n",
    "edit_distance(left_text, right_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_multiple_precision_recall(classifier, featuresLists,names,colors=['r','g','b'] , n_fold=20):\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.spines['bottom'].set_color('white')\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "    ax.spines['left'].set_color('white')\n",
    "    ax.tick_params(axis='x', colors='white')\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    ax.title.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision Recall Curve')\n",
    "    \n",
    "    for g,featuresList in enumerate(featuresLists):\n",
    "\n",
    "        SCORES = np.array([])\n",
    "        LABELS = np.array([])\n",
    "\n",
    "        X, Y = XYGenerator(featuresList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sss = StratifiedShuffleSplit(Y, n_fold, random_state=randint(0,65536) )\n",
    "\n",
    "\n",
    "        for train_index, test_index in sss:\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            classifier.fit(X_train, Y_train)\n",
    "            score = classifier.predict_proba(X_test)[:,1]\n",
    "            SCORES = np.concatenate((SCORES, score))\n",
    "            LABELS = np.concatenate((LABELS, Y_test))\n",
    "\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(LABELS, SCORES, pos_label=1)\n",
    "        average_precision = average_precision_score(LABELS, SCORES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        BPD_train = np.count_nonzero(Y_train)\n",
    "        BPD_test = np.count_nonzero(Y_test)\n",
    "        normal_train = Y_train.shape[0] - BPD_train\n",
    "        normal_test = Y_test.shape[0] - BPD_test\n",
    "\n",
    "       \n",
    "\n",
    "        plt.plot(recall, precision, label=names[g] , color=colors[g],linewidth=3)\n",
    "      \n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "    print(\"{} Normal and {} BPD in Training Data\".format(normal_train,BPD_train))\n",
    "    print(\"{} Normal and {} BPD in Test Data\".format(normal_test,BPD_test))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\"GB\",\"JH\",\"JH+GB\"]\n",
    "new_feature_lists = [featureExtraction(groups, text_extraction=False), JHfeatureExtraction(groups),CombinedfeatureExtraction(groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_multiple_precision_recall(RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=1024),new_feature_lists,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "coach_polarities = getUsersPolarities(\"coach_tweets_polarity\")\n",
    "coach_timeSeries = groupFilter(timeSeriesTransform(coach_polarities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresList, text_model = featureExtraction(groups, polarity_extraction=False)\n",
    "forest = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "classifer = plot_precision_recall(forest, featuresList)\n",
    "try:\n",
    "    featureScores = np.argsort(classifer.feature_importances_)[::-1]\n",
    "except:\n",
    "    featureScores = np.argsort(np.fabs(classifer.coef_[0]))#[::-1]\n",
    "    \n",
    "showImportantFeatures(featureScores, text_model, n_features=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coach_feature, _ = getTextFeature([coach_timeSeries], text_model)\n",
    "coach_X = coach_feature[0]\n",
    "print(classifer.score(coach_X, np.zeros(coach_X.shape[0],dtype=int)))\n",
    "classifer.predict(coach_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_featuresList = CombinedfeatureExtraction(groups)\n",
    "classifer = plot_precision_recall(forest, com_featuresList)\n",
    "classifer.feature_importances_\n",
    "coach_feature_polarty  = getCombinedFeature([coach_timeSeries])\n",
    "coach_X = coach_feature_polarty[0]\n",
    "print(classifer.score(coach_X,np.zeros(coach_X.shape[0],dtype= int)))\n",
    "classifer.predict(coach_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_featuresList = getJHFeature(groups)\n",
    "classifer = plot_precision_recall(forest, com_featuresList)\n",
    "classifer.feature_importances_\n",
    "coach_feature_polarty  = getCombinedFeature([coach_timeSeries])\n",
    "coach_X = coach_feature_polarty[0]\n",
    "print(classifer.score(coach_X,np.zeros(coach_X.shape[0],dtype= int)))\n",
    "classifer.predict(coach_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
